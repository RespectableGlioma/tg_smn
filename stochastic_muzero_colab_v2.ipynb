{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero with Learned Temporal Abstractions\n",
    "## V2: Atari Games Edition\n",
    "\n",
    "This notebook implements **Stochastic MuZero** with macro discovery on **deterministic Atari games**.\n",
    "\n",
    "### Why Atari?\n",
    "- **Single-player**: Agent controls all actions (no opponent uncertainty)\n",
    "- **Deterministic physics**: Ball bounces, gravity, collisions are predictable\n",
    "- **Rich temporal structure**: Positioning sequences, attack patterns, navigation\n",
    "\n",
    "### Core Idea\n",
    "Rules are **compressible causal structure** in the transition dynamics:\n",
    "- If a sequence of transitions is deterministic and repeatable, collapse it into a **macro-operator**\n",
    "- Macros enable faster planning and capture reusable temporal abstractions\n",
    "\n",
    "### Games\n",
    "- **Breakout**: Ball trajectory, paddle positioning\n",
    "- **Pong**: Ball interception patterns\n",
    "- **Space Invaders**: Enemy patterns, shooting sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy gymnasium ale-py tqdm matplotlib -q\n",
    "\n",
    "# Install Atari ROMs (required for ALE games)\n",
    "!pip install autorom[accept-rom-license] -q\n",
    "!python -m atari_py.import_roms . 2>/dev/null || true\n",
    "\n",
    "# Alternative ROM installation if above fails\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(['AutoROM', '--accept-license'], capture_output=True, timeout=60)\n",
    "    print(\"ROMs installed via AutoROM\")\n",
    "except:\n",
    "    print(\"AutoROM not available, trying alternative...\")\n",
    "    !pip install ale-py[roms] -q 2>/dev/null || true\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Atari environment\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "try:\n",
    "    env = gym.make(\"ALE/Breakout-v5\")\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Breakout works!\")\n",
    "    print(f\"  Observation: {obs.shape}\")\n",
    "    print(f\"  Actions: {env.action_space.n} - {env.unwrapped.get_action_meanings()}\")\n",
    "    env.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTrying to fix...\")\n",
    "    !pip install 'gymnasium[atari,accept-rom-license]' -q\n",
    "    print(\"\\nPlease restart runtime and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Atari Game Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import List, Dict, Optional, Tuple, Any\nimport gymnasium as gym\nfrom gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n\n# Register ALE environments\nimport ale_py\ngym.register_envs(ale_py)\n\n\n@dataclass\nclass AtariState:\n    \"\"\"Wrapper for Atari game state.\"\"\"\n    env: gym.Env\n    observation: np.ndarray\n    done: bool = False\n    lives: int = 0\n    score: int = 0\n\n\nclass AtariGame:\n    \"\"\"\n    Atari game wrapper with deterministic mode for macro discovery.\n    \n    Key settings:\n    - repeat_action_probability=0: No sticky actions (deterministic)\n    - noop_max=0: No random initial noops\n    - 84x84 grayscale, 4 stacked frames\n    \"\"\"\n    \n    ACTIONS = {\n        0: \"NOOP\", 1: \"FIRE\", 2: \"UP\", 3: \"RIGHT\", 4: \"LEFT\", 5: \"DOWN\",\n        6: \"UPRIGHT\", 7: \"UPLEFT\", 8: \"DOWNRIGHT\", 9: \"DOWNLEFT\",\n        10: \"UPFIRE\", 11: \"RIGHTFIRE\", 12: \"LEFTFIRE\", 13: \"DOWNFIRE\",\n    }\n\n    def __init__(self, game_name: str = \"Breakout\", frame_stack: int = 4, \n                 frame_skip: int = 4, deterministic: bool = True):\n        self.game_name = game_name\n        self.frame_stack = frame_stack\n        self.frame_skip = frame_skip\n        self.deterministic = deterministic\n        \n        self._env = self._make_env()\n        self.action_space_size = self._env.action_space.n\n        self.action_meanings = self._env.unwrapped.get_action_meanings()\n        self.observation_dim = 84 * 84 * frame_stack  # Flattened\n\n    def _make_env(self) -> gym.Env:\n        env = gym.make(\n            f\"ALE/{self.game_name}-v5\",\n            repeat_action_probability=0.0 if self.deterministic else 0.25,\n            frameskip=1,\n        )\n        env = AtariPreprocessing(\n            env,\n            noop_max=0 if self.deterministic else 30,\n            frame_skip=self.frame_skip,\n            screen_size=84,\n            grayscale_obs=True,\n            grayscale_newaxis=True,\n            scale_obs=True,\n        )\n        env = FrameStackObservation(env, self.frame_stack)\n        return env\n\n    def reset(self) -> AtariState:\n        env = self._make_env()\n        obs, info = env.reset()\n        return AtariState(env=env, observation=obs, done=False, \n                         lives=info.get('lives', 0), score=0)\n\n    def step(self, state: AtariState, action: int) -> Tuple[AtariState, float, bool]:\n        if state.done:\n            return state, 0.0, True\n        \n        obs, reward, terminated, truncated, info = state.env.step(action)\n        done = terminated or truncated\n        \n        new_state = AtariState(\n            env=state.env, observation=obs, done=done,\n            lives=info.get('lives', state.lives),\n            score=state.score + int(reward),\n        )\n        return new_state, float(reward), done\n\n    def encode(self, state: AtariState) -> torch.Tensor:\n        \"\"\"Encode observation as flattened tensor.\"\"\"\n        obs = state.observation\n        if obs.ndim == 4 and obs.shape[-1] == 1:\n            obs = obs.squeeze(-1)  # (4, 84, 84)\n        obs = np.asarray(obs, dtype=np.float32).reshape(-1)\n        return torch.tensor(obs, dtype=torch.float32)\n\n    def legal_actions(self, state: AtariState) -> List[int]:\n        return [] if state.done else list(range(self.action_space_size))\n\n    def action_name(self, action: int) -> str:\n        if action < len(self.action_meanings):\n            return self.action_meanings[action]\n        return f\"Action_{action}\"\n\n\n# Test\ngame = AtariGame(\"Breakout\")\nprint(f\"Game: {game.game_name}\")\nprint(f\"Observation dim: {game.observation_dim}\")\nprint(f\"Actions: {game.action_space_size} - {game.action_meanings}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MuZero Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MuZeroNetwork(nn.Module):\n    \"\"\"\n    MuZero network with entropy estimation for macro discovery.\n    \n    Key addition: The dynamics network outputs both next_state AND\n    a confidence/entropy estimate. Low entropy = deterministic transition.\n    \"\"\"\n    \n    def __init__(self, obs_dim: int, action_dim: int, \n                 state_dim: int = 256, hidden_dim: int = 256):\n        super().__init__()\n        self.action_dim = action_dim\n        self.state_dim = state_dim\n        \n        # Representation network: obs -> state\n        self.representation = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim),\n        )\n        \n        # Dynamics network: (state, action) -> next_state\n        self.dynamics = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim),\n        )\n        \n        # Entropy head: predicts uncertainty of transition\n        # Low entropy = deterministic (macro-able)\n        # High entropy = stochastic (can't compress)\n        self.entropy_head = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Softplus(),  # Ensure positive entropy\n        )\n        \n        # Reward prediction\n        self.reward_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1),\n        )\n        \n        # Policy head\n        self.policy_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n        )\n        \n        # Value head\n        self.value_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n    \n    def initial_inference(self, obs: torch.Tensor):\n        \"\"\"Initial inference: obs -> (state, policy, value)\"\"\"\n        state = self.representation(obs)\n        policy = self.policy_head(state)\n        value = self.value_head(state).squeeze(-1)\n        return state, policy, value\n    \n    def recurrent_inference(self, state: torch.Tensor, action: torch.Tensor):\n        \"\"\"\n        Recurrent inference with entropy estimation.\n        \n        Returns: (next_state, reward, policy, value, entropy)\n        \n        entropy: Model's uncertainty about this transition.\n                 Low = deterministic, good for macros.\n                 High = stochastic, can't compress.\n        \"\"\"\n        # One-hot encode action\n        action_onehot = F.one_hot(action, self.action_dim).float()\n        x = torch.cat([state, action_onehot], dim=-1)\n        \n        next_state = self.dynamics(x)\n        entropy = self.entropy_head(x).squeeze(-1)  # Predicted transition entropy\n        \n        reward = self.reward_head(next_state).squeeze(-1)\n        policy = self.policy_head(next_state)\n        value = self.value_head(next_state).squeeze(-1)\n        \n        return next_state, reward, policy, value, entropy\n\n\n# Test\nmodel = MuZeroNetwork(game.observation_dim, game.action_space_size).to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\nstate = game.reset()\nobs = game.encode(state).unsqueeze(0).to(DEVICE)\nhidden, policy, value = model.initial_inference(obs)\n\n# Test dynamics with entropy\naction = torch.tensor([1], device=DEVICE)\nnext_hidden, reward, next_policy, next_value, entropy = model.recurrent_inference(hidden, action)\nprint(f\"Hidden state: {hidden.shape}\")\nprint(f\"Transition entropy: {entropy.item():.4f}\")\nprint(f\"  (Low entropy = deterministic transition = macro candidate)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple MCTS with entropy tracking for macro discovery\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Optional\nimport math\n\n\n@dataclass\nclass MCTSNode:\n    \"\"\"Node in the MCTS tree.\"\"\"\n    hidden_state: torch.Tensor\n    reward: float = 0.0\n    prior: float = 0.0\n    entropy: float = 0.0  # Model's uncertainty at this transition\n    \n    visit_count: int = 0\n    value_sum: float = 0.0\n    children: Dict[int, 'MCTSNode'] = field(default_factory=dict)\n    action_from_parent: int = -1\n    \n    @property\n    def value(self) -> float:\n        if self.visit_count == 0:\n            return 0.0\n        return self.value_sum / self.visit_count\n    \n    def expanded(self) -> bool:\n        return len(self.children) > 0\n\n\nclass MCTS:\n    \"\"\"\n    Monte Carlo Tree Search with entropy tracking.\n    \n    During search, we track the entropy of each transition.\n    Low-entropy paths through the tree are macro candidates.\n    \"\"\"\n    \n    def __init__(self, model: MuZeroNetwork, device: torch.device,\n                 num_simulations: int = 25,\n                 c_puct: float = 1.25,\n                 discount: float = 0.99):\n        self.model = model\n        self.device = device\n        self.num_simulations = num_simulations\n        self.c_puct = c_puct\n        self.discount = discount\n        \n        # Track rollout entropies for macro discovery\n        self.rollout_actions: List[List[int]] = []\n        self.rollout_entropies: List[List[float]] = []\n        self.rollout_rewards: List[List[float]] = []\n    \n    def search(self, obs: torch.Tensor, legal_actions: List[int]) -> MCTSNode:\n        \"\"\"Run MCTS from observation, return root node.\"\"\"\n        self.model.eval()\n        \n        # Initialize root\n        with torch.no_grad():\n            hidden, policy_logits, value = self.model.initial_inference(obs)\n        \n        root = MCTSNode(hidden_state=hidden)\n        self._expand(root, policy_logits, legal_actions)\n        \n        # Run simulations\n        for _ in range(self.num_simulations):\n            node = root\n            path = []\n            actions = []\n            entropies = []\n            rewards = []\n            \n            # Selection\n            while node.expanded():\n                action, child = self._select_child(node)\n                path.append(node)\n                actions.append(action)\n                entropies.append(child.entropy)\n                rewards.append(child.reward)\n                node = child\n            \n            # Expansion\n            if not node.expanded():\n                with torch.no_grad():\n                    next_hidden, reward, policy, value, entropy = self.model.recurrent_inference(\n                        node.hidden_state, \n                        torch.tensor([actions[-1]] if actions else [0], device=self.device)\n                    )\n                node.hidden_state = next_hidden\n                self._expand(node, policy, legal_actions)\n                leaf_value = value.item()\n            else:\n                leaf_value = node.value\n            \n            # Backprop\n            self._backpropagate(path + [node], leaf_value)\n            \n            # Store rollout for macro discovery\n            if len(actions) >= 2:\n                self.rollout_actions.append(actions)\n                self.rollout_entropies.append(entropies)\n                self.rollout_rewards.append(rewards)\n        \n        return root\n    \n    def _expand(self, node: MCTSNode, policy_logits: torch.Tensor, legal_actions: List[int]):\n        \"\"\"Expand node with children for legal actions.\"\"\"\n        probs = F.softmax(policy_logits[0], dim=0).cpu().numpy()\n        \n        for action in legal_actions:\n            with torch.no_grad():\n                next_hidden, reward, _, value, entropy = self.model.recurrent_inference(\n                    node.hidden_state,\n                    torch.tensor([action], device=self.device)\n                )\n            \n            child = MCTSNode(\n                hidden_state=next_hidden,\n                reward=reward.item(),\n                prior=probs[action],\n                entropy=entropy.item(),\n                action_from_parent=action,\n            )\n            node.children[action] = child\n    \n    def _select_child(self, node: MCTSNode) -> tuple:\n        \"\"\"Select child using PUCT formula.\"\"\"\n        best_score = -float('inf')\n        best_action = None\n        best_child = None\n        \n        sqrt_total = math.sqrt(node.visit_count + 1)\n        \n        for action, child in node.children.items():\n            # PUCT score\n            if child.visit_count == 0:\n                q_value = 0\n            else:\n                q_value = child.value\n            \n            exploration = self.c_puct * child.prior * sqrt_total / (1 + child.visit_count)\n            score = q_value + exploration\n            \n            if score > best_score:\n                best_score = score\n                best_action = action\n                best_child = child\n        \n        return best_action, best_child\n    \n    def _backpropagate(self, path: List[MCTSNode], value: float):\n        \"\"\"Backpropagate value through path.\"\"\"\n        for node in reversed(path):\n            node.visit_count += 1\n            node.value_sum += value\n            value = node.reward + self.discount * value\n    \n    def get_action_probs(self, root: MCTSNode, temperature: float = 1.0) -> tuple:\n        \"\"\"Get action probabilities from visit counts.\"\"\"\n        actions = list(root.children.keys())\n        visits = np.array([root.children[a].visit_count for a in actions])\n        \n        if temperature == 0:\n            action = actions[np.argmax(visits)]\n            probs = np.zeros(len(actions))\n            probs[np.argmax(visits)] = 1.0\n        else:\n            visits = visits ** (1 / temperature)\n            probs = visits / visits.sum()\n            action = np.random.choice(actions, p=probs)\n        \n        return action, probs\n    \n    def get_mcts_rollouts(self) -> tuple:\n        \"\"\"Return collected rollouts for macro discovery.\"\"\"\n        actions = self.rollout_actions\n        entropies = self.rollout_entropies\n        rewards = self.rollout_rewards\n        \n        # Clear for next episode\n        self.rollout_actions = []\n        self.rollout_entropies = []\n        self.rollout_rewards = []\n        \n        return actions, entropies, rewards\n\n\n# Test MCTS\nmcts = MCTS(model, DEVICE, num_simulations=10)\nstate = game.reset()\nobs = game.encode(state).unsqueeze(0).to(DEVICE)\nroot = mcts.search(obs, game.legal_actions(state))\naction, probs = mcts.get_action_probs(root, temperature=1.0)\nprint(f\"MCTS search complete. Selected action: {game.action_name(action)}\")\nprint(f\"Visit counts: {[(game.action_name(a), c.visit_count) for a, c in root.children.items()]}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Macro Discovery\n",
    "\n",
    "Macros are **reusable action sequences** discovered from gameplay:\n",
    "- Track action patterns that occur frequently\n",
    "- In deterministic Atari, all transitions have entropy \u2248 0\n",
    "- Patterns that repeat become macro candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from collections import defaultdict\n\n\n@dataclass\nclass MacroOperator:\n    \"\"\"A discovered macro-operator.\"\"\"\n    actions: tuple\n    avg_entropy: float\n    count: int = 0\n    total_reward: float = 0.0\n    state_buckets: set = field(default_factory=set)\n    source: str = \"play\"  # \"play\" or \"mcts\"\n    \n    @property\n    def avg_reward(self) -> float:\n        return self.total_reward / max(self.count, 1)\n    \n    @property\n    def length(self) -> int:\n        return len(self.actions)\n    \n    @property\n    def is_deterministic(self) -> bool:\n        return self.avg_entropy < 0.1\n    \n    @property\n    def state_generality(self) -> int:\n        return len(self.state_buckets)\n\n\nclass MacroCache:\n    \"\"\"\n    Dual-threshold macro discovery:\n    - Tight threshold (0.1) for actual played trajectories\n    - Looser threshold (0.3) for MCTS speculative rollouts\n    \n    This accounts for MCTS exploring more uncertain paths.\n    \"\"\"\n    \n    def __init__(self, \n                 play_entropy_threshold: float = 0.15,   # Tight for played path\n                 mcts_entropy_threshold: float = 0.35,   # Looser for MCTS\n                 min_occurrences: int = 3, \n                 min_length: int = 2, \n                 max_length: int = 6,\n                 state_dim: int = 256,\n                 num_state_buckets: int = 64):\n        self.play_threshold = play_entropy_threshold\n        self.mcts_threshold = mcts_entropy_threshold\n        self.min_occurrences = min_occurrences\n        self.min_length = min_length\n        self.max_length = max_length\n        self.num_state_buckets = num_state_buckets\n        \n        self.state_projection = torch.randn(state_dim, num_state_buckets)\n        \n        self.candidates = defaultdict(lambda: {\n            \"count\": 0, \"total_entropy\": 0.0, \"total_reward\": 0.0, \n            \"state_buckets\": set(), \"source\": set()\n        })\n        self.macros: Dict[tuple, MacroOperator] = {}\n        \n        self.total_transitions = 0\n        self.low_entropy_play = 0\n        self.low_entropy_mcts = 0\n        self.mcts_rollouts = 0\n        self.play_rollouts = 0\n    \n    def _get_state_bucket(self, hidden_state: torch.Tensor) -> int:\n        with torch.no_grad():\n            if hidden_state.dim() > 1:\n                hidden_state = hidden_state.squeeze(0)\n            proj = hidden_state.cpu() @ self.state_projection\n            bucket = int(proj.argmax().item())\n        return bucket\n    \n    def add_played_trajectory(self, actions: List[int], entropies: List[float], \n                              rewards: List[float], initial_state: Optional[torch.Tensor] = None):\n        \"\"\"Add trajectory from actual gameplay (tight threshold).\"\"\"\n        self._add_rollout(actions, entropies, rewards, initial_state, \n                         threshold=self.play_threshold, source=\"play\")\n        self.play_rollouts += 1\n    \n    def add_mcts_rollout(self, actions: List[int], entropies: List[float], \n                         rewards: List[float], initial_state: Optional[torch.Tensor] = None):\n        \"\"\"Add MCTS speculative rollout (looser threshold).\"\"\"\n        self._add_rollout(actions, entropies, rewards, initial_state,\n                         threshold=self.mcts_threshold, source=\"mcts\")\n        self.mcts_rollouts += 1\n    \n    def _add_rollout(self, actions, entropies, rewards, initial_state, threshold, source):\n        n = len(actions)\n        if n < self.min_length or len(entropies) < n:\n            return\n        \n        self.total_transitions += n\n        \n        state_bucket = 0\n        if initial_state is not None:\n            state_bucket = self._get_state_bucket(initial_state)\n        \n        for length in range(self.min_length, min(self.max_length + 1, n + 1)):\n            for i in range(n - length + 1):\n                if i + length > len(entropies):\n                    continue\n                window_entropies = entropies[i:i + length]\n                \n                if all(e < threshold for e in window_entropies):\n                    pattern = tuple(actions[i:i + length])\n                    \n                    if len(set(pattern)) <= 1:\n                        continue\n                    \n                    if source == \"play\":\n                        self.low_entropy_play += 1\n                    else:\n                        self.low_entropy_mcts += 1\n                    \n                    avg_entropy = sum(window_entropies) / length\n                    pattern_reward = sum(rewards[i:i + length]) if i + length <= len(rewards) else 0\n                    \n                    self.candidates[pattern][\"count\"] += 1\n                    self.candidates[pattern][\"total_entropy\"] += avg_entropy\n                    self.candidates[pattern][\"total_reward\"] += pattern_reward\n                    self.candidates[pattern][\"state_buckets\"].add(state_bucket)\n                    self.candidates[pattern][\"source\"].add(source)\n                    \n                    if (self.candidates[pattern][\"count\"] >= self.min_occurrences \n                        and pattern not in self.macros):\n                        c = self.candidates[pattern]\n                        primary_source = \"play\" if \"play\" in c[\"source\"] else \"mcts\"\n                        self.macros[pattern] = MacroOperator(\n                            actions=pattern,\n                            avg_entropy=c[\"total_entropy\"] / c[\"count\"],\n                            count=c[\"count\"],\n                            total_reward=c[\"total_reward\"],\n                            state_buckets=c[\"state_buckets\"].copy(),\n                            source=primary_source,\n                        )\n                    elif pattern in self.macros:\n                        m = self.macros[pattern]\n                        c = self.candidates[pattern]\n                        m.count = c[\"count\"]\n                        m.total_reward = c[\"total_reward\"]\n                        m.avg_entropy = c[\"total_entropy\"] / m.count\n                        m.state_buckets = c[\"state_buckets\"].copy()\n    \n    def add_mcts_rollouts_batch(self, mcts: 'MCTS', initial_state: torch.Tensor):\n        \"\"\"Process all rollouts from MCTS.\"\"\"\n        actions_list, entropies_list, rewards_list = mcts.get_mcts_rollouts()\n        for actions, entropies, rewards in zip(actions_list, entropies_list, rewards_list):\n            self.add_mcts_rollout(actions, entropies, rewards, initial_state)\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        total_low = self.low_entropy_play + self.low_entropy_mcts\n        det_rate = total_low / max(self.total_transitions, 1)\n        \n        if self.macros:\n            avg_gen = np.mean([m.state_generality for m in self.macros.values()])\n            max_gen = max(m.state_generality for m in self.macros.values())\n            play_macros = sum(1 for m in self.macros.values() if m.source == \"play\")\n        else:\n            avg_gen = max_gen = play_macros = 0\n        \n        return {\n            \"num_macros\": len(self.macros),\n            \"play_macros\": play_macros,\n            \"mcts_macros\": len(self.macros) - play_macros,\n            \"num_candidates\": len(self.candidates),\n            \"total_transitions\": self.total_transitions,\n            \"low_entropy_play\": self.low_entropy_play,\n            \"low_entropy_mcts\": self.low_entropy_mcts,\n            \"deterministic_rate\": det_rate,\n            \"mcts_rollouts\": self.mcts_rollouts,\n            \"play_rollouts\": self.play_rollouts,\n            \"avg_state_generality\": avg_gen,\n            \"max_state_generality\": max_gen,\n        }\n    \n    def get_top_macros(self, n: int = 10, sort_by: str = \"count\") -> List[MacroOperator]:\n        if sort_by == \"entropy\":\n            key = lambda m: m.avg_entropy\n        elif sort_by == \"generality\":\n            key = lambda m: -m.state_generality\n        else:\n            key = lambda m: -m.count\n        return sorted(self.macros.values(), key=key)[:n]\n    \n    def decode_macro(self, macro: MacroOperator, game) -> str:\n        return \" -> \".join([game.action_name(a) for a in macro.actions])\n\n\ncache = MacroCache(play_entropy_threshold=0.15, mcts_entropy_threshold=0.35)\nprint(f\"Dual-threshold MacroCache:\")\nprint(f\"  Play threshold: {cache.play_threshold} (tight)\")\nprint(f\"  MCTS threshold: {cache.mcts_threshold} (loose)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Play with Macro Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def play_episode_mcts(game: AtariGame, model: MuZeroNetwork, \n                      macro_cache: MacroCache, device: torch.device,\n                      max_steps: int = 1000, \n                      num_simulations: int = 25,\n                      temperature: float = 1.0) -> Tuple[List, List, List, List, int]:\n    \"\"\"\n    Play episode with MCTS, tracking both:\n    1. MCTS rollouts (speculative, loose threshold)\n    2. Actual played trajectory (tight threshold)\n    \"\"\"\n    model.eval()\n    mcts = MCTS(model, device, num_simulations=num_simulations)\n    \n    state = game.reset()\n    \n    observations = []\n    actions = []\n    rewards = []\n    entropies = []\n    \n    for step in range(max_steps):\n        if state.done:\n            break\n        \n        obs = game.encode(state).to(device).unsqueeze(0)\n        observations.append(obs.cpu())\n        \n        legal = game.legal_actions(state)\n        if not legal:\n            break\n        \n        # Run MCTS\n        root = mcts.search(obs, legal)\n        \n        # Get initial hidden state for macro tracking\n        with torch.no_grad():\n            hidden, _, _ = model.initial_inference(obs)\n        \n        # Add MCTS rollouts (loose threshold)\n        macro_cache.add_mcts_rollouts_batch(mcts, hidden)\n        \n        # Select action\n        action, _ = mcts.get_action_probs(root, temperature=temperature)\n        actions.append(action)\n        \n        # Get entropy for selected action\n        if action in root.children:\n            entropies.append(root.children[action].entropy)\n        else:\n            entropies.append(0.5)\n        \n        state, reward, done = game.step(state, action)\n        rewards.append(reward)\n    \n    # Add the ACTUALLY PLAYED trajectory (tight threshold)\n    if len(actions) >= 2 and observations:\n        first_obs = observations[0].to(device)\n        with torch.no_grad():\n            first_hidden, _, _ = model.initial_inference(first_obs)\n        macro_cache.add_played_trajectory(actions, entropies, rewards, first_hidden)\n    \n    return observations, actions, rewards, entropies, state.score\n\n\ndef play_episode_simple(game: AtariGame, model: MuZeroNetwork, \n                        macro_cache: MacroCache, device: torch.device,\n                        max_steps: int = 1000, epsilon: float = 0.1,\n                        temperature: float = 1.0) -> Tuple[List, List, List, List, int]:\n    \"\"\"Simple policy episode (no MCTS).\"\"\"\n    model.eval()\n    state = game.reset()\n    \n    observations = []\n    actions = []\n    rewards = []\n    entropies = []\n    \n    for step in range(max_steps):\n        if state.done:\n            break\n        \n        obs = game.encode(state).to(device).unsqueeze(0)\n        observations.append(obs.cpu())\n        \n        with torch.no_grad():\n            hidden, policy_logits, _ = model.initial_inference(obs)\n        \n        legal = game.legal_actions(state)\n        if not legal:\n            break\n        \n        if np.random.random() < epsilon:\n            action = np.random.choice(legal)\n        else:\n            probs = F.softmax(policy_logits[0] / temperature, dim=0).cpu().numpy()\n            mask = np.zeros(game.action_space_size)\n            mask[legal] = 1\n            probs = probs * mask\n            if probs.sum() > 0:\n                probs = probs / probs.sum()\n                action = np.random.choice(game.action_space_size, p=probs)\n            else:\n                action = np.random.choice(legal)\n        \n        with torch.no_grad():\n            action_tensor = torch.tensor([action], device=device)\n            _, _, _, _, entropy = model.recurrent_inference(hidden, action_tensor)\n            entropies.append(entropy.item())\n        \n        actions.append(action)\n        state, reward, done = game.step(state, action)\n        rewards.append(reward)\n    \n    # Add played trajectory\n    if len(actions) >= 2 and observations:\n        first_obs = observations[0].to(device)\n        with torch.no_grad():\n            first_hidden, _, _ = model.initial_inference(first_obs)\n        macro_cache.add_played_trajectory(actions, entropies, rewards, first_hidden)\n    \n    return observations, actions, rewards, entropies, state.score\n\n\nprint(\"Self-play ready with dual-threshold macro discovery!\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.optim as optim\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n\ndef train_muzero_mcts(game: AtariGame, model: MuZeroNetwork, \n                      macro_cache: MacroCache, device: torch.device,\n                      num_iterations: int = 100,\n                      episodes_per_iter: int = 2,\n                      max_steps: int = 500,\n                      num_simulations: int = 15,\n                      batch_size: int = 64,\n                      lr: float = 1e-4,\n                      use_mcts: bool = True):\n    \"\"\"\n    Train MuZero with MCTS-based macro discovery.\n    \n    Key improvements:\n    1. MCTS explores many paths -> more macro candidates\n    2. State-conditioned -> know which macros are general\n    3. Longer training -> accumulate confident patterns\n    \"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    replay_obs = []\n    replay_actions = []\n    replay_rewards = []\n    max_buffer = 10000\n    \n    history = {\n        \"scores\": [], \"lengths\": [], \"losses\": [], \n        \"macros\": [], \"avg_entropy\": [], \"deterministic_rate\": [],\n        \"mcts_rollouts\": [], \"state_generality\": []\n    }\n    \n    mode = \"MCTS\" if use_mcts else \"Policy\"\n    print(f\"Training with {mode} for {num_iterations} iterations\")\n    print(f\"  Episodes/iter: {episodes_per_iter}, Steps/episode: {max_steps}\")\n    if use_mcts:\n        print(f\"  MCTS simulations: {num_simulations}\")\n    print(f\"  Macro threshold: {macro_cache.entropy_threshold}\")\n    print()\n    \n    for iteration in tqdm(range(num_iterations), desc=\"Training\"):\n        # Temperature schedule\n        temp = max(0.5, 1.5 - iteration / (num_iterations / 2))\n        epsilon = max(0.05, 1.0 - iteration / num_iterations)\n        \n        iter_scores = []\n        iter_lengths = []\n        iter_entropies = []\n        \n        for _ in range(episodes_per_iter):\n            if use_mcts:\n                obs, acts, rews, ents, score = play_episode_mcts(\n                    game, model, macro_cache, device,\n                    max_steps=max_steps,\n                    num_simulations=num_simulations,\n                    temperature=temp\n                )\n            else:\n                obs, acts, rews, ents, score = play_episode_simple(\n                    game, model, device,\n                    max_steps=max_steps,\n                    epsilon=epsilon,\n                    temperature=temp\n                )\n                # Also add to macro cache\n                if obs:\n                    macro_cache.add_rollout(acts, ents, rews, obs[0].to(device))\n            \n            replay_obs.extend(obs)\n            replay_actions.extend(acts)\n            replay_rewards.extend(rews)\n            \n            iter_scores.append(score)\n            iter_lengths.append(len(acts))\n            iter_entropies.extend(ents)\n        \n        # Trim buffer\n        if len(replay_obs) > max_buffer:\n            replay_obs = replay_obs[-max_buffer:]\n            replay_actions = replay_actions[-max_buffer:]\n            replay_rewards = replay_rewards[-max_buffer:]\n        \n        # Training\n        if len(replay_obs) >= batch_size:\n            model.train()\n            \n            indices = np.random.choice(len(replay_obs), batch_size, replace=False)\n            batch_obs = torch.cat([replay_obs[i] for i in indices]).to(device)\n            batch_actions = torch.tensor([replay_actions[i] for i in indices], device=device)\n            batch_rewards = torch.tensor([replay_rewards[i] for i in indices], \n                                         device=device, dtype=torch.float32)\n            \n            hidden, policy_logits, values = model.initial_inference(batch_obs)\n            \n            policy_loss = F.cross_entropy(policy_logits, batch_actions)\n            value_loss = F.mse_loss(values, batch_rewards)\n            \n            next_hidden, pred_reward, _, _, pred_entropy = model.recurrent_inference(\n                hidden, batch_actions\n            )\n            reward_loss = F.mse_loss(pred_reward, batch_rewards)\n            \n            # Entropy regularization\n            entropy_target = 0.1\n            entropy_loss = F.mse_loss(pred_entropy, torch.full_like(pred_entropy, entropy_target))\n            \n            total_loss = policy_loss + 0.5 * value_loss + 0.5 * reward_loss + 0.1 * entropy_loss\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            history[\"losses\"].append(total_loss.item())\n        \n        # Record\n        history[\"scores\"].append(np.mean(iter_scores))\n        history[\"lengths\"].append(np.mean(iter_lengths))\n        history[\"macros\"].append(len(macro_cache.macros))\n        history[\"avg_entropy\"].append(np.mean(iter_entropies) if iter_entropies else 0)\n        \n        stats = macro_cache.get_statistics()\n        history[\"deterministic_rate\"].append(stats[\"deterministic_rate\"])\n        history[\"mcts_rollouts\"].append(stats[\"mcts_rollouts\"])\n        history[\"state_generality\"].append(stats[\"avg_state_generality\"])\n        \n        if iteration % 20 == 0:\n            print(f\"\\nIter {iteration}: Score={np.mean(iter_scores):.1f}, \"\n                  f\"Entropy={np.mean(iter_entropies):.3f}, \"\n                  f\"Macros={stats['num_macros']}, \"\n                  f\"MCTS rollouts={stats['mcts_rollouts']}, \"\n                  f\"Generality={stats['avg_state_generality']:.1f}\")\n    \n    return history\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train on Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize\nbreakout = AtariGame(\"Breakout\")\nmodel = MuZeroNetwork(breakout.observation_dim, breakout.action_space_size).to(DEVICE)\n\n# Dual-threshold macro cache\nmacro_cache = MacroCache(\n    play_entropy_threshold=0.2,   # Tight for played trajectories\n    mcts_entropy_threshold=0.4,   # Looser for MCTS exploration\n    min_occurrences=3,\n    min_length=2,\n    max_length=6,\n)\n\nprint(f\"Training: {breakout.game_name}\")\nprint(f\"Dual-threshold macro discovery:\")\nprint(f\"  Played trajectory: {macro_cache.play_threshold}\")\nprint(f\"  MCTS rollouts: {macro_cache.mcts_threshold}\")\nprint()\n\n# Train - fewer iterations to avoid collapse\nhistory = train_muzero_mcts(\n    game=breakout,\n    model=model,\n    macro_cache=macro_cache,\n    device=DEVICE,\n    num_iterations=80,\n    episodes_per_iter=3,\n    max_steps=500,\n    num_simulations=10,\n    use_mcts=True,\n)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training results\nfig, axes = plt.subplots(2, 4, figsize=(18, 8))\n\naxes[0, 0].plot(history[\"scores\"])\naxes[0, 0].set_xlabel(\"Iteration\"); axes[0, 0].set_ylabel(\"Score\")\naxes[0, 0].set_title(\"Breakout Score\")\n\naxes[0, 1].plot(history[\"lengths\"])\naxes[0, 1].set_xlabel(\"Iteration\"); axes[0, 1].set_ylabel(\"Length\")\naxes[0, 1].set_title(\"Episode Length\")\n\naxes[0, 2].plot(history[\"losses\"])\naxes[0, 2].set_xlabel(\"Step\"); axes[0, 2].set_ylabel(\"Loss\")\naxes[0, 2].set_title(\"Training Loss\")\n\naxes[0, 3].plot(history[\"macros\"])\naxes[0, 3].set_xlabel(\"Iteration\"); axes[0, 3].set_ylabel(\"Macros\")\naxes[0, 3].set_title(\"Macros Discovered\")\n\naxes[1, 0].plot(history[\"avg_entropy\"])\naxes[1, 0].axhline(y=macro_cache.entropy_threshold, color='r', linestyle='--', label='Threshold')\naxes[1, 0].set_xlabel(\"Iteration\"); axes[1, 0].set_ylabel(\"Entropy\")\naxes[1, 0].set_title(\"Model Entropy\"); axes[1, 0].legend()\n\naxes[1, 1].plot(history[\"deterministic_rate\"])\naxes[1, 1].set_xlabel(\"Iteration\"); axes[1, 1].set_ylabel(\"Rate\")\naxes[1, 1].set_title(\"Deterministic Rate\"); axes[1, 1].set_ylim(0, 1)\n\naxes[1, 2].plot(history[\"mcts_rollouts\"])\naxes[1, 2].set_xlabel(\"Iteration\"); axes[1, 2].set_ylabel(\"Rollouts\")\naxes[1, 2].set_title(\"MCTS Rollouts Processed\")\n\naxes[1, 3].plot(history[\"state_generality\"])\naxes[1, 3].set_xlabel(\"Iteration\"); axes[1, 3].set_ylabel(\"Avg Buckets\")\naxes[1, 3].set_title(\"Macro State Generality\")\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Macro Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"MACRO ANALYSIS: Dual-Threshold Discovery\")\nprint(\"=\" * 70)\n\nstats = macro_cache.get_statistics()\nprint(f\"\\nTransitions: {stats['total_transitions']}\")\nprint(f\"Low-entropy (play): {stats['low_entropy_play']}\")\nprint(f\"Low-entropy (MCTS): {stats['low_entropy_mcts']}\")\nprint(f\"Deterministic rate: {stats['deterministic_rate']:.1%}\")\nprint(f\"\\nRollouts - Play: {stats['play_rollouts']}, MCTS: {stats['mcts_rollouts']}\")\nprint(f\"\\nMacros discovered: {stats['num_macros']}\")\nprint(f\"  From play: {stats['play_macros']}\")\nprint(f\"  From MCTS: {stats['mcts_macros']}\")\nprint(f\"  Candidates: {stats['num_candidates']}\")\n\nif stats['num_macros'] > 0:\n    print(\"\\n\" + \"=\" * 70)\n    print(\"MOST GENERAL MACROS (work in many states)\")\n    print(\"=\" * 70)\n    for i, macro in enumerate(macro_cache.get_top_macros(10, sort_by=\"generality\")):\n        decoded = macro_cache.decode_macro(macro, breakout)\n        print(f\"{i+1:2d}. {decoded}\")\n        print(f\"    States: {macro.state_generality}, Count: {macro.count}, \"\n              f\"Entropy: {macro.avg_entropy:.3f}, Source: {macro.source}\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"MOST DETERMINISTIC (lowest entropy)\")\n    print(\"=\" * 70)\n    for i, macro in enumerate(macro_cache.get_top_macros(10, sort_by=\"entropy\")):\n        decoded = macro_cache.decode_macro(macro, breakout)\n        det = \"DET\" if macro.is_deterministic else \"learned\"\n        print(f\"{i+1:2d}. {decoded}\")\n        print(f\"    Entropy: {macro.avg_entropy:.4f} [{det}], Count: {macro.count}\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"MOST FREQUENT\")\n    print(\"=\" * 70)\n    for i, macro in enumerate(macro_cache.get_top_macros(10, sort_by=\"count\")):\n        decoded = macro_cache.decode_macro(macro, breakout)\n        print(f\"{i+1:2d}. {decoded} (count={macro.count}, entropy={macro.avg_entropy:.3f})\")\nelse:\n    print(\"\\nNo macros yet. Check:\")\n    print(\"  1. Is entropy dropping? (model learning)\")\n    print(\"  2. Are thresholds appropriate?\")\n    print(\"  3. Need more training iterations?\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Entropy distribution of discovered macros\nimport matplotlib.pyplot as plt\n\nif macro_cache.macros:\n    entropies = [m.avg_entropy for m in macro_cache.macros.values()]\n    \n    plt.figure(figsize=(10, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.hist(entropies, bins=30, edgecolor='black')\n    plt.axvline(x=0.1, color='r', linestyle='--', label='Deterministic threshold')\n    plt.axvline(x=macro_cache.entropy_threshold, color='orange', linestyle='--', label='Discovery threshold')\n    plt.xlabel(\"Average Entropy\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Entropy Distribution of Discovered Macros\")\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    lengths = [m.length for m in macro_cache.macros.values()]\n    plt.hist(lengths, bins=range(2, 8), edgecolor='black', align='left')\n    plt.xlabel(\"Macro Length\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Length Distribution of Macros\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Stats\n    det_macros = sum(1 for e in entropies if e < 0.1)\n    print(f\"Truly deterministic macros (entropy < 0.1): {det_macros} / {len(entropies)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train on Other Games (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Pong\n",
    "\n",
    "# pong = AtariGame(\"Pong\")\n",
    "# pong_model = MuZeroNetwork(pong.observation_dim, pong.action_space_size).to(DEVICE)\n",
    "# pong_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# pong_history = train_muzero(\n",
    "#     game=pong,\n",
    "#     model=pong_model,\n",
    "#     macro_cache=pong_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nPong Macros:\")\n",
    "# for macro in pong_cache.get_top_macros(10):\n",
    "#     print(f\"  {pong_cache.decode_macro(macro, pong)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Space Invaders\n",
    "\n",
    "# invaders = AtariGame(\"SpaceInvaders\")\n",
    "# invaders_model = MuZeroNetwork(invaders.observation_dim, invaders.action_space_size).to(DEVICE)\n",
    "# invaders_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# invaders_history = train_muzero(\n",
    "#     game=invaders,\n",
    "#     model=invaders_model,\n",
    "#     macro_cache=invaders_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nSpace Invaders Macros:\")\n",
    "# for macro in invaders_cache.get_top_macros(10):\n",
    "#     print(f\"  {invaders_cache.decode_macro(macro, invaders)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Deterministic Atari** games are ideal for macro discovery because:\n",
    "   - All transitions have entropy \u2248 0 (no sticky actions)\n",
    "   - Physics are predictable (ball bounces, gravity)\n",
    "   - Agent controls all actions (no opponent uncertainty)\n",
    "\n",
    "2. **Macros as Compressible Causal Structure**:\n",
    "   - Frequent action patterns represent reusable temporal abstractions\n",
    "   - These are \"rules\" discovered from dynamics, not symbolic definitions\n",
    "   - Longer training reveals more meaningful patterns\n",
    "\n",
    "3. **Next Steps**:\n",
    "   - Use macros to accelerate MCTS planning\n",
    "   - Learn macro preconditions (when to apply)\n",
    "   - Transfer macros across similar games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}