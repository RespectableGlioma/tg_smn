{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero with Learned Temporal Abstractions\n",
    "## V2: Atari Games Edition\n",
    "\n",
    "This notebook implements **Stochastic MuZero** with macro discovery on **deterministic Atari games**.\n",
    "\n",
    "### Why Atari?\n",
    "- **Single-player**: Agent controls all actions (no opponent uncertainty)\n",
    "- **Deterministic physics**: Ball bounces, gravity, collisions are predictable\n",
    "- **Rich temporal structure**: Positioning sequences, attack patterns, navigation\n",
    "\n",
    "### Core Idea\n",
    "Rules are **compressible causal structure** in the transition dynamics:\n",
    "- If a sequence of transitions is deterministic and repeatable, collapse it into a **macro-operator**\n",
    "- Macros enable faster planning and capture reusable temporal abstractions\n",
    "\n",
    "### Games\n",
    "- **Breakout**: Ball trajectory, paddle positioning\n",
    "- **Pong**: Ball interception patterns\n",
    "- **Space Invaders**: Enemy patterns, shooting sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy gymnasium ale-py tqdm matplotlib -q\n",
    "\n",
    "# Install Atari ROMs (required for ALE games)\n",
    "!pip install autorom[accept-rom-license] -q\n",
    "!python -m atari_py.import_roms . 2>/dev/null || true\n",
    "\n",
    "# Alternative ROM installation if above fails\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(['AutoROM', '--accept-license'], capture_output=True, timeout=60)\n",
    "    print(\"ROMs installed via AutoROM\")\n",
    "except:\n",
    "    print(\"AutoROM not available, trying alternative...\")\n",
    "    !pip install ale-py[roms] -q 2>/dev/null || true\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Atari environment\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "try:\n",
    "    env = gym.make(\"ALE/Breakout-v5\")\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Breakout works!\")\n",
    "    print(f\"  Observation: {obs.shape}\")\n",
    "    print(f\"  Actions: {env.action_space.n} - {env.unwrapped.get_action_meanings()}\")\n",
    "    env.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTrying to fix...\")\n",
    "    !pip install 'gymnasium[atari,accept-rom-license]' -q\n",
    "    print(\"\\nPlease restart runtime and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Atari Game Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import List, Dict, Optional, Tuple, Any\nimport gymnasium as gym\nfrom gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n\n# Register ALE environments\nimport ale_py\ngym.register_envs(ale_py)\n\n\n@dataclass\nclass AtariState:\n    \"\"\"Wrapper for Atari game state.\"\"\"\n    env: gym.Env\n    observation: np.ndarray\n    done: bool = False\n    lives: int = 0\n    score: int = 0\n\n\nclass AtariGame:\n    \"\"\"\n    Atari game wrapper with deterministic mode for macro discovery.\n    \n    Key settings:\n    - repeat_action_probability=0: No sticky actions (deterministic)\n    - noop_max=0: No random initial noops\n    - 84x84 grayscale, 4 stacked frames\n    \"\"\"\n    \n    ACTIONS = {\n        0: \"NOOP\", 1: \"FIRE\", 2: \"UP\", 3: \"RIGHT\", 4: \"LEFT\", 5: \"DOWN\",\n        6: \"UPRIGHT\", 7: \"UPLEFT\", 8: \"DOWNRIGHT\", 9: \"DOWNLEFT\",\n        10: \"UPFIRE\", 11: \"RIGHTFIRE\", 12: \"LEFTFIRE\", 13: \"DOWNFIRE\",\n    }\n\n    def __init__(self, game_name: str = \"Breakout\", frame_stack: int = 4, \n                 frame_skip: int = 4, deterministic: bool = True):\n        self.game_name = game_name\n        self.frame_stack = frame_stack\n        self.frame_skip = frame_skip\n        self.deterministic = deterministic\n        \n        self._env = self._make_env()\n        self.action_space_size = self._env.action_space.n\n        self.action_meanings = self._env.unwrapped.get_action_meanings()\n        self.observation_dim = 84 * 84 * frame_stack  # Flattened\n\n    def _make_env(self) -> gym.Env:\n        env = gym.make(\n            f\"ALE/{self.game_name}-v5\",\n            repeat_action_probability=0.0 if self.deterministic else 0.25,\n            frameskip=1,\n        )\n        env = AtariPreprocessing(\n            env,\n            noop_max=0 if self.deterministic else 30,\n            frame_skip=self.frame_skip,\n            screen_size=84,\n            grayscale_obs=True,\n            grayscale_newaxis=True,\n            scale_obs=True,\n        )\n        env = FrameStackObservation(env, self.frame_stack)\n        return env\n\n    def reset(self) -> AtariState:\n        env = self._make_env()\n        obs, info = env.reset()\n        return AtariState(env=env, observation=obs, done=False, \n                         lives=info.get('lives', 0), score=0)\n\n    def step(self, state: AtariState, action: int) -> Tuple[AtariState, float, bool]:\n        if state.done:\n            return state, 0.0, True\n        \n        obs, reward, terminated, truncated, info = state.env.step(action)\n        done = terminated or truncated\n        \n        new_state = AtariState(\n            env=state.env, observation=obs, done=done,\n            lives=info.get('lives', state.lives),\n            score=state.score + int(reward),\n        )\n        return new_state, float(reward), done\n\n    def encode(self, state: AtariState) -> torch.Tensor:\n        \"\"\"Encode observation as flattened tensor.\"\"\"\n        obs = state.observation\n        if obs.ndim == 4 and obs.shape[-1] == 1:\n            obs = obs.squeeze(-1)  # (4, 84, 84)\n        obs = np.asarray(obs, dtype=np.float32).reshape(-1)\n        return torch.tensor(obs, dtype=torch.float32)\n\n    def legal_actions(self, state: AtariState) -> List[int]:\n        return [] if state.done else list(range(self.action_space_size))\n\n    def action_name(self, action: int) -> str:\n        if action < len(self.action_meanings):\n            return self.action_meanings[action]\n        return f\"Action_{action}\"\n\n\n# Test\ngame = AtariGame(\"Breakout\")\nprint(f\"Game: {game.game_name}\")\nprint(f\"Observation dim: {game.observation_dim}\")\nprint(f\"Actions: {game.action_space_size} - {game.action_meanings}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MuZero Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MuZeroNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    MuZero network with:\n",
    "    - Representation: obs -> hidden state\n",
    "    - Dynamics: (state, action) -> (next_state, reward)\n",
    "    - Prediction: state -> (policy, value)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, \n",
    "                 state_dim: int = 256, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        # Representation network: obs -> state\n",
    "        self.representation = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim),\n",
    "        )\n",
    "        \n",
    "        # Dynamics network: (state, action) -> next_state\n",
    "        self.dynamics = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim),\n",
    "        )\n",
    "        \n",
    "        # Reward prediction\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "        )\n",
    "        \n",
    "        # Policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "        )\n",
    "        \n",
    "        # Value head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    \n",
    "    def initial_inference(self, obs: torch.Tensor):\n",
    "        \"\"\"Initial inference: obs -> (state, policy, value)\"\"\"\n",
    "        state = self.representation(obs)\n",
    "        policy = self.policy_head(state)\n",
    "        value = self.value_head(state).squeeze(-1)\n",
    "        return state, policy, value\n",
    "    \n",
    "    def recurrent_inference(self, state: torch.Tensor, action: torch.Tensor):\n",
    "        \"\"\"Recurrent inference: (state, action) -> (next_state, reward, policy, value)\"\"\"\n",
    "        # One-hot encode action\n",
    "        action_onehot = F.one_hot(action, self.action_dim).float()\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        \n",
    "        next_state = self.dynamics(x)\n",
    "        reward = self.reward_head(next_state).squeeze(-1)\n",
    "        policy = self.policy_head(next_state)\n",
    "        value = self.value_head(next_state).squeeze(-1)\n",
    "        \n",
    "        return next_state, reward, policy, value\n",
    "\n",
    "\n",
    "# Test\n",
    "model = MuZeroNetwork(game.observation_dim, game.action_space_size).to(DEVICE)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "state = game.reset()\n",
    "obs = game.encode(state).unsqueeze(0).to(DEVICE)\n",
    "hidden, policy, value = model.initial_inference(obs)\n",
    "print(f\"Hidden state: {hidden.shape}\")\n",
    "print(f\"Policy: {policy.shape}\")\n",
    "print(f\"Value: {value.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Macro Discovery\n",
    "\n",
    "Macros are **reusable action sequences** discovered from gameplay:\n",
    "- Track action patterns that occur frequently\n",
    "- In deterministic Atari, all transitions have entropy \u2248 0\n",
    "- Patterns that repeat become macro candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MacroOperator:\n",
    "    \"\"\"A discovered macro-operator (reusable action sequence).\"\"\"\n",
    "    actions: tuple\n",
    "    count: int = 0\n",
    "    total_reward: float = 0.0\n",
    "    \n",
    "    @property\n",
    "    def avg_reward(self) -> float:\n",
    "        return self.total_reward / max(self.count, 1)\n",
    "    \n",
    "    @property\n",
    "    def length(self) -> int:\n",
    "        return len(self.actions)\n",
    "\n",
    "\n",
    "class MacroCache:\n",
    "    \"\"\"\n",
    "    Cache for discovering and storing macro-operators.\n",
    "    \n",
    "    Key insight: In deterministic games, frequent action patterns\n",
    "    represent compressible causal structure (rules).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_occurrences: int = 5, min_length: int = 2, max_length: int = 6):\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.candidates = defaultdict(lambda: {\"count\": 0, \"reward\": 0.0})\n",
    "        self.macros: Dict[tuple, MacroOperator] = {}\n",
    "        self.total_patterns_seen = 0\n",
    "    \n",
    "    def add_trajectory(self, actions: List[int], rewards: List[float]):\n",
    "        \"\"\"Extract patterns from a trajectory.\"\"\"\n",
    "        n = len(actions)\n",
    "        \n",
    "        for length in range(self.min_length, min(self.max_length + 1, n + 1)):\n",
    "            for i in range(n - length + 1):\n",
    "                pattern = tuple(actions[i:i + length])\n",
    "                \n",
    "                # Skip uniform patterns (all same action)\n",
    "                if len(set(pattern)) <= 1:\n",
    "                    continue\n",
    "                \n",
    "                # Track pattern\n",
    "                pattern_reward = sum(rewards[i:i + length])\n",
    "                self.candidates[pattern][\"count\"] += 1\n",
    "                self.candidates[pattern][\"reward\"] += pattern_reward\n",
    "                self.total_patterns_seen += 1\n",
    "                \n",
    "                # Promote to macro if seen enough times\n",
    "                if (self.candidates[pattern][\"count\"] >= self.min_occurrences \n",
    "                    and pattern not in self.macros):\n",
    "                    self.macros[pattern] = MacroOperator(\n",
    "                        actions=pattern,\n",
    "                        count=self.candidates[pattern][\"count\"],\n",
    "                        total_reward=self.candidates[pattern][\"reward\"],\n",
    "                    )\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"num_macros\": len(self.macros),\n",
    "            \"num_candidates\": len(self.candidates),\n",
    "            \"total_patterns\": self.total_patterns_seen,\n",
    "        }\n",
    "    \n",
    "    def get_top_macros(self, n: int = 10) -> List[MacroOperator]:\n",
    "        \"\"\"Get top macros by occurrence count.\"\"\"\n",
    "        sorted_macros = sorted(self.macros.values(), key=lambda m: m.count, reverse=True)\n",
    "        return sorted_macros[:n]\n",
    "    \n",
    "    def decode_macro(self, macro: MacroOperator, game: AtariGame) -> str:\n",
    "        \"\"\"Decode macro actions to readable names.\"\"\"\n",
    "        names = [game.action_name(a) for a in macro.actions]\n",
    "        return \" -> \".join(names)\n",
    "\n",
    "\n",
    "# Test\n",
    "cache = MacroCache(min_occurrences=3)\n",
    "print(f\"MacroCache initialized\")\n",
    "print(f\"  Min occurrences: {cache.min_occurrences}\")\n",
    "print(f\"  Pattern length: {cache.min_length}-{cache.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Play with Macro Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(game: AtariGame, model: MuZeroNetwork, device: torch.device,\n",
    "                 max_steps: int = 1000, epsilon: float = 0.1,\n",
    "                 temperature: float = 1.0) -> Tuple[List, List, List, int]:\n",
    "    \"\"\"\n",
    "    Play one episode using the model with exploration.\n",
    "    \n",
    "    Returns: (observations, actions, rewards, final_score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    state = game.reset()\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if state.done:\n",
    "            break\n",
    "        \n",
    "        obs = game.encode(state).to(device).unsqueeze(0)\n",
    "        observations.append(obs.cpu())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, policy_logits, _ = model.initial_inference(obs)\n",
    "        \n",
    "        legal = game.legal_actions(state)\n",
    "        if not legal:\n",
    "            break\n",
    "        \n",
    "        # Epsilon-greedy with softmax\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(legal)\n",
    "        else:\n",
    "            probs = F.softmax(policy_logits[0] / temperature, dim=0).cpu().numpy()\n",
    "            # Mask illegal actions\n",
    "            mask = np.zeros(game.action_space_size)\n",
    "            mask[legal] = 1\n",
    "            probs = probs * mask\n",
    "            if probs.sum() > 0:\n",
    "                probs = probs / probs.sum()\n",
    "                action = np.random.choice(game.action_space_size, p=probs)\n",
    "            else:\n",
    "                action = np.random.choice(legal)\n",
    "        \n",
    "        actions.append(action)\n",
    "        state, reward, done = game.step(state, action)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return observations, actions, rewards, state.score\n",
    "\n",
    "\n",
    "# Test\n",
    "obs, acts, rews, score = play_episode(game, model, DEVICE, max_steps=100, epsilon=1.0)\n",
    "print(f\"Test episode: {len(acts)} steps, score={score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_muzero(game: AtariGame, model: MuZeroNetwork, \n",
    "                 macro_cache: MacroCache, device: torch.device,\n",
    "                 num_iterations: int = 50, episodes_per_iter: int = 3,\n",
    "                 max_steps: int = 1000, batch_size: int = 64,\n",
    "                 lr: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Train MuZero with macro discovery.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_obs = []\n",
    "    replay_actions = []\n",
    "    replay_rewards = []\n",
    "    max_buffer = 10000\n",
    "    \n",
    "    # Tracking\n",
    "    history = {\n",
    "        \"scores\": [], \"lengths\": [], \"losses\": [], \n",
    "        \"macros\": [], \"epsilon\": []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training for {num_iterations} iterations\")\n",
    "    print(f\"  Episodes per iter: {episodes_per_iter}\")\n",
    "    print(f\"  Max steps: {max_steps}\")\n",
    "    print()\n",
    "    \n",
    "    for iteration in tqdm(range(num_iterations), desc=\"Training\"):\n",
    "        # Decay exploration\n",
    "        epsilon = max(0.05, 1.0 - iteration / num_iterations)\n",
    "        temperature = max(0.5, 2.0 - iteration / (num_iterations / 2))\n",
    "        \n",
    "        iter_scores = []\n",
    "        iter_lengths = []\n",
    "        \n",
    "        # Self-play\n",
    "        for _ in range(episodes_per_iter):\n",
    "            obs, acts, rews, score = play_episode(\n",
    "                game, model, device, max_steps=max_steps,\n",
    "                epsilon=epsilon, temperature=temperature\n",
    "            )\n",
    "            \n",
    "            # Store in replay buffer\n",
    "            replay_obs.extend(obs)\n",
    "            replay_actions.extend(acts)\n",
    "            replay_rewards.extend(rews)\n",
    "            \n",
    "            # Discover macros\n",
    "            macro_cache.add_trajectory(acts, rews)\n",
    "            \n",
    "            iter_scores.append(score)\n",
    "            iter_lengths.append(len(acts))\n",
    "        \n",
    "        # Trim buffer\n",
    "        if len(replay_obs) > max_buffer:\n",
    "            replay_obs = replay_obs[-max_buffer:]\n",
    "            replay_actions = replay_actions[-max_buffer:]\n",
    "            replay_rewards = replay_rewards[-max_buffer:]\n",
    "        \n",
    "        # Training step\n",
    "        if len(replay_obs) >= batch_size:\n",
    "            model.train()\n",
    "            \n",
    "            indices = np.random.choice(len(replay_obs), batch_size, replace=False)\n",
    "            batch_obs = torch.cat([replay_obs[i] for i in indices]).to(device)\n",
    "            batch_actions = torch.tensor([replay_actions[i] for i in indices], device=device)\n",
    "            batch_rewards = torch.tensor([replay_rewards[i] for i in indices], \n",
    "                                         device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Forward\n",
    "            hidden, policy_logits, values = model.initial_inference(batch_obs)\n",
    "            \n",
    "            # Losses\n",
    "            # Policy: cross-entropy with action taken\n",
    "            policy_loss = F.cross_entropy(policy_logits, batch_actions)\n",
    "            \n",
    "            # Value: predict reward\n",
    "            value_loss = F.mse_loss(values, batch_rewards)\n",
    "            \n",
    "            # Dynamics consistency\n",
    "            next_hidden, pred_reward, _, _ = model.recurrent_inference(hidden, batch_actions)\n",
    "            reward_loss = F.mse_loss(pred_reward, batch_rewards)\n",
    "            \n",
    "            total_loss = policy_loss + 0.5 * value_loss + 0.5 * reward_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            history[\"losses\"].append(total_loss.item())\n",
    "        \n",
    "        # Record\n",
    "        history[\"scores\"].append(np.mean(iter_scores))\n",
    "        history[\"lengths\"].append(np.mean(iter_lengths))\n",
    "        history[\"macros\"].append(len(macro_cache.macros))\n",
    "        history[\"epsilon\"].append(epsilon)\n",
    "        \n",
    "        # Log\n",
    "        if iteration % 10 == 0:\n",
    "            stats = macro_cache.get_statistics()\n",
    "            print(f\"\\nIter {iteration}: Score={np.mean(iter_scores):.1f}, \"\n",
    "                  f\"Length={np.mean(iter_lengths):.0f}, \"\n",
    "                  f\"Macros={stats['num_macros']}, \"\n",
    "                  f\"eps={epsilon:.2f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train on Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "breakout = AtariGame(\"Breakout\")\n",
    "model = MuZeroNetwork(breakout.observation_dim, breakout.action_space_size).to(DEVICE)\n",
    "macro_cache = MacroCache(min_occurrences=5, min_length=2, max_length=6)\n",
    "\n",
    "print(f\"Training on: {breakout.game_name}\")\n",
    "print(f\"Observation dim: {breakout.observation_dim}\")\n",
    "print(f\"Actions: {breakout.action_space_size}\")\n",
    "print(f\"Deterministic: {breakout.deterministic}\")\n",
    "print()\n",
    "\n",
    "# Train\n",
    "history = train_muzero(\n",
    "    game=breakout,\n",
    "    model=model,\n",
    "    macro_cache=macro_cache,\n",
    "    device=DEVICE,\n",
    "    num_iterations=50,\n",
    "    episodes_per_iter=3,\n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(history[\"scores\"])\n",
    "axes[0, 0].set_xlabel(\"Iteration\")\n",
    "axes[0, 0].set_ylabel(\"Score\")\n",
    "axes[0, 0].set_title(\"Breakout Score\")\n",
    "\n",
    "axes[0, 1].plot(history[\"lengths\"])\n",
    "axes[0, 1].set_xlabel(\"Iteration\")\n",
    "axes[0, 1].set_ylabel(\"Episode Length\")\n",
    "axes[0, 1].set_title(\"Episode Length\")\n",
    "\n",
    "axes[1, 0].plot(history[\"losses\"])\n",
    "axes[1, 0].set_xlabel(\"Training Step\")\n",
    "axes[1, 0].set_ylabel(\"Loss\")\n",
    "axes[1, 0].set_title(\"Training Loss\")\n",
    "\n",
    "axes[1, 1].plot(history[\"macros\"])\n",
    "axes[1, 1].set_xlabel(\"Iteration\")\n",
    "axes[1, 1].set_ylabel(\"Macros\")\n",
    "axes[1, 1].set_title(\"Macro Discovery\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Macro Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MACRO ANALYSIS: Breakout\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = macro_cache.get_statistics()\n",
    "print(f\"\\nMacros discovered: {stats['num_macros']}\")\n",
    "print(f\"Candidate patterns: {stats['num_candidates']}\")\n",
    "print(f\"Total patterns seen: {stats['total_patterns']}\")\n",
    "\n",
    "if stats['num_macros'] > 0:\n",
    "    print(\"\\nTop 15 Macros (by frequency):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, macro in enumerate(macro_cache.get_top_macros(15)):\n",
    "        decoded = macro_cache.decode_macro(macro, breakout)\n",
    "        print(f\"{i+1:2d}. {decoded}\")\n",
    "        print(f\"    Count: {macro.count}, Avg Reward: {macro.avg_reward:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo macros discovered yet.\")\n",
    "    print(\"Try training for more iterations.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Since Breakout is deterministic (no sticky actions), all action \n",
    "sequences are candidates for macro discovery. The patterns above\n",
    "represent common control sequences:\n",
    "\n",
    "- LEFT -> LEFT -> LEFT: Positioning paddle left\n",
    "- RIGHT -> RIGHT -> FIRE: Position and shoot\n",
    "- NOOP -> FIRE: Wait and serve\n",
    "\n",
    "These are temporal abstractions that the agent uses frequently.\n",
    "With more training, longer and more meaningful patterns emerge.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train on Other Games (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Pong\n",
    "\n",
    "# pong = AtariGame(\"Pong\")\n",
    "# pong_model = MuZeroNetwork(pong.observation_dim, pong.action_space_size).to(DEVICE)\n",
    "# pong_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# pong_history = train_muzero(\n",
    "#     game=pong,\n",
    "#     model=pong_model,\n",
    "#     macro_cache=pong_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nPong Macros:\")\n",
    "# for macro in pong_cache.get_top_macros(10):\n",
    "#     print(f\"  {pong_cache.decode_macro(macro, pong)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Space Invaders\n",
    "\n",
    "# invaders = AtariGame(\"SpaceInvaders\")\n",
    "# invaders_model = MuZeroNetwork(invaders.observation_dim, invaders.action_space_size).to(DEVICE)\n",
    "# invaders_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# invaders_history = train_muzero(\n",
    "#     game=invaders,\n",
    "#     model=invaders_model,\n",
    "#     macro_cache=invaders_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nSpace Invaders Macros:\")\n",
    "# for macro in invaders_cache.get_top_macros(10):\n",
    "#     print(f\"  {invaders_cache.decode_macro(macro, invaders)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Deterministic Atari** games are ideal for macro discovery because:\n",
    "   - All transitions have entropy \u2248 0 (no sticky actions)\n",
    "   - Physics are predictable (ball bounces, gravity)\n",
    "   - Agent controls all actions (no opponent uncertainty)\n",
    "\n",
    "2. **Macros as Compressible Causal Structure**:\n",
    "   - Frequent action patterns represent reusable temporal abstractions\n",
    "   - These are \"rules\" discovered from dynamics, not symbolic definitions\n",
    "   - Longer training reveals more meaningful patterns\n",
    "\n",
    "3. **Next Steps**:\n",
    "   - Use macros to accelerate MCTS planning\n",
    "   - Learn macro preconditions (when to apply)\n",
    "   - Transfer macros across similar games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}