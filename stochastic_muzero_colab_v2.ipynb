{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero with Learned Temporal Abstractions\n",
    "## V2: Atari Games Edition\n",
    "\n",
    "This notebook implements **Stochastic MuZero** with macro discovery on **deterministic Atari games**.\n",
    "\n",
    "### Why Atari?\n",
    "- **Single-player**: Agent controls all actions (no opponent uncertainty)\n",
    "- **Deterministic physics**: Ball bounces, gravity, collisions are predictable\n",
    "- **Rich temporal structure**: Positioning sequences, attack patterns, navigation\n",
    "\n",
    "### Core Idea\n",
    "Rules are **compressible causal structure** in the transition dynamics:\n",
    "- If a sequence of transitions is deterministic and repeatable, collapse it into a **macro-operator**\n",
    "- Macros enable faster planning and capture reusable temporal abstractions\n",
    "\n",
    "### Games\n",
    "- **Breakout**: Ball trajectory, paddle positioning\n",
    "- **Pong**: Ball interception patterns\n",
    "- **Space Invaders**: Enemy patterns, shooting sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy gymnasium ale-py tqdm matplotlib -q\n",
    "\n",
    "# Install Atari ROMs (required for ALE games)\n",
    "!pip install autorom[accept-rom-license] -q\n",
    "!python -m atari_py.import_roms . 2>/dev/null || true\n",
    "\n",
    "# Alternative ROM installation if above fails\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(['AutoROM', '--accept-license'], capture_output=True, timeout=60)\n",
    "    print(\"ROMs installed via AutoROM\")\n",
    "except:\n",
    "    print(\"AutoROM not available, trying alternative...\")\n",
    "    !pip install ale-py[roms] -q 2>/dev/null || true\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Atari environment\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "# Register ALE environments\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "try:\n",
    "    env = gym.make(\"ALE/Breakout-v5\")\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Breakout works!\")\n",
    "    print(f\"  Observation: {obs.shape}\")\n",
    "    print(f\"  Actions: {env.action_space.n} - {env.unwrapped.get_action_meanings()}\")\n",
    "    env.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nTrying to fix...\")\n",
    "    !pip install 'gymnasium[atari,accept-rom-license]' -q\n",
    "    print(\"\\nPlease restart runtime and run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Atari Game Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass\nfrom typing import List, Dict, Optional, Tuple, Any\nimport gymnasium as gym\nfrom gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n\n# Register ALE environments\nimport ale_py\ngym.register_envs(ale_py)\n\n\n@dataclass\nclass AtariState:\n    \"\"\"Wrapper for Atari game state.\"\"\"\n    env: gym.Env\n    observation: np.ndarray\n    done: bool = False\n    lives: int = 0\n    score: int = 0\n\n\nclass AtariGame:\n    \"\"\"\n    Atari game wrapper with deterministic mode for macro discovery.\n    \n    Key settings:\n    - repeat_action_probability=0: No sticky actions (deterministic)\n    - noop_max=0: No random initial noops\n    - 84x84 grayscale, 4 stacked frames\n    \"\"\"\n    \n    ACTIONS = {\n        0: \"NOOP\", 1: \"FIRE\", 2: \"UP\", 3: \"RIGHT\", 4: \"LEFT\", 5: \"DOWN\",\n        6: \"UPRIGHT\", 7: \"UPLEFT\", 8: \"DOWNRIGHT\", 9: \"DOWNLEFT\",\n        10: \"UPFIRE\", 11: \"RIGHTFIRE\", 12: \"LEFTFIRE\", 13: \"DOWNFIRE\",\n    }\n\n    def __init__(self, game_name: str = \"Breakout\", frame_stack: int = 4, \n                 frame_skip: int = 4, deterministic: bool = True):\n        self.game_name = game_name\n        self.frame_stack = frame_stack\n        self.frame_skip = frame_skip\n        self.deterministic = deterministic\n        \n        self._env = self._make_env()\n        self.action_space_size = self._env.action_space.n\n        self.action_meanings = self._env.unwrapped.get_action_meanings()\n        self.observation_dim = 84 * 84 * frame_stack  # Flattened\n\n    def _make_env(self) -> gym.Env:\n        env = gym.make(\n            f\"ALE/{self.game_name}-v5\",\n            repeat_action_probability=0.0 if self.deterministic else 0.25,\n            frameskip=1,\n        )\n        env = AtariPreprocessing(\n            env,\n            noop_max=0 if self.deterministic else 30,\n            frame_skip=self.frame_skip,\n            screen_size=84,\n            grayscale_obs=True,\n            grayscale_newaxis=True,\n            scale_obs=True,\n        )\n        env = FrameStackObservation(env, self.frame_stack)\n        return env\n\n    def reset(self) -> AtariState:\n        env = self._make_env()\n        obs, info = env.reset()\n        return AtariState(env=env, observation=obs, done=False, \n                         lives=info.get('lives', 0), score=0)\n\n    def step(self, state: AtariState, action: int) -> Tuple[AtariState, float, bool]:\n        if state.done:\n            return state, 0.0, True\n        \n        obs, reward, terminated, truncated, info = state.env.step(action)\n        done = terminated or truncated\n        \n        new_state = AtariState(\n            env=state.env, observation=obs, done=done,\n            lives=info.get('lives', state.lives),\n            score=state.score + int(reward),\n        )\n        return new_state, float(reward), done\n\n    def encode(self, state: AtariState) -> torch.Tensor:\n        \"\"\"Encode observation as flattened tensor.\"\"\"\n        obs = state.observation\n        if obs.ndim == 4 and obs.shape[-1] == 1:\n            obs = obs.squeeze(-1)  # (4, 84, 84)\n        obs = np.asarray(obs, dtype=np.float32).reshape(-1)\n        return torch.tensor(obs, dtype=torch.float32)\n\n    def legal_actions(self, state: AtariState) -> List[int]:\n        return [] if state.done else list(range(self.action_space_size))\n\n    def action_name(self, action: int) -> str:\n        if action < len(self.action_meanings):\n            return self.action_meanings[action]\n        return f\"Action_{action}\"\n\n\n# Test\ngame = AtariGame(\"Breakout\")\nprint(f\"Game: {game.game_name}\")\nprint(f\"Observation dim: {game.observation_dim}\")\nprint(f\"Actions: {game.action_space_size} - {game.action_meanings}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MuZero Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MuZeroNetwork(nn.Module):\n    \"\"\"\n    MuZero network with entropy estimation for macro discovery.\n    \n    Key addition: The dynamics network outputs both next_state AND\n    a confidence/entropy estimate. Low entropy = deterministic transition.\n    \"\"\"\n    \n    def __init__(self, obs_dim: int, action_dim: int, \n                 state_dim: int = 256, hidden_dim: int = 256):\n        super().__init__()\n        self.action_dim = action_dim\n        self.state_dim = state_dim\n        \n        # Representation network: obs -> state\n        self.representation = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim),\n        )\n        \n        # Dynamics network: (state, action) -> next_state\n        self.dynamics = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim),\n        )\n        \n        # Entropy head: predicts uncertainty of transition\n        # Low entropy = deterministic (macro-able)\n        # High entropy = stochastic (can't compress)\n        self.entropy_head = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Softplus(),  # Ensure positive entropy\n        )\n        \n        # Reward prediction\n        self.reward_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1),\n        )\n        \n        # Policy head\n        self.policy_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n        )\n        \n        # Value head\n        self.value_head = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n        )\n    \n    def initial_inference(self, obs: torch.Tensor):\n        \"\"\"Initial inference: obs -> (state, policy, value)\"\"\"\n        state = self.representation(obs)\n        policy = self.policy_head(state)\n        value = self.value_head(state).squeeze(-1)\n        return state, policy, value\n    \n    def recurrent_inference(self, state: torch.Tensor, action: torch.Tensor):\n        \"\"\"\n        Recurrent inference with entropy estimation.\n        \n        Returns: (next_state, reward, policy, value, entropy)\n        \n        entropy: Model's uncertainty about this transition.\n                 Low = deterministic, good for macros.\n                 High = stochastic, can't compress.\n        \"\"\"\n        # One-hot encode action\n        action_onehot = F.one_hot(action, self.action_dim).float()\n        x = torch.cat([state, action_onehot], dim=-1)\n        \n        next_state = self.dynamics(x)\n        entropy = self.entropy_head(x).squeeze(-1)  # Predicted transition entropy\n        \n        reward = self.reward_head(next_state).squeeze(-1)\n        policy = self.policy_head(next_state)\n        value = self.value_head(next_state).squeeze(-1)\n        \n        return next_state, reward, policy, value, entropy\n\n\n# Test\nmodel = MuZeroNetwork(game.observation_dim, game.action_space_size).to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Test forward pass\nstate = game.reset()\nobs = game.encode(state).unsqueeze(0).to(DEVICE)\nhidden, policy, value = model.initial_inference(obs)\n\n# Test dynamics with entropy\naction = torch.tensor([1], device=DEVICE)\nnext_hidden, reward, next_policy, next_value, entropy = model.recurrent_inference(hidden, action)\nprint(f\"Hidden state: {hidden.shape}\")\nprint(f\"Transition entropy: {entropy.item():.4f}\")\nprint(f\"  (Low entropy = deterministic transition = macro candidate)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Macro Discovery\n",
    "\n",
    "Macros are **reusable action sequences** discovered from gameplay:\n",
    "- Track action patterns that occur frequently\n",
    "- In deterministic Atari, all transitions have entropy \u2248 0\n",
    "- Patterns that repeat become macro candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from collections import defaultdict\n\n\n@dataclass\nclass MacroOperator:\n    \"\"\"A discovered macro-operator (low-entropy action sequence).\"\"\"\n    actions: tuple\n    avg_entropy: float  # Average entropy across transitions\n    count: int = 0\n    total_reward: float = 0.0\n    \n    @property\n    def avg_reward(self) -> float:\n        return self.total_reward / max(self.count, 1)\n    \n    @property\n    def length(self) -> int:\n        return len(self.actions)\n    \n    @property\n    def is_deterministic(self) -> bool:\n        \"\"\"True if this macro represents a deterministic sequence.\"\"\"\n        return self.avg_entropy < 0.1\n\n\nclass MacroCache:\n    \"\"\"\n    Entropy-based macro discovery from MCTS rollouts.\n    \n    Key insight: Macros are sequences where the model predicts\n    transitions with LOW ENTROPY (high confidence). These represent\n    compressible causal structure - the \"rules\" of the game.\n    \n    Discovery process:\n    1. During MCTS, track (state, action, entropy) for each transition\n    2. Find sequences where ALL transitions have entropy < threshold\n    3. These are macro candidates (deterministic chains)\n    4. Promote to macros when seen multiple times\n    \"\"\"\n    \n    def __init__(self, entropy_threshold: float = 0.5, \n                 min_occurrences: int = 3, \n                 min_length: int = 2, \n                 max_length: int = 6):\n        self.entropy_threshold = entropy_threshold\n        self.min_occurrences = min_occurrences\n        self.min_length = min_length\n        self.max_length = max_length\n        \n        # Candidates: pattern -> {count, total_entropy, total_reward}\n        self.candidates = defaultdict(lambda: {\n            \"count\": 0, \"total_entropy\": 0.0, \"total_reward\": 0.0\n        })\n        self.macros: Dict[tuple, MacroOperator] = {}\n        \n        # Statistics\n        self.total_transitions = 0\n        self.low_entropy_transitions = 0\n    \n    def add_rollout(self, actions: List[int], entropies: List[float], \n                    rewards: List[float]):\n        \"\"\"\n        Extract macro candidates from an MCTS rollout.\n        \n        Only considers sequences where ALL transitions have\n        entropy below threshold (deterministic chains).\n        \"\"\"\n        n = len(actions)\n        if n < self.min_length:\n            return\n        \n        self.total_transitions += n\n        self.low_entropy_transitions += sum(1 for e in entropies if e < self.entropy_threshold)\n        \n        # Find low-entropy windows\n        for length in range(self.min_length, min(self.max_length + 1, n + 1)):\n            for i in range(n - length + 1):\n                window_entropies = entropies[i:i + length]\n                \n                # ALL transitions must be low-entropy (deterministic)\n                if all(e < self.entropy_threshold for e in window_entropies):\n                    pattern = tuple(actions[i:i + length])\n                    \n                    # Skip uniform patterns\n                    if len(set(pattern)) <= 1:\n                        continue\n                    \n                    avg_entropy = sum(window_entropies) / length\n                    pattern_reward = sum(rewards[i:i + length])\n                    \n                    self.candidates[pattern][\"count\"] += 1\n                    self.candidates[pattern][\"total_entropy\"] += avg_entropy\n                    self.candidates[pattern][\"total_reward\"] += pattern_reward\n                    \n                    # Promote to macro\n                    if (self.candidates[pattern][\"count\"] >= self.min_occurrences \n                        and pattern not in self.macros):\n                        c = self.candidates[pattern]\n                        self.macros[pattern] = MacroOperator(\n                            actions=pattern,\n                            avg_entropy=c[\"total_entropy\"] / c[\"count\"],\n                            count=c[\"count\"],\n                            total_reward=c[\"total_reward\"],\n                        )\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        deterministic_rate = (self.low_entropy_transitions / max(self.total_transitions, 1))\n        return {\n            \"num_macros\": len(self.macros),\n            \"num_candidates\": len(self.candidates),\n            \"total_transitions\": self.total_transitions,\n            \"low_entropy_transitions\": self.low_entropy_transitions,\n            \"deterministic_rate\": deterministic_rate,\n        }\n    \n    def get_top_macros(self, n: int = 10) -> List[MacroOperator]:\n        \"\"\"Get top macros by count (most frequent deterministic patterns).\"\"\"\n        sorted_macros = sorted(self.macros.values(), key=lambda m: m.count, reverse=True)\n        return sorted_macros[:n]\n    \n    def decode_macro(self, macro: MacroOperator, game: AtariGame) -> str:\n        names = [game.action_name(a) for a in macro.actions]\n        return \" -> \".join(names)\n\n\n# Test\ncache = MacroCache(entropy_threshold=0.5, min_occurrences=3)\nprint(f\"MacroCache initialized\")\nprint(f\"  Entropy threshold: {cache.entropy_threshold}\")\nprint(f\"  Min occurrences: {cache.min_occurrences}\")\nprint(f\"  Pattern length: {cache.min_length}-{cache.max_length}\")\nprint()\nprint(\"Macro discovery now based on MODEL ENTROPY, not just action frequency!\")\nprint(\"Low entropy = model confidently predicts transition = deterministic = macro\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Play with Macro Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def play_episode(game: AtariGame, model: MuZeroNetwork, device: torch.device,\n                 max_steps: int = 1000, epsilon: float = 0.1,\n                 temperature: float = 1.0) -> Tuple[List, List, List, List, int]:\n    \"\"\"\n    Play one episode, tracking model entropy for each transition.\n    \n    Returns: (observations, actions, rewards, entropies, final_score)\n    \n    The entropies list contains the model's uncertainty for each transition.\n    Low entropy = deterministic = good for macro discovery.\n    \"\"\"\n    model.eval()\n    state = game.reset()\n    \n    observations = []\n    actions = []\n    rewards = []\n    entropies = []  # Model's predicted entropy for each transition\n    \n    hidden_state = None\n    \n    for step in range(max_steps):\n        if state.done:\n            break\n        \n        obs = game.encode(state).to(device).unsqueeze(0)\n        observations.append(obs.cpu())\n        \n        with torch.no_grad():\n            hidden_state, policy_logits, _ = model.initial_inference(obs)\n        \n        legal = game.legal_actions(state)\n        if not legal:\n            break\n        \n        # Epsilon-greedy with softmax\n        if np.random.random() < epsilon:\n            action = np.random.choice(legal)\n        else:\n            probs = F.softmax(policy_logits[0] / temperature, dim=0).cpu().numpy()\n            mask = np.zeros(game.action_space_size)\n            mask[legal] = 1\n            probs = probs * mask\n            if probs.sum() > 0:\n                probs = probs / probs.sum()\n                action = np.random.choice(game.action_space_size, p=probs)\n            else:\n                action = np.random.choice(legal)\n        \n        # Get entropy for this transition from the model\n        with torch.no_grad():\n            action_tensor = torch.tensor([action], device=device)\n            _, _, _, _, entropy = model.recurrent_inference(hidden_state, action_tensor)\n            entropies.append(entropy.item())\n        \n        actions.append(action)\n        state, reward, done = game.step(state, action)\n        rewards.append(reward)\n    \n    return observations, actions, rewards, entropies, state.score\n\n\n# Test episode with entropy tracking\nobs, acts, rews, ents, score = play_episode(game, model, DEVICE, max_steps=100, epsilon=1.0)\nprint(f\"Test episode: {len(acts)} steps, score={score}\")\nprint(f\"Entropy stats: min={min(ents):.3f}, max={max(ents):.3f}, mean={np.mean(ents):.3f}\")\nprint(f\"  (Untrained model has high entropy - will decrease as it learns)\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.optim as optim\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\n\ndef train_muzero(game: AtariGame, model: MuZeroNetwork, \n                 macro_cache: MacroCache, device: torch.device,\n                 num_iterations: int = 50, episodes_per_iter: int = 3,\n                 max_steps: int = 1000, batch_size: int = 64,\n                 lr: float = 1e-4):\n    \"\"\"\n    Train MuZero with entropy-based macro discovery.\n    \n    Key insight: As the model learns the game dynamics, its\n    prediction entropy decreases for deterministic transitions.\n    These low-entropy sequences become macro candidates.\n    \"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Replay buffer\n    replay_obs = []\n    replay_actions = []\n    replay_rewards = []\n    replay_entropies = []\n    max_buffer = 10000\n    \n    # Tracking\n    history = {\n        \"scores\": [], \"lengths\": [], \"losses\": [], \n        \"macros\": [], \"avg_entropy\": [], \"deterministic_rate\": []\n    }\n    \n    print(f\"Training for {num_iterations} iterations\")\n    print(f\"Macro discovery based on MODEL ENTROPY (not action frequency)\")\n    print(f\"  Entropy threshold: {macro_cache.entropy_threshold}\")\n    print()\n    \n    for iteration in tqdm(range(num_iterations), desc=\"Training\"):\n        epsilon = max(0.05, 1.0 - iteration / num_iterations)\n        temperature = max(0.5, 2.0 - iteration / (num_iterations / 2))\n        \n        iter_scores = []\n        iter_lengths = []\n        iter_entropies = []\n        \n        # Self-play with entropy tracking\n        for _ in range(episodes_per_iter):\n            obs, acts, rews, ents, score = play_episode(\n                game, model, device, max_steps=max_steps,\n                epsilon=epsilon, temperature=temperature\n            )\n            \n            replay_obs.extend(obs)\n            replay_actions.extend(acts)\n            replay_rewards.extend(rews)\n            replay_entropies.extend(ents)\n            \n            # Discover macros from LOW-ENTROPY sequences\n            macro_cache.add_rollout(acts, ents, rews)\n            \n            iter_scores.append(score)\n            iter_lengths.append(len(acts))\n            iter_entropies.extend(ents)\n        \n        # Trim buffer\n        if len(replay_obs) > max_buffer:\n            replay_obs = replay_obs[-max_buffer:]\n            replay_actions = replay_actions[-max_buffer:]\n            replay_rewards = replay_rewards[-max_buffer:]\n            replay_entropies = replay_entropies[-max_buffer:]\n        \n        # Training step\n        if len(replay_obs) >= batch_size:\n            model.train()\n            \n            indices = np.random.choice(len(replay_obs), batch_size, replace=False)\n            batch_obs = torch.cat([replay_obs[i] for i in indices]).to(device)\n            batch_actions = torch.tensor([replay_actions[i] for i in indices], device=device)\n            batch_rewards = torch.tensor([replay_rewards[i] for i in indices], \n                                         device=device, dtype=torch.float32)\n            \n            # Forward\n            hidden, policy_logits, values = model.initial_inference(batch_obs)\n            \n            # Policy loss\n            policy_loss = F.cross_entropy(policy_logits, batch_actions)\n            \n            # Value loss\n            value_loss = F.mse_loss(values, batch_rewards)\n            \n            # Dynamics + entropy loss\n            next_hidden, pred_reward, _, _, pred_entropy = model.recurrent_inference(\n                hidden, batch_actions\n            )\n            reward_loss = F.mse_loss(pred_reward, batch_rewards)\n            \n            # Entropy regularization: encourage low entropy for consistent dynamics\n            # But not too low (prevent collapse)\n            entropy_target = 0.1  # Target entropy for deterministic Atari\n            entropy_loss = F.mse_loss(pred_entropy, \n                                      torch.full_like(pred_entropy, entropy_target))\n            \n            total_loss = policy_loss + 0.5 * value_loss + 0.5 * reward_loss + 0.1 * entropy_loss\n            \n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            history[\"losses\"].append(total_loss.item())\n        \n        # Record\n        history[\"scores\"].append(np.mean(iter_scores))\n        history[\"lengths\"].append(np.mean(iter_lengths))\n        history[\"macros\"].append(len(macro_cache.macros))\n        history[\"avg_entropy\"].append(np.mean(iter_entropies) if iter_entropies else 0)\n        \n        stats = macro_cache.get_statistics()\n        history[\"deterministic_rate\"].append(stats[\"deterministic_rate\"])\n        \n        if iteration % 10 == 0:\n            print(f\"\\nIter {iteration}: Score={np.mean(iter_scores):.1f}, \"\n                  f\"Entropy={np.mean(iter_entropies):.3f}, \"\n                  f\"DetRate={stats['deterministic_rate']:.1%}, \"\n                  f\"Macros={stats['num_macros']}\")\n    \n    return history\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train on Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "breakout = AtariGame(\"Breakout\")\n",
    "model = MuZeroNetwork(breakout.observation_dim, breakout.action_space_size).to(DEVICE)\n",
    "macro_cache = MacroCache(min_occurrences=5, min_length=2, max_length=6)\n",
    "\n",
    "print(f\"Training on: {breakout.game_name}\")\n",
    "print(f\"Observation dim: {breakout.observation_dim}\")\n",
    "print(f\"Actions: {breakout.action_space_size}\")\n",
    "print(f\"Deterministic: {breakout.deterministic}\")\n",
    "print()\n",
    "\n",
    "# Train\n",
    "history = train_muzero(\n",
    "    game=breakout,\n",
    "    model=model,\n",
    "    macro_cache=macro_cache,\n",
    "    device=DEVICE,\n",
    "    num_iterations=50,\n",
    "    episodes_per_iter=3,\n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training results\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\naxes[0, 0].plot(history[\"scores\"])\naxes[0, 0].set_xlabel(\"Iteration\")\naxes[0, 0].set_ylabel(\"Score\")\naxes[0, 0].set_title(\"Breakout Score\")\n\naxes[0, 1].plot(history[\"lengths\"])\naxes[0, 1].set_xlabel(\"Iteration\")\naxes[0, 1].set_ylabel(\"Episode Length\")\naxes[0, 1].set_title(\"Episode Length\")\n\naxes[0, 2].plot(history[\"losses\"])\naxes[0, 2].set_xlabel(\"Training Step\")\naxes[0, 2].set_ylabel(\"Loss\")\naxes[0, 2].set_title(\"Training Loss\")\n\naxes[1, 0].plot(history[\"macros\"])\naxes[1, 0].set_xlabel(\"Iteration\")\naxes[1, 0].set_ylabel(\"Macros\")\naxes[1, 0].set_title(\"Macro Discovery\")\n\naxes[1, 1].plot(history[\"avg_entropy\"])\naxes[1, 1].set_xlabel(\"Iteration\")\naxes[1, 1].set_ylabel(\"Entropy\")\naxes[1, 1].set_title(\"Model Entropy (lower = more deterministic)\")\naxes[1, 1].axhline(y=macro_cache.entropy_threshold, color='r', linestyle='--', label='Threshold')\naxes[1, 1].legend()\n\naxes[1, 2].plot(history[\"deterministic_rate\"])\naxes[1, 2].set_xlabel(\"Iteration\")\naxes[1, 2].set_ylabel(\"Rate\")\naxes[1, 2].set_title(\"Deterministic Transition Rate\")\naxes[1, 2].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Macro Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"ENTROPY-BASED MACRO ANALYSIS: Breakout\")\nprint(\"=\" * 60)\n\nstats = macro_cache.get_statistics()\nprint(f\"\\nTotal transitions analyzed: {stats['total_transitions']}\")\nprint(f\"Low-entropy transitions: {stats['low_entropy_transitions']} ({stats['deterministic_rate']:.1%})\")\nprint(f\"\\nMacros discovered: {stats['num_macros']}\")\nprint(f\"Candidate patterns: {stats['num_candidates']}\")\n\nif stats['num_macros'] > 0:\n    print(\"\\nTop 15 Macros (by frequency of low-entropy occurrence):\")\n    print(\"-\" * 60)\n    \n    for i, macro in enumerate(macro_cache.get_top_macros(15)):\n        decoded = macro_cache.decode_macro(macro, breakout)\n        det_str = \"DETERMINISTIC\" if macro.is_deterministic else \"partial\"\n        print(f\"{i+1:2d}. {decoded}\")\n        print(f\"    Count: {macro.count}, Avg Entropy: {macro.avg_entropy:.4f} [{det_str}]\")\n        print(f\"    Avg Reward: {macro.avg_reward:.2f}\")\nelse:\n    print(\"\\nNo macros discovered yet.\")\n    print(\"This could mean:\")\n    print(\"  1. Model hasn't learned deterministic dynamics yet (train longer)\")\n    print(\"  2. Entropy threshold too low (try 1.0 or higher)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"KEY INSIGHT\")\nprint(\"=\" * 60)\nprint(\"\"\"\nMacros are now discovered based on MODEL ENTROPY, not just action frequency.\n\nWhen the model learns to predict transitions with HIGH CONFIDENCE (low entropy),\nthose sequences become macro candidates. This captures:\n\n- Deterministic physics (ball trajectory after paddle hit)\n- Predictable game rules (FIRE always launches ball)\n- Learned control patterns (paddle positioning sequences)\n\nThe \"deterministic rate\" shows what fraction of transitions the model\nconsiders predictable. As training progresses, this should increase\nfor deterministic games like Breakout.\n\"\"\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train on Other Games (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Pong\n",
    "\n",
    "# pong = AtariGame(\"Pong\")\n",
    "# pong_model = MuZeroNetwork(pong.observation_dim, pong.action_space_size).to(DEVICE)\n",
    "# pong_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# pong_history = train_muzero(\n",
    "#     game=pong,\n",
    "#     model=pong_model,\n",
    "#     macro_cache=pong_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nPong Macros:\")\n",
    "# for macro in pong_cache.get_top_macros(10):\n",
    "#     print(f\"  {pong_cache.decode_macro(macro, pong)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train on Space Invaders\n",
    "\n",
    "# invaders = AtariGame(\"SpaceInvaders\")\n",
    "# invaders_model = MuZeroNetwork(invaders.observation_dim, invaders.action_space_size).to(DEVICE)\n",
    "# invaders_cache = MacroCache(min_occurrences=5)\n",
    "\n",
    "# invaders_history = train_muzero(\n",
    "#     game=invaders,\n",
    "#     model=invaders_model,\n",
    "#     macro_cache=invaders_cache,\n",
    "#     device=DEVICE,\n",
    "#     num_iterations=50,\n",
    "#     episodes_per_iter=3,\n",
    "# )\n",
    "\n",
    "# print(\"\\nSpace Invaders Macros:\")\n",
    "# for macro in invaders_cache.get_top_macros(10):\n",
    "#     print(f\"  {invaders_cache.decode_macro(macro, invaders)} (count={macro.count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Deterministic Atari** games are ideal for macro discovery because:\n",
    "   - All transitions have entropy \u2248 0 (no sticky actions)\n",
    "   - Physics are predictable (ball bounces, gravity)\n",
    "   - Agent controls all actions (no opponent uncertainty)\n",
    "\n",
    "2. **Macros as Compressible Causal Structure**:\n",
    "   - Frequent action patterns represent reusable temporal abstractions\n",
    "   - These are \"rules\" discovered from dynamics, not symbolic definitions\n",
    "   - Longer training reveals more meaningful patterns\n",
    "\n",
    "3. **Next Steps**:\n",
    "   - Use macros to accelerate MCTS planning\n",
    "   - Learn macro preconditions (when to apply)\n",
    "   - Transfer macros across similar games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}