{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero with Learned Temporal Abstractions\n",
    "\n",
    "This notebook implements **Stochastic MuZero** with a novel approach to discovering **rules as compressible causal structure**.\n",
    "\n",
    "## Core Idea\n",
    "Instead of treating rules as explicit symbols, we discover them operationally:\n",
    "- **Rules** = sequences of transitions that are deterministic, repeatable, and compositional\n",
    "- These can be collapsed into **macro-operators** for faster planning\n",
    "- The model learns to separate **deterministic dynamics** (rules) from **stochastic chance** (environment randomness)\n",
    "\n",
    "## Architecture\n",
    "- **Afterstate separation**: `s → afterstate → chance → s'`\n",
    "- **Entropy tracking**: Identifies which transitions are deterministic\n",
    "- **Macro cache**: Stores and reuses discovered temporal abstractions\n",
    "- **Hierarchical MCTS**: Plans at multiple temporal scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy pyyaml tqdm tensorboard matplotlib -q\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure\n",
    "import os\n",
    "import numpy as np\n",
    "directories = ['config', 'games', 'networks', 'mcts', 'training', 'utils', 'runs']\n",
    "for d in directories:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "print(\"Project structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/__init__.py\n",
    "from .config import Config, load_config\n",
    "from .support import scalar_to_support, support_to_scalar\n",
    "\n",
    "__all__ = [\"Config\", \"load_config\", \"scalar_to_support\", \"support_to_scalar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/config.py\n",
    "\"\"\"Configuration management for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Stochastic MuZero with macro-operator discovery.\"\"\"\n",
    "\n",
    "    # Game settings\n",
    "    game: str = \"2048\"\n",
    "    action_space_size: int = 4\n",
    "    chance_space_size: int = 33\n",
    "\n",
    "    # Network architecture\n",
    "    state_dim: int = 256\n",
    "    hidden_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    observation_dim: int = 496\n",
    "    support_size: int = 31\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 256\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    max_grad_norm: float = 5.0\n",
    "    num_unroll_steps: int = 5\n",
    "    td_steps: int = 10\n",
    "    discount: float = 0.997\n",
    "\n",
    "    # Self-play\n",
    "    num_simulations: int = 50\n",
    "    num_actors: int = 4\n",
    "    max_moves: int = 10000\n",
    "    temperature_init: float = 1.0\n",
    "    temperature_final: float = 0.1\n",
    "    temperature_decay_steps: int = 10000\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer_size: int = 100000\n",
    "    priority_alpha: float = 1.0\n",
    "    priority_beta: float = 1.0\n",
    "\n",
    "    # MCTS\n",
    "    pb_c_base: float = 19652.0\n",
    "    pb_c_init: float = 1.25\n",
    "    root_dirichlet_alpha: float = 0.3\n",
    "    root_exploration_fraction: float = 0.25\n",
    "\n",
    "    # Macro-operator discovery\n",
    "    entropy_threshold: float = 0.1\n",
    "    composition_threshold: float = 0.01\n",
    "    min_macro_length: int = 2\n",
    "    max_macro_length: int = 8\n",
    "    macro_confidence_decay: float = 0.9\n",
    "    macro_confidence_boost: float = 1.05\n",
    "    max_macros: int = 1000\n",
    "\n",
    "    # Chance node handling\n",
    "    chance_entropy_threshold: float = 0.5\n",
    "    top_k_chances: int = 5\n",
    "\n",
    "    # Logging\n",
    "    log_interval: int = 100\n",
    "    save_interval: int = 1000\n",
    "    eval_interval: int = 500\n",
    "\n",
    "    # Device\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {k: getattr(self, k) for k in self.__dataclass_fields__}\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict[str, Any]) -> \"Config\":\n",
    "        valid_keys = cls.__dataclass_fields__.keys()\n",
    "        filtered = {k: v for k, v in d.items() if k in valid_keys}\n",
    "        return cls(**filtered)\n",
    "\n",
    "\n",
    "def load_config(config_path: Optional[str] = None, **overrides) -> Config:\n",
    "    config_dict = {}\n",
    "    if config_path is not None:\n",
    "        path = Path(config_path)\n",
    "        if path.exists():\n",
    "            with open(path, \"r\") as f:\n",
    "                config_dict = yaml.safe_load(f) or {}\n",
    "    config_dict.update(overrides)\n",
    "    return Config.from_dict(config_dict)\n",
    "\n",
    "\n",
    "def save_config(config: Config, path: str) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(config.to_dict(), f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/support.py\n",
    "\"\"\"Support-based scalar transformations for MuZero.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def scalar_to_support(x: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Transform scalar values to categorical support representation.\"\"\"\n",
    "    eps = 0.001\n",
    "    transformed = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + eps * x\n",
    "    transformed = torch.clamp(transformed, -support_size, support_size)\n",
    "    shifted = transformed + support_size\n",
    "    \n",
    "    floor_idx = shifted.floor().long()\n",
    "    ceil_idx = floor_idx + 1\n",
    "    floor_idx = torch.clamp(floor_idx, 0, 2 * support_size)\n",
    "    ceil_idx = torch.clamp(ceil_idx, 0, 2 * support_size)\n",
    "    \n",
    "    ceil_weight = shifted - floor_idx.float()\n",
    "    floor_weight = 1.0 - ceil_weight\n",
    "    \n",
    "    batch_shape = x.shape\n",
    "    support_dim = 2 * support_size + 1\n",
    "    \n",
    "    flat_floor = floor_idx.flatten()\n",
    "    flat_ceil = ceil_idx.flatten()\n",
    "    flat_floor_weight = floor_weight.flatten()\n",
    "    flat_ceil_weight = ceil_weight.flatten()\n",
    "    \n",
    "    output = torch.zeros(flat_floor.numel(), support_dim, device=x.device, dtype=x.dtype)\n",
    "    output.scatter_add_(1, flat_floor.unsqueeze(1), flat_floor_weight.unsqueeze(1))\n",
    "    output.scatter_add_(1, flat_ceil.unsqueeze(1), flat_ceil_weight.unsqueeze(1))\n",
    "    \n",
    "    return output.view(*batch_shape, support_dim)\n",
    "\n",
    "\n",
    "def support_to_scalar(probs: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Transform categorical support distribution back to scalar values.\"\"\"\n",
    "    support = torch.arange(-support_size, support_size + 1, device=probs.device, dtype=probs.dtype)\n",
    "    expected = (probs * support).sum(dim=-1)\n",
    "    eps = 0.001\n",
    "    sign = torch.sign(expected)\n",
    "    abs_expected = torch.abs(expected)\n",
    "    return sign * ((abs_expected + 1).square() - 1) / (1 + 2 * eps * (abs_expected + 1))\n",
    "\n",
    "\n",
    "def compute_cross_entropy_loss(pred_logits: torch.Tensor, target_scalar: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Compute cross-entropy loss between predicted logits and target scalar.\"\"\"\n",
    "    target_probs = scalar_to_support(target_scalar, support_size)\n",
    "    log_probs = F.log_softmax(pred_logits, dim=-1)\n",
    "    loss = -(target_probs * log_probs).sum(dim=-1)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Game Environment (2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile games/__init__.py\n",
    "from .base import Game\n",
    "from .game_2048 import Game2048\n",
    "\n",
    "__all__ = [\"Game\", \"Game2048\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile games/base.py\n",
    "\"\"\"Abstract base class for game environments.\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple, TypeVar\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "State = TypeVar(\"State\")\n",
    "Afterstate = TypeVar(\"Afterstate\")\n",
    "ChanceOutcome = int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StepResult:\n",
    "    \"\"\"Result of applying an action and chance outcome.\"\"\"\n",
    "    afterstate: Any\n",
    "    next_state: Any\n",
    "    reward: float\n",
    "    done: bool\n",
    "    chance_outcome: int\n",
    "    info: Dict[str, Any]\n",
    "\n",
    "\n",
    "class Game(ABC):\n",
    "    \"\"\"Abstract base class for game environments with afterstate separation.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def action_space_size(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def chance_space_size(self) -> int:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def observation_shape(self) -> Tuple[int, ...]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self) -> State:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def clone_state(self, state: State) -> State:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def legal_actions(self, state: State) -> List[int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply_action(self, state: State, action: int) -> Tuple[Afterstate, float, Dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def sample_chance(self, afterstate: Afterstate, info: Dict[str, Any]) -> ChanceOutcome:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_chance_distribution(self, afterstate: Afterstate, info: Dict[str, Any]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def apply_chance(self, afterstate: Afterstate, chance: ChanceOutcome) -> State:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self, state: State) -> bool:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode_state(self, state: State) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode_afterstate(self, afterstate: Afterstate) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    def step(self, state: State, action: int) -> StepResult:\n",
    "        \"\"\"Full step: apply action, sample chance, apply chance.\"\"\"\n",
    "        afterstate, reward, info = self.apply_action(state, action)\n",
    "        chance = self.sample_chance(afterstate, info)\n",
    "        next_state = self.apply_chance(afterstate, chance)\n",
    "        done = self.is_terminal(next_state)\n",
    "        return StepResult(afterstate=afterstate, next_state=next_state, reward=reward, done=done, chance_outcome=chance, info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile games/game_2048.py\n",
    "\"\"\"2048 game environment with afterstate separation.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from .base import Game, ChanceOutcome\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State2048:\n",
    "    \"\"\"State of a 2048 game.\"\"\"\n",
    "    grid: np.ndarray\n",
    "    score: int = 0\n",
    "    done: bool = False\n",
    "\n",
    "    def copy(self) -> \"State2048\":\n",
    "        return State2048(grid=self.grid.copy(), score=self.score, done=self.done)\n",
    "\n",
    "\n",
    "ACTION_UP, ACTION_RIGHT, ACTION_DOWN, ACTION_LEFT = 0, 1, 2, 3\n",
    "ACTION_NAMES = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "\n",
    "class Game2048(Game):\n",
    "    \"\"\"2048 game with explicit afterstate separation.\"\"\"\n",
    "\n",
    "    BITS_PER_TILE = 31\n",
    "    GRID_SIZE = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        self._rng = np.random.default_rng()\n",
    "\n",
    "    @property\n",
    "    def action_space_size(self) -> int:\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def chance_space_size(self) -> int:\n",
    "        return 33\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self) -> Tuple[int, ...]:\n",
    "        return (self.BITS_PER_TILE * self.GRID_SIZE * self.GRID_SIZE,)\n",
    "\n",
    "    def reset(self) -> State2048:\n",
    "        grid = np.zeros((self.GRID_SIZE, self.GRID_SIZE), dtype=np.int64)\n",
    "        empty = list(zip(*np.where(grid == 0)))\n",
    "        pos1 = empty[self._rng.integers(len(empty))]\n",
    "        grid[pos1] = 2 if self._rng.random() < 0.9 else 4\n",
    "        empty = list(zip(*np.where(grid == 0)))\n",
    "        pos2 = empty[self._rng.integers(len(empty))]\n",
    "        grid[pos2] = 2 if self._rng.random() < 0.9 else 4\n",
    "        return State2048(grid=grid, score=0, done=False)\n",
    "\n",
    "    def clone_state(self, state: State2048) -> State2048:\n",
    "        return state.copy()\n",
    "\n",
    "    def legal_actions(self, state: State2048) -> List[int]:\n",
    "        legal = []\n",
    "        for action in range(4):\n",
    "            afterstate, _, _ = self.apply_action(state, action)\n",
    "            if not np.array_equal(afterstate.grid, state.grid):\n",
    "                legal.append(action)\n",
    "        return legal\n",
    "\n",
    "    def _slide_and_merge_line(self, line: np.ndarray) -> Tuple[np.ndarray, int]:\n",
    "        non_zero = line[line != 0]\n",
    "        if len(non_zero) == 0:\n",
    "            return np.zeros_like(line), 0\n",
    "        merged, score, i = [], 0, 0\n",
    "        while i < len(non_zero):\n",
    "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
    "                merged_val = non_zero[i] * 2\n",
    "                merged.append(merged_val)\n",
    "                score += merged_val\n",
    "                i += 2\n",
    "            else:\n",
    "                merged.append(non_zero[i])\n",
    "                i += 1\n",
    "        result = np.zeros_like(line)\n",
    "        result[:len(merged)] = merged\n",
    "        return result, score\n",
    "\n",
    "    def _apply_action_to_grid(self, grid: np.ndarray, action: int) -> Tuple[np.ndarray, int]:\n",
    "        new_grid = grid.copy()\n",
    "        total_score = 0\n",
    "        if action == ACTION_UP:\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                new_grid[:, col], score = self._slide_and_merge_line(grid[:, col])\n",
    "                total_score += score\n",
    "        elif action == ACTION_DOWN:\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                merged, score = self._slide_and_merge_line(grid[:, col][::-1])\n",
    "                new_grid[:, col] = merged[::-1]\n",
    "                total_score += score\n",
    "        elif action == ACTION_LEFT:\n",
    "            for row in range(self.GRID_SIZE):\n",
    "                new_grid[row, :], score = self._slide_and_merge_line(grid[row, :])\n",
    "                total_score += score\n",
    "        elif action == ACTION_RIGHT:\n",
    "            for row in range(self.GRID_SIZE):\n",
    "                merged, score = self._slide_and_merge_line(grid[row, :][::-1])\n",
    "                new_grid[row, :] = merged[::-1]\n",
    "                total_score += score\n",
    "        return new_grid, total_score\n",
    "\n",
    "    def apply_action(self, state: State2048, action: int) -> Tuple[State2048, float, Dict[str, Any]]:\n",
    "        new_grid, score_gained = self._apply_action_to_grid(state.grid, action)\n",
    "        empty_positions = list(zip(*np.where(new_grid == 0)))\n",
    "        afterstate = State2048(grid=new_grid, score=state.score + score_gained, done=False)\n",
    "        info = {\"empty_positions\": empty_positions, \"grid_changed\": not np.array_equal(new_grid, state.grid)}\n",
    "        return afterstate, float(score_gained), info\n",
    "\n",
    "    def sample_chance(self, afterstate: State2048, info: Dict[str, Any]) -> ChanceOutcome:\n",
    "        empty_positions = info.get(\"empty_positions\", [])\n",
    "        grid_changed = info.get(\"grid_changed\", True)\n",
    "        if not grid_changed or len(empty_positions) == 0:\n",
    "            return 0\n",
    "        pos_idx = self._rng.integers(len(empty_positions))\n",
    "        row, col = empty_positions[pos_idx]\n",
    "        flat_pos = row * self.GRID_SIZE + col\n",
    "        if self._rng.random() < 0.9:\n",
    "            return flat_pos + 1\n",
    "        else:\n",
    "            return flat_pos + 17\n",
    "\n",
    "    def get_chance_distribution(self, afterstate: State2048, info: Dict[str, Any]) -> np.ndarray:\n",
    "        dist = np.zeros(self.chance_space_size, dtype=np.float32)\n",
    "        empty_positions = info.get(\"empty_positions\", [])\n",
    "        grid_changed = info.get(\"grid_changed\", True)\n",
    "        if not grid_changed or len(empty_positions) == 0:\n",
    "            dist[0] = 1.0\n",
    "            return dist\n",
    "        prob_per_pos = 1.0 / len(empty_positions)\n",
    "        for row, col in empty_positions:\n",
    "            flat_pos = row * self.GRID_SIZE + col\n",
    "            dist[flat_pos + 1] = prob_per_pos * 0.9\n",
    "            dist[flat_pos + 17] = prob_per_pos * 0.1\n",
    "        return dist\n",
    "\n",
    "    def apply_chance(self, afterstate: State2048, chance: ChanceOutcome) -> State2048:\n",
    "        if chance == 0:\n",
    "            next_state = afterstate.copy()\n",
    "            if len(self.legal_actions(next_state)) == 0:\n",
    "                next_state.done = True\n",
    "            return next_state\n",
    "        if chance <= 16:\n",
    "            flat_pos, value = chance - 1, 2\n",
    "        else:\n",
    "            flat_pos, value = chance - 17, 4\n",
    "        row, col = flat_pos // self.GRID_SIZE, flat_pos % self.GRID_SIZE\n",
    "        next_grid = afterstate.grid.copy()\n",
    "        next_grid[row, col] = value\n",
    "        next_state = State2048(grid=next_grid, score=afterstate.score, done=False)\n",
    "        if len(self.legal_actions(next_state)) == 0:\n",
    "            next_state.done = True\n",
    "        return next_state\n",
    "\n",
    "    def is_terminal(self, state: State2048) -> bool:\n",
    "        return state.done\n",
    "\n",
    "    def encode_state(self, state: State2048) -> torch.Tensor:\n",
    "        return self._encode_grid(state.grid)\n",
    "\n",
    "    def encode_afterstate(self, afterstate: State2048) -> torch.Tensor:\n",
    "        return self._encode_grid(afterstate.grid)\n",
    "\n",
    "    def _encode_grid(self, grid: np.ndarray) -> torch.Tensor:\n",
    "        features = []\n",
    "        for row in range(self.GRID_SIZE):\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                val = grid[row, col]\n",
    "                if val == 0:\n",
    "                    bits = [0] * self.BITS_PER_TILE\n",
    "                else:\n",
    "                    exp = int(np.log2(val))\n",
    "                    bits = [(exp >> i) & 1 for i in range(self.BITS_PER_TILE)]\n",
    "                features.extend(bits)\n",
    "        return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    def get_max_tile(self, state: State2048) -> int:\n",
    "        return int(state.grid.max())\n",
    "\n",
    "    def render(self, state: State2048) -> str:\n",
    "        lines = [f\"Score: {state.score}\", \"-\" * 25]\n",
    "        for row in range(self.GRID_SIZE):\n",
    "            cells = [f\"{state.grid[row, col]:5d}\" if state.grid[row, col] > 0 else \"    .\" for col in range(self.GRID_SIZE)]\n",
    "            lines.append(\" \".join(cells))\n",
    "        lines.append(\"-\" * 25)\n",
    "        if state.done:\n",
    "            lines.append(\"GAME OVER\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile networks/__init__.py\n",
    "from .muzero_network import MuZeroNetwork\n",
    "\n",
    "__all__ = [\"MuZeroNetwork\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile networks/muzero_network.py\n",
    "\"\"\"Combined MuZero network with all components.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(current_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU()])\n",
    "            current_dim = hidden_dim\n",
    "        layers.extend([nn.Linear(current_dim, output_dim), nn.LayerNorm(output_dim)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NetworkOutput:\n",
    "    state: torch.Tensor\n",
    "    policy_logits: torch.Tensor\n",
    "    value_logits: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DynamicsOutput:\n",
    "    afterstate: torch.Tensor\n",
    "    next_state: torch.Tensor\n",
    "    reward_logits: torch.Tensor\n",
    "    chance_logits: torch.Tensor\n",
    "    chance_entropy: torch.Tensor\n",
    "    afterstate_policy_logits: torch.Tensor\n",
    "    afterstate_value_logits: torch.Tensor\n",
    "\n",
    "\n",
    "class MuZeroNetwork(nn.Module):\n",
    "    \"\"\"Complete Stochastic MuZero network.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_dim: int, action_space_size: int, chance_space_size: int,\n",
    "                 state_dim: int = 256, hidden_dim: int = 128, num_layers: int = 2, support_size: int = 31):\n",
    "        super().__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_space_size = action_space_size\n",
    "        self.chance_space_size = chance_space_size\n",
    "        self.state_dim = state_dim\n",
    "        self.support_size = support_size\n",
    "\n",
    "        # Representation\n",
    "        self.representation = MLP(observation_dim, hidden_dim, state_dim, num_layers)\n",
    "        \n",
    "        # Afterstate dynamics\n",
    "        self.afterstate_dynamics = MLP(state_dim + action_space_size, hidden_dim, state_dim, num_layers)\n",
    "        \n",
    "        # Chance encoder\n",
    "        self.chance_encoder = MLP(state_dim, hidden_dim, chance_space_size, num_layers)\n",
    "        \n",
    "        # Dynamics\n",
    "        self.dynamics_trunk = MLP(state_dim + chance_space_size, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.dynamics_state_head = nn.Sequential(nn.Linear(hidden_dim, state_dim), nn.LayerNorm(state_dim))\n",
    "        self.dynamics_reward_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "        \n",
    "        # Prediction\n",
    "        self.prediction_trunk = MLP(state_dim, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.policy_head = nn.Linear(hidden_dim, action_space_size)\n",
    "        self.value_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "        \n",
    "        # Afterstate prediction\n",
    "        self.afterstate_trunk = MLP(state_dim, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.afterstate_policy_head = nn.Linear(hidden_dim, action_space_size)\n",
    "        self.afterstate_value_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "\n",
    "    def initial_inference(self, observation: torch.Tensor) -> NetworkOutput:\n",
    "        state = self.representation(observation)\n",
    "        features = self.prediction_trunk(state)\n",
    "        policy_logits = self.policy_head(features)\n",
    "        value_logits = self.value_head(features)\n",
    "        return NetworkOutput(state=state, policy_logits=policy_logits, value_logits=value_logits)\n",
    "\n",
    "    def recurrent_inference(self, state: torch.Tensor, action: torch.Tensor, chance: Optional[torch.Tensor] = None) -> DynamicsOutput:\n",
    "        # Afterstate\n",
    "        if action.dim() == 1:\n",
    "            action_onehot = F.one_hot(action, self.action_space_size).float()\n",
    "        else:\n",
    "            action_onehot = action\n",
    "        afterstate = self.afterstate_dynamics(torch.cat([state, action_onehot], dim=-1))\n",
    "        \n",
    "        # Chance\n",
    "        chance_logits = self.chance_encoder(afterstate)\n",
    "        probs = F.softmax(chance_logits, dim=-1)\n",
    "        log_probs = F.log_softmax(chance_logits, dim=-1)\n",
    "        entropy = -(probs * log_probs).sum(dim=-1)\n",
    "        \n",
    "        if chance is None:\n",
    "            chance = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "        \n",
    "        chance_onehot = F.one_hot(chance, self.chance_space_size).float()\n",
    "        \n",
    "        # Dynamics\n",
    "        dyn_features = self.dynamics_trunk(torch.cat([afterstate, chance_onehot], dim=-1))\n",
    "        next_state = self.dynamics_state_head(dyn_features)\n",
    "        reward_logits = self.dynamics_reward_head(dyn_features)\n",
    "        \n",
    "        # Afterstate prediction\n",
    "        as_features = self.afterstate_trunk(afterstate)\n",
    "        as_policy = self.afterstate_policy_head(as_features)\n",
    "        as_value = self.afterstate_value_head(as_features)\n",
    "        \n",
    "        return DynamicsOutput(afterstate=afterstate, next_state=next_state, reward_logits=reward_logits,\n",
    "                             chance_logits=chance_logits, chance_entropy=entropy,\n",
    "                             afterstate_policy_logits=as_policy, afterstate_value_logits=as_value)\n",
    "\n",
    "    def predict_state(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.prediction_trunk(state)\n",
    "        return self.policy_head(features), self.value_head(features)\n",
    "\n",
    "    def predict_afterstate(self, afterstate: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.afterstate_trunk(afterstate)\n",
    "        return self.afterstate_policy_head(features), self.afterstate_value_head(features)\n",
    "\n",
    "    def unroll(self, observation: torch.Tensor, actions: torch.Tensor, chances: torch.Tensor) -> Dict[str, List[torch.Tensor]]:\n",
    "        batch_size, K = actions.shape\n",
    "        initial = self.initial_inference(observation)\n",
    "        \n",
    "        states = [initial.state]\n",
    "        afterstates = []\n",
    "        policy_logits = [initial.policy_logits]\n",
    "        value_logits = [initial.value_logits]\n",
    "        reward_logits = []\n",
    "        chance_logits = []\n",
    "        chance_entropies = []\n",
    "        \n",
    "        current_state = initial.state\n",
    "        for k in range(K):\n",
    "            current_state = scale_gradient(current_state, 0.5)\n",
    "            dynamics_out = self.recurrent_inference(current_state, actions[:, k], chances[:, k])\n",
    "            \n",
    "            afterstates.append(dynamics_out.afterstate)\n",
    "            states.append(dynamics_out.next_state)\n",
    "            reward_logits.append(dynamics_out.reward_logits)\n",
    "            chance_logits.append(dynamics_out.chance_logits)\n",
    "            chance_entropies.append(dynamics_out.chance_entropy)\n",
    "            \n",
    "            next_policy, next_value = self.predict_state(dynamics_out.next_state)\n",
    "            policy_logits.append(next_policy)\n",
    "            value_logits.append(next_value)\n",
    "            current_state = dynamics_out.next_state\n",
    "        \n",
    "        return {\"states\": states, \"afterstates\": afterstates, \"policy_logits\": policy_logits,\n",
    "                \"value_logits\": value_logits, \"reward_logits\": reward_logits,\n",
    "                \"chance_logits\": chance_logits, \"chance_entropies\": chance_entropies}\n",
    "\n",
    "\n",
    "class ScaleGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale):\n",
    "        ctx.scale = scale\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * ctx.scale, None\n",
    "\n",
    "\n",
    "def scale_gradient(x: torch.Tensor, scale: float) -> torch.Tensor:\n",
    "    return ScaleGradient.apply(x, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MCTS with Macro Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcts/__init__.py\n",
    "from .node import Node\n",
    "from .tree_search import StochasticMCTS, MCTSConfig\n",
    "from .macro_cache import MacroCache, MacroOperator\n",
    "\n",
    "__all__ = [\"Node\", \"StochasticMCTS\", \"MCTSConfig\", \"MacroCache\", \"MacroOperator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcts/node.py\n",
    "\"\"\"MCTS tree node for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    hidden_state: Optional[torch.Tensor] = None\n",
    "    is_chance_node: bool = False\n",
    "    prior: float = 0.0\n",
    "    visit_count: int = 0\n",
    "    value_sum: float = 0.0\n",
    "    reward: float = 0.0\n",
    "    children: Dict[int, \"Node\"] = field(default_factory=dict)\n",
    "    parent: Optional[\"Node\"] = None\n",
    "    action_from_parent: Optional[int] = None\n",
    "    macro_id: Optional[int] = None\n",
    "    macro_confidence: float = 1.0\n",
    "    chance_entropy: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n",
    "\n",
    "    def expand(self, hidden_state: torch.Tensor, policy_logits: torch.Tensor, legal_actions: List[int], is_chance_node: bool = False):\n",
    "        self.hidden_state = hidden_state\n",
    "        policy = torch.softmax(policy_logits, dim=-1).cpu().numpy()\n",
    "        legal_mask = np.zeros(len(policy))\n",
    "        legal_mask[legal_actions] = 1.0\n",
    "        policy = policy * legal_mask\n",
    "        policy_sum = policy.sum()\n",
    "        if policy_sum > 0:\n",
    "            policy = policy / policy_sum\n",
    "        else:\n",
    "            policy[legal_actions] = 1.0 / len(legal_actions)\n",
    "        for action in legal_actions:\n",
    "            self.children[action] = Node(is_chance_node=is_chance_node, prior=float(policy[action]), parent=self, action_from_parent=action)\n",
    "\n",
    "    def expand_chance(self, hidden_state: torch.Tensor, chance_logits: torch.Tensor, top_k: int = 5, entropy_threshold: float = 0.5):\n",
    "        self.hidden_state = hidden_state\n",
    "        probs = torch.softmax(chance_logits, dim=-1).cpu().numpy()\n",
    "        log_probs = np.log(probs + 1e-10)\n",
    "        entropy = -np.sum(probs * log_probs)\n",
    "        self.chance_entropy = entropy\n",
    "        if entropy < entropy_threshold:\n",
    "            top_indices = np.argsort(probs)[-top_k:][::-1]\n",
    "            top_probs = probs[top_indices]\n",
    "            top_probs = top_probs / top_probs.sum()\n",
    "            for idx, prob in zip(top_indices, top_probs):\n",
    "                self.children[int(idx)] = Node(is_chance_node=False, prior=float(prob), parent=self, action_from_parent=int(idx))\n",
    "            return list(top_indices), True\n",
    "        else:\n",
    "            sampled = np.random.choice(len(probs), p=probs)\n",
    "            self.children[int(sampled)] = Node(is_chance_node=False, prior=1.0, parent=self, action_from_parent=int(sampled))\n",
    "            return [int(sampled)], False\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha: float, exploration_fraction: float):\n",
    "        if not self.children:\n",
    "            return\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        for i, action in enumerate(actions):\n",
    "            self.children[action].prior = self.children[action].prior * (1 - exploration_fraction) + noise[i] * exploration_fraction\n",
    "\n",
    "    def select_child(self, pb_c_base: float, pb_c_init: float, discount: float, min_max_stats):\n",
    "        best_score, best_action, best_child = float(\"-inf\"), None, None\n",
    "        for action, child in self.children.items():\n",
    "            pb_c = np.log((self.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n",
    "            prior_score = pb_c * child.prior * np.sqrt(self.visit_count) / (1 + child.visit_count)\n",
    "            value_score = min_max_stats.normalize(child.reward + discount * child.value) if child.visit_count > 0 else 0.0\n",
    "            score = prior_score + value_score\n",
    "            if score > best_score:\n",
    "                best_score, best_action, best_child = score, action, child\n",
    "        return best_action, best_child\n",
    "\n",
    "    def select_action(self, temperature: float = 1.0) -> int:\n",
    "        actions = list(self.children.keys())\n",
    "        visit_counts = np.array([self.children[a].visit_count for a in actions])\n",
    "        if temperature == 0:\n",
    "            return actions[np.argmax(visit_counts)]\n",
    "        counts_temp = visit_counts ** (1.0 / temperature)\n",
    "        probs = counts_temp / counts_temp.sum()\n",
    "        return int(np.random.choice(actions, p=probs))\n",
    "\n",
    "    def get_policy(self):\n",
    "        actions = np.array(list(self.children.keys()))\n",
    "        visit_counts = np.array([self.children[a].visit_count for a in actions])\n",
    "        probs = visit_counts / visit_counts.sum()\n",
    "        return actions, probs\n",
    "\n",
    "\n",
    "class MinMaxStats:\n",
    "    def __init__(self):\n",
    "        self.minimum = float(\"inf\")\n",
    "        self.maximum = float(\"-inf\")\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.minimum = min(self.minimum, value)\n",
    "        self.maximum = max(self.maximum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        return (value - self.minimum) / (self.maximum - self.minimum) if self.maximum > self.minimum else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcts/macro_cache.py\n",
    "\"\"\"Macro-operator cache for learned temporal abstractions.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MacroOperator:\n",
    "    id: int\n",
    "    action_sequence: Tuple[int, ...]\n",
    "    length: int\n",
    "    precondition_features: Optional[torch.Tensor] = None\n",
    "    confidence: float = 1.0\n",
    "    usage_count: int = 0\n",
    "    success_count: int = 0\n",
    "    creation_step: int = 0\n",
    "    entropy_history: List[float] = field(default_factory=list)\n",
    "    max_entropy_seen: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def success_rate(self) -> float:\n",
    "        return self.success_count / self.usage_count if self.usage_count > 0 else 1.0\n",
    "\n",
    "    def record_usage(self, success: bool, entropy: float):\n",
    "        self.usage_count += 1\n",
    "        if success:\n",
    "            self.success_count += 1\n",
    "        self.entropy_history.append(entropy)\n",
    "        self.max_entropy_seen = max(self.max_entropy_seen, entropy)\n",
    "\n",
    "\n",
    "class MacroCache:\n",
    "    def __init__(self, state_dim: int = 256, entropy_threshold: float = 0.1, composition_threshold: float = 0.01,\n",
    "                 min_macro_length: int = 2, max_macro_length: int = 8, max_macros: int = 1000,\n",
    "                 confidence_decay: float = 0.9, confidence_boost: float = 1.05, min_confidence: float = 0.5):\n",
    "        self.state_dim = state_dim\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "        self.composition_threshold = composition_threshold\n",
    "        self.min_macro_length = min_macro_length\n",
    "        self.max_macro_length = max_macro_length\n",
    "        self.max_macros = max_macros\n",
    "        self.confidence_decay = confidence_decay\n",
    "        self.confidence_boost = confidence_boost\n",
    "        self.min_confidence = min_confidence\n",
    "        \n",
    "        self.macros: Dict[int, MacroOperator] = {}\n",
    "        self.action_index: Dict[Tuple[int, ...], List[int]] = defaultdict(list)\n",
    "        self._next_id = 0\n",
    "        self.total_discoveries = 0\n",
    "        self.total_uses = 0\n",
    "        self.total_successes = 0\n",
    "\n",
    "    def discover_macro(self, states: List[torch.Tensor], actions: List[int], entropies: List[float], training_step: int = 0):\n",
    "        k = len(actions)\n",
    "        if k < self.min_macro_length or k > self.max_macro_length:\n",
    "            return None\n",
    "        if max(entropies) > self.entropy_threshold:\n",
    "            return None\n",
    "        action_tuple = tuple(actions)\n",
    "        if action_tuple in self.action_index:\n",
    "            for macro_id in self.action_index[action_tuple]:\n",
    "                self.macros[macro_id].entropy_history.append(max(entropies))\n",
    "            return None\n",
    "        \n",
    "        macro = MacroOperator(id=self._next_id, action_sequence=action_tuple, length=k,\n",
    "                             precondition_features=states[0].detach().clone() if states[0] is not None else None,\n",
    "                             confidence=1.0, creation_step=training_step, entropy_history=[max(entropies)], max_entropy_seen=max(entropies))\n",
    "        self._next_id += 1\n",
    "        self.total_discoveries += 1\n",
    "        self.macros[macro.id] = macro\n",
    "        self.action_index[action_tuple].append(macro.id)\n",
    "        \n",
    "        if len(self.macros) > self.max_macros:\n",
    "            worst_id = min(self.macros.keys(), key=lambda m: self.macros[m].confidence)\n",
    "            del self.macros[worst_id]\n",
    "        return macro\n",
    "\n",
    "    def get_applicable_macros(self, state: torch.Tensor, legal_actions: List[int]) -> List[MacroOperator]:\n",
    "        applicable = [m for m in self.macros.values() if m.action_sequence[0] in legal_actions and m.confidence >= self.min_confidence]\n",
    "        applicable.sort(key=lambda m: m.confidence, reverse=True)\n",
    "        return applicable\n",
    "\n",
    "    def update_macro(self, macro_id: int, success: bool, entropy: float):\n",
    "        if macro_id not in self.macros:\n",
    "            return\n",
    "        macro = self.macros[macro_id]\n",
    "        macro.record_usage(success, entropy)\n",
    "        self.total_uses += 1\n",
    "        if entropy > self.entropy_threshold:\n",
    "            macro.confidence *= self.confidence_decay\n",
    "        elif success:\n",
    "            macro.confidence = min(1.0, macro.confidence * self.confidence_boost)\n",
    "            self.total_successes += 1\n",
    "        else:\n",
    "            macro.confidence *= self.confidence_decay\n",
    "\n",
    "    def get_statistics(self) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"num_macros\": len(self.macros),\n",
    "            \"total_discoveries\": self.total_discoveries,\n",
    "            \"total_uses\": self.total_uses,\n",
    "            \"total_successes\": self.total_successes,\n",
    "            \"success_rate\": self.total_successes / self.total_uses if self.total_uses > 0 else 0.0,\n",
    "            \"avg_confidence\": np.mean([m.confidence for m in self.macros.values()]) if self.macros else 0.0,\n",
    "            \"avg_length\": np.mean([m.length for m in self.macros.values()]) if self.macros else 0.0,\n",
    "        }\n",
    "\n",
    "\n",
    "def discover_macros_from_trajectory(trajectory: List[Dict], macro_cache: MacroCache, min_length: int = 2, max_length: int = 8, training_step: int = 0):\n",
    "    discoveries = []\n",
    "    n = len(trajectory)\n",
    "    if n < min_length:\n",
    "        return discoveries\n",
    "    for length in range(min_length, min(max_length + 1, n + 1)):\n",
    "        for start in range(n - length + 1):\n",
    "            segment = trajectory[start:start + length]\n",
    "            states = [t[\"state\"] for t in segment]\n",
    "            if \"next_state\" in segment[-1] and segment[-1][\"next_state\"] is not None:\n",
    "                states.append(segment[-1][\"next_state\"])\n",
    "            else:\n",
    "                continue\n",
    "            actions = [t[\"action\"] for t in segment]\n",
    "            entropies = [t[\"entropy\"] for t in segment]\n",
    "            macro = macro_cache.discover_macro(states=states, actions=actions, entropies=entropies, training_step=training_step)\n",
    "            if macro is not None:\n",
    "                discoveries.append(macro)\n",
    "    return discoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile mcts/tree_search.py\n\"\"\"Stochastic MCTS with macro-operator support.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom .node import Node, MinMaxStats\nfrom .macro_cache import MacroCache\n\n\n@dataclass\nclass MCTSConfig:\n    num_simulations: int = 50\n    discount: float = 0.997\n    pb_c_base: float = 19652.0\n    pb_c_init: float = 1.25\n    root_dirichlet_alpha: float = 0.3\n    root_exploration_fraction: float = 0.25\n    entropy_threshold: float = 0.5\n    top_k_chances: int = 5\n    use_macros: bool = True\n    macro_verification_threshold: float = 0.1\n    action_space_size: int = 4  # Needed for policy output\n\n\nclass StochasticMCTS:\n    def __init__(self, model, config: MCTSConfig, macro_cache: Optional[MacroCache] = None):\n        self.model = model\n        self.config = config\n        self.macro_cache = macro_cache\n        self.min_max_stats = MinMaxStats()\n\n    @torch.no_grad()\n    def search(self, observation: torch.Tensor, legal_actions: List[int], add_exploration_noise: bool = True) -> Node:\n        if observation.dim() == 1:\n            observation = observation.unsqueeze(0)\n        initial = self.model.initial_inference(observation)\n        root = Node()\n        root.expand(hidden_state=initial.state, policy_logits=initial.policy_logits.squeeze(0), legal_actions=legal_actions, is_chance_node=False)\n        if add_exploration_noise:\n            root.add_exploration_noise(dirichlet_alpha=self.config.root_dirichlet_alpha, exploration_fraction=self.config.root_exploration_fraction)\n        \n        for _ in range(self.config.num_simulations):\n            node = root\n            search_path = [node]\n            while node.expanded:\n                action, child = node.select_child(pb_c_base=self.config.pb_c_base, pb_c_init=self.config.pb_c_init,\n                                                  discount=self.config.discount, min_max_stats=self.min_max_stats)\n                search_path.append(child)\n                node = child\n            parent = search_path[-2] if len(search_path) > 1 else None\n            value = self._expand(node, parent)\n            self._backpropagate(search_path, value)\n        return root\n\n    def _expand(self, node: Node, parent: Optional[Node]) -> float:\n        if parent is None:\n            return 0.0\n        action = node.action_from_parent\n        parent_state = parent.hidden_state\n        \n        if node.is_chance_node:\n            action_tensor = torch.tensor([action], device=parent_state.device)\n            dynamics_out = self.model.recurrent_inference(parent_state, action_tensor)\n            node.expand_chance(hidden_state=dynamics_out.afterstate, chance_logits=dynamics_out.chance_logits.squeeze(0),\n                              top_k=self.config.top_k_chances, entropy_threshold=self.config.entropy_threshold)\n            _, value_logits = self.model.predict_afterstate(dynamics_out.afterstate)\n            value_probs = F.softmax(value_logits, dim=-1)\n            return self._support_to_scalar(value_probs).item()\n        else:\n            chance = node.action_from_parent\n            chance_tensor = torch.tensor([chance], device=parent_state.device)\n            dyn_features = self.model.dynamics_trunk(torch.cat([parent_state, F.one_hot(chance_tensor, self.model.chance_space_size).float()], dim=-1))\n            next_state = self.model.dynamics_state_head(dyn_features)\n            reward_logits = self.model.dynamics_reward_head(dyn_features)\n            \n            reward_probs = F.softmax(reward_logits, dim=-1)\n            node.reward = self._support_to_scalar(reward_probs).item()\n            policy_logits, value_logits = self.model.predict_state(next_state)\n            node.expand(hidden_state=next_state, policy_logits=policy_logits.squeeze(0),\n                       legal_actions=list(range(policy_logits.shape[-1])), is_chance_node=True)\n            value_probs = F.softmax(value_logits, dim=-1)\n            return self._support_to_scalar(value_probs).item()\n\n    def _backpropagate(self, search_path: List[Node], value: float):\n        for node in reversed(search_path):\n            node.visit_count += 1\n            node.value_sum += value\n            self.min_max_stats.update(node.reward + self.config.discount * value)\n            value = node.reward + self.config.discount * value\n\n    def _support_to_scalar(self, probs: torch.Tensor) -> torch.Tensor:\n        support_size = (probs.shape[-1] - 1) // 2\n        support = torch.arange(-support_size, support_size + 1, device=probs.device, dtype=probs.dtype)\n        expected = (probs * support).sum(dim=-1)\n        eps = 0.001\n        sign = torch.sign(expected)\n        abs_expected = torch.abs(expected)\n        return sign * ((abs_expected + 1).square() - 1) / (1 + 2 * eps * (abs_expected + 1))\n\n    def get_action_policy(self, root: Node, temperature: float = 1.0) -> Tuple[int, np.ndarray]:\n        actions, probs = root.get_policy()\n        # Use action_space_size from config, not len(root.children)\n        policy = np.zeros(self.config.action_space_size)\n        for action, prob in zip(actions, probs):\n            policy[action] = prob\n        selected = root.select_action(temperature=temperature)\n        return selected, policy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/__init__.py\n",
    "from .replay_buffer import ReplayBuffer, GameHistory\n",
    "from .trainer import Trainer, TrainerConfig\n",
    "\n",
    "__all__ = [\"ReplayBuffer\", \"GameHistory\", \"Trainer\", \"TrainerConfig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/replay_buffer.py\n",
    "\"\"\"Replay buffer for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GameHistory:\n",
    "    observations: List[torch.Tensor] = field(default_factory=list)\n",
    "    actions: List[int] = field(default_factory=list)\n",
    "    rewards: List[float] = field(default_factory=list)\n",
    "    policies: List[np.ndarray] = field(default_factory=list)\n",
    "    root_values: List[float] = field(default_factory=list)\n",
    "    chance_outcomes: List[int] = field(default_factory=list)\n",
    "    entropies: List[float] = field(default_factory=list)\n",
    "    latent_states: List[torch.Tensor] = field(default_factory=list)\n",
    "    priorities: Optional[np.ndarray] = None\n",
    "    game_priority: float = 1.0\n",
    "    total_reward: float = 0.0\n",
    "    max_tile: int = 0\n",
    "    length: int = 0\n",
    "\n",
    "    def append(self, observation, action, reward, policy, root_value, chance_outcome=0, entropy=0.0, latent_state=None, afterstate=None):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.policies.append(policy)\n",
    "        self.root_values.append(root_value)\n",
    "        self.chance_outcomes.append(chance_outcome)\n",
    "        self.entropies.append(entropy)\n",
    "        if latent_state is not None:\n",
    "            self.latent_states.append(latent_state)\n",
    "        self.total_reward += reward\n",
    "        self.length += 1\n",
    "\n",
    "    def compute_target_values(self, discount: float, td_steps: int) -> List[float]:\n",
    "        targets = []\n",
    "        n = len(self.rewards)\n",
    "        for i in range(n):\n",
    "            value = sum((discount ** j) * self.rewards[i + j] for j in range(td_steps) if i + j < n)\n",
    "            bootstrap_idx = i + td_steps\n",
    "            if bootstrap_idx < n:\n",
    "                value += (discount ** td_steps) * self.root_values[bootstrap_idx]\n",
    "            targets.append(value)\n",
    "        return targets\n",
    "\n",
    "    def compute_priorities(self, td_steps: int, discount: float):\n",
    "        targets = self.compute_target_values(discount, td_steps)\n",
    "        self.priorities = np.array([abs(t - v) for t, v in zip(targets, self.root_values)])\n",
    "        self.game_priority = float(np.max(self.priorities)) if len(self.priorities) > 0 else 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    observations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    target_values: torch.Tensor\n",
    "    target_rewards: torch.Tensor\n",
    "    target_policies: torch.Tensor\n",
    "    chance_outcomes: torch.Tensor\n",
    "    weights: torch.Tensor\n",
    "    game_indices: List[int] = field(default_factory=list)\n",
    "    position_indices: List[int] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000, batch_size=256, num_unroll_steps=5, td_steps=10, discount=0.997, priority_alpha=1.0, priority_beta=1.0):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unroll_steps = num_unroll_steps\n",
    "        self.td_steps = td_steps\n",
    "        self.discount = discount\n",
    "        self.priority_alpha = priority_alpha\n",
    "        self.priority_beta = priority_beta\n",
    "        self.games = deque(maxlen=capacity)\n",
    "        self.total_positions = 0\n",
    "\n",
    "    def save_game(self, game: GameHistory):\n",
    "        game.compute_priorities(self.td_steps, self.discount)\n",
    "        if len(self.games) == self.games.maxlen:\n",
    "            self.total_positions -= self.games[0].length\n",
    "        self.games.append(game)\n",
    "        self.total_positions += game.length\n",
    "\n",
    "    def sample_batch(self, device=torch.device(\"cpu\")) -> Batch:\n",
    "        game_priorities = np.array([g.game_priority ** self.priority_alpha for g in self.games])\n",
    "        game_probs = game_priorities / game_priorities.sum()\n",
    "        game_indices = np.random.choice(len(self.games), size=self.batch_size, p=game_probs, replace=True)\n",
    "        \n",
    "        observations, actions, target_values, target_rewards, target_policies, chance_outcomes, weights, position_indices = [], [], [], [], [], [], [], []\n",
    "        \n",
    "        for game_idx in game_indices:\n",
    "            game = self.games[game_idx]\n",
    "            pos_idx = np.random.randint(len(game.observations)) if game.priorities is None else np.random.choice(len(game.observations), p=game.priorities / game.priorities.sum())\n",
    "            weight = 1.0\n",
    "            observations.append(game.observations[pos_idx])\n",
    "            \n",
    "            action_seq, reward_seq, chance_seq, value_targets, policy_targets = [], [], [], [], []\n",
    "            all_targets = game.compute_target_values(self.discount, self.td_steps)\n",
    "            value_targets.append(all_targets[pos_idx])\n",
    "            \n",
    "            for k in range(self.num_unroll_steps):\n",
    "                idx = pos_idx + k\n",
    "                if idx < len(game.actions):\n",
    "                    action_seq.append(game.actions[idx])\n",
    "                    reward_seq.append(game.rewards[idx])\n",
    "                    chance_seq.append(game.chance_outcomes[idx])\n",
    "                    policy_targets.append(game.policies[idx])\n",
    "                    value_targets.append(all_targets[idx + 1] if idx + 1 < len(all_targets) else 0.0)\n",
    "                else:\n",
    "                    action_seq.append(0)\n",
    "                    reward_seq.append(0.0)\n",
    "                    chance_seq.append(0)\n",
    "                    policy_targets.append(game.policies[-1])\n",
    "                    value_targets.append(0.0)\n",
    "            \n",
    "            policy_targets.insert(0, game.policies[pos_idx])\n",
    "            actions.append(action_seq)\n",
    "            target_rewards.append(reward_seq)\n",
    "            target_values.append(value_targets)\n",
    "            target_policies.append(policy_targets)\n",
    "            chance_outcomes.append(chance_seq)\n",
    "            weights.append(weight)\n",
    "            position_indices.append(pos_idx)\n",
    "        \n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.max()\n",
    "        action_space_size = len(target_policies[0][0])\n",
    "        padded_policies = [[[p[i] if i < len(p) else 0.0 for i in range(action_space_size)] for p in ps] for ps in target_policies]\n",
    "        \n",
    "        return Batch(\n",
    "            observations=torch.stack(observations).to(device),\n",
    "            actions=torch.tensor(actions, dtype=torch.long, device=device),\n",
    "            target_values=torch.tensor(target_values, dtype=torch.float32, device=device),\n",
    "            target_rewards=torch.tensor(target_rewards, dtype=torch.float32, device=device),\n",
    "            target_policies=torch.tensor(np.array(padded_policies), dtype=torch.float32, device=device),\n",
    "            chance_outcomes=torch.tensor(chance_outcomes, dtype=torch.long, device=device),\n",
    "            weights=torch.tensor(weights, dtype=torch.float32, device=device),\n",
    "            game_indices=list(game_indices),\n",
    "            position_indices=position_indices,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/trainer.py\n",
    "\"\"\"Training loop for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from .replay_buffer import Batch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.support import scalar_to_support, compute_cross_entropy_loss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    max_grad_norm: float = 5.0\n",
    "    policy_loss_weight: float = 1.0\n",
    "    value_loss_weight: float = 0.5\n",
    "    reward_loss_weight: float = 1.0\n",
    "    chance_loss_weight: float = 1.0\n",
    "    support_size: int = 31\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, config: TrainerConfig, device=torch.device(\"cpu\")):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        self.training_step = 0\n",
    "        self.loss_history = {\"total\": [], \"policy\": [], \"value\": [], \"reward\": [], \"chance\": []}\n",
    "\n",
    "    def train_step(self, batch: Batch) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        unroll_outputs = self.model.unroll(observation=batch.observations, actions=batch.actions, chances=batch.chance_outcomes)\n",
    "        \n",
    "        K = batch.actions.shape[1]\n",
    "        \n",
    "        # Policy loss\n",
    "        policy_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K + 1):\n",
    "            predicted_policy = F.log_softmax(unroll_outputs[\"policy_logits\"][k], dim=-1)\n",
    "            target_policy = batch.target_policies[:, k, :]\n",
    "            mask = target_policy.sum(dim=-1) > 0\n",
    "            if mask.any():\n",
    "                policy_loss += -(target_policy[mask] * predicted_policy[mask]).sum(dim=-1).mean()\n",
    "        policy_loss = policy_loss / (K + 1)\n",
    "        \n",
    "        # Value loss\n",
    "        value_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K + 1):\n",
    "            value_loss += compute_cross_entropy_loss(unroll_outputs[\"value_logits\"][k], batch.target_values[:, k], self.config.support_size)\n",
    "        value_loss = value_loss / (K + 1)\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K):\n",
    "            reward_loss += compute_cross_entropy_loss(unroll_outputs[\"reward_logits\"][k], batch.target_rewards[:, k], self.config.support_size)\n",
    "        reward_loss = reward_loss / max(K, 1)\n",
    "        \n",
    "        # Chance loss\n",
    "        chance_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K):\n",
    "            predicted_chance = F.log_softmax(unroll_outputs[\"chance_logits\"][k], dim=-1)\n",
    "            target_chance = batch.chance_outcomes[:, k]\n",
    "            mask = target_chance >= 0\n",
    "            if mask.any():\n",
    "                chance_loss += F.nll_loss(predicted_chance[mask], target_chance[mask], reduction=\"mean\")\n",
    "        chance_loss = chance_loss / max(K, 1)\n",
    "        \n",
    "        total_loss = (self.config.policy_loss_weight * policy_loss + self.config.value_loss_weight * value_loss +\n",
    "                     self.config.reward_loss_weight * reward_loss + self.config.chance_loss_weight * chance_loss)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.training_step += 1\n",
    "        return {\"total\": total_loss.item(), \"policy\": policy_loss.item(), \"value\": value_loss.item(),\n",
    "                \"reward\": reward_loss.item(), \"chance\": chance_loss.item()}\n",
    "\n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\"model_state_dict\": self.model.state_dict(), \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                   \"training_step\": self.training_step}, path)\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.training_step = checkpoint[\"training_step\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2048 game\n",
    "from games.game_2048 import Game2048\n",
    "\n",
    "game = Game2048()\n",
    "state = game.reset()\n",
    "print(\"Initial state:\")\n",
    "print(game.render(state))\n",
    "\n",
    "# Play a few random moves\n",
    "for i in range(5):\n",
    "    legal = game.legal_actions(state)\n",
    "    if not legal:\n",
    "        break\n",
    "    action = np.random.choice(legal)\n",
    "    result = game.step(state, action)\n",
    "    state = result.next_state\n",
    "    print(f\"\\nAfter action {['up', 'right', 'down', 'left'][action]} (reward={result.reward}):\")\n",
    "    print(game.render(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model creation\n",
    "from networks.muzero_network import MuZeroNetwork\n",
    "from utils.config import Config\n",
    "\n",
    "config = Config()\n",
    "model = MuZeroNetwork(\n",
    "    observation_dim=config.observation_dim,\n",
    "    action_space_size=config.action_space_size,\n",
    "    chance_space_size=config.chance_space_size,\n",
    "    state_dim=config.state_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "obs = game.encode_state(state).unsqueeze(0).to(DEVICE)\n",
    "output = model.initial_inference(obs)\n",
    "print(f\"State shape: {output.state.shape}\")\n",
    "print(f\"Policy shape: {output.policy_logits.shape}\")\n",
    "print(f\"Value shape: {output.value_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts.tree_search import StochasticMCTS, MCTSConfig\n",
    "from mcts.macro_cache import MacroCache, discover_macros_from_trajectory\n",
    "from training.replay_buffer import ReplayBuffer, GameHistory\n",
    "from training.trainer import Trainer, TrainerConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "NUM_ITERATIONS = 100  # Increase for better results\n",
    "GAMES_PER_ITERATION = 5\n",
    "BATCHES_PER_ITERATION = 20\n",
    "NUM_SIMULATIONS = 25  # Reduced for speed\n",
    "\n",
    "# Initialize components\n",
    "game = Game2048()\n",
    "model = MuZeroNetwork(\n",
    "    observation_dim=config.observation_dim,\n",
    "    action_space_size=config.action_space_size,\n",
    "    chance_space_size=config.chance_space_size,\n",
    "    state_dim=config.state_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    ").to(DEVICE)\n",
    "\n",
    "macro_cache = MacroCache(state_dim=config.state_dim, entropy_threshold=config.entropy_threshold)\n",
    "replay_buffer = ReplayBuffer(capacity=10000, batch_size=64, num_unroll_steps=config.num_unroll_steps)\n",
    "trainer = Trainer(model, TrainerConfig(), DEVICE)\n",
    "mcts_config = MCTSConfig(num_simulations=NUM_SIMULATIONS)\n",
    "\n",
    "# Tracking\n",
    "rewards_history = []\n",
    "max_tiles_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_mcts(game, model, mcts_config, device, max_moves=500):\n",
    "    \"\"\"Play one game with MCTS.\"\"\"\n",
    "    mcts = StochasticMCTS(model, mcts_config)\n",
    "    state = game.reset()\n",
    "    history = GameHistory()\n",
    "    \n",
    "    model.eval()\n",
    "    for step in range(max_moves):\n",
    "        if game.is_terminal(state):\n",
    "            break\n",
    "            \n",
    "        obs = game.encode_state(state).to(device)\n",
    "        legal = game.legal_actions(state)\n",
    "        if not legal:\n",
    "            break\n",
    "            \n",
    "        root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=True)\n",
    "        temp = 1.0 if step < 30 else 0.1\n",
    "        action, policy = mcts.get_action_policy(root, temp)\n",
    "        \n",
    "        result = game.step(state, action)\n",
    "        \n",
    "        # Get entropy\n",
    "        with torch.no_grad():\n",
    "            dyn_out = model.recurrent_inference(root.hidden_state, torch.tensor([action], device=device))\n",
    "            entropy = dyn_out.chance_entropy.item()\n",
    "        \n",
    "        # Pad policy to action_space_size\n",
    "        full_policy = np.zeros(game.action_space_size)\n",
    "        for i, p in enumerate(policy):\n",
    "            if i < len(full_policy):\n",
    "                full_policy[i] = p\n",
    "        \n",
    "        history.append(\n",
    "            observation=obs.cpu(),\n",
    "            action=action,\n",
    "            reward=result.reward,\n",
    "            policy=full_policy,\n",
    "            root_value=root.value,\n",
    "            chance_outcome=result.chance_outcome,\n",
    "            entropy=entropy,\n",
    "            latent_state=root.hidden_state.cpu() if root.hidden_state is not None else None,\n",
    "        )\n",
    "        \n",
    "        state = result.next_state\n",
    "    \n",
    "    history.max_tile = game.get_max_tile(state)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Starting training for {NUM_ITERATIONS} iterations...\")\n",
    "print(f\"  Games per iteration: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  MCTS simulations: {NUM_SIMULATIONS}\")\n",
    "print()\n",
    "\n",
    "for iteration in tqdm(range(NUM_ITERATIONS), desc=\"Training\"):\n",
    "    # Self-play\n",
    "    iter_rewards = []\n",
    "    iter_max_tiles = []\n",
    "    \n",
    "    for _ in range(GAMES_PER_ITERATION):\n",
    "        history = play_game_with_mcts(game, model, mcts_config, DEVICE, max_moves=500)\n",
    "        replay_buffer.save_game(history)\n",
    "        iter_rewards.append(history.total_reward)\n",
    "        iter_max_tiles.append(history.max_tile)\n",
    "    \n",
    "    rewards_history.append(np.mean(iter_rewards))\n",
    "    max_tiles_history.append(np.max(iter_max_tiles))\n",
    "    \n",
    "    # Training\n",
    "    if len(replay_buffer) >= 64:\n",
    "        iter_losses = []\n",
    "        for _ in range(BATCHES_PER_ITERATION):\n",
    "            batch = replay_buffer.sample_batch(DEVICE)\n",
    "            losses = trainer.train_step(batch)\n",
    "            iter_losses.append(losses[\"total\"])\n",
    "        loss_history.append(np.mean(iter_losses))\n",
    "    \n",
    "    # Logging\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        macro_stats = macro_cache.get_statistics()\n",
    "        print(f\"\\nIter {iteration + 1}: Reward={rewards_history[-1]:.0f}, MaxTile={max_tiles_history[-1]}, \"\n",
    "              f\"Loss={loss_history[-1] if loss_history else 0:.4f}, Macros={macro_stats['num_macros']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(rewards_history)\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Average Reward\")\n",
    "axes[0].set_title(\"Training Reward\")\n",
    "\n",
    "axes[1].plot(max_tiles_history)\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Max Tile\")\n",
    "axes[1].set_title(\"Best Tile Achieved\")\n",
    "axes[1].set_yscale('log', base=2)\n",
    "\n",
    "if loss_history:\n",
    "    axes[2].plot(loss_history)\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "    axes[2].set_ylabel(\"Loss\")\n",
    "    axes[2].set_title(\"Training Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with visualization\n",
    "print(\"Playing a game with trained model...\\n\")\n",
    "\n",
    "model.eval()\n",
    "mcts = StochasticMCTS(model, MCTSConfig(num_simulations=50))\n",
    "\n",
    "state = game.reset()\n",
    "print(\"Initial:\")\n",
    "print(game.render(state))\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(200):\n",
    "    if game.is_terminal(state):\n",
    "        break\n",
    "    \n",
    "    obs = game.encode_state(state).to(DEVICE)\n",
    "    legal = game.legal_actions(state)\n",
    "    if not legal:\n",
    "        break\n",
    "    \n",
    "    root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=False)\n",
    "    action, _ = mcts.get_action_policy(root, temperature=0.0)  # Greedy\n",
    "    \n",
    "    result = game.step(state, action)\n",
    "    total_reward += result.reward\n",
    "    state = result.next_state\n",
    "    \n",
    "    if step % 50 == 49:\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(game.render(state))\n",
    "\n",
    "print(f\"\\nFinal state:\")\n",
    "print(game.render(state))\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "print(f\"Max tile: {game.get_max_tile(state)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_checkpoint(\"runs/stochastic_muzero_2048.pt\")\n",
    "print(\"Model saved to runs/stochastic_muzero_2048.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Macro Analysis\n",
    "\n",
    "Analyze the discovered macro-operators to see what temporal abstractions emerged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze macros\n",
    "stats = macro_cache.get_statistics()\n",
    "print(\"Macro-Operator Statistics:\")\n",
    "print(f\"  Total macros discovered: {stats['total_discoveries']}\")\n",
    "print(f\"  Active macros: {stats['num_macros']}\")\n",
    "print(f\"  Total uses: {stats['total_uses']}\")\n",
    "print(f\"  Success rate: {stats['success_rate']:.2%}\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"  Average length: {stats['avg_length']:.1f} steps\")\n",
    "\n",
    "# Show top macros\n",
    "if macro_cache.macros:\n",
    "    print(\"\\nTop 10 macros by usage:\")\n",
    "    sorted_macros = sorted(macro_cache.macros.values(), key=lambda m: m.usage_count, reverse=True)[:10]\n",
    "    action_names = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    for m in sorted_macros:\n",
    "        actions_str = \" -> \".join(action_names[a] for a in m.action_sequence)\n",
    "        print(f\"  [{actions_str}] uses={m.usage_count}, success={m.success_rate:.2%}, conf={m.confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Train longer**: Increase `NUM_ITERATIONS` to 500-1000 for better performance\n",
    "2. **More MCTS simulations**: Increase `NUM_SIMULATIONS` to 100-200\n",
    "3. **Add Go environment**: Implement Go game interface for fully deterministic setting\n",
    "4. **Analyze entropy distribution**: Track how entropy separates deterministic vs stochastic transitions\n",
    "5. **Measure planning speedup**: Compare planning with and without macro usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}