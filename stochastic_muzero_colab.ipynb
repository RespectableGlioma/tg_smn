{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic MuZero with Learned Temporal Abstractions\n",
    "\n",
    "This notebook implements **Stochastic MuZero** with a novel approach to discovering **rules as compressible causal structure**.\n",
    "\n",
    "## Core Idea\n",
    "Instead of treating rules as explicit symbols, we discover them operationally:\n",
    "- **Rules** = sequences of transitions that are deterministic, repeatable, and compositional\n",
    "- These can be collapsed into **macro-operators** for faster planning\n",
    "- The model learns to separate **deterministic dynamics** (rules) from **stochastic chance** (environment randomness)\n",
    "\n",
    "## Architecture\n",
    "- **Afterstate separation**: `s \u2192 afterstate \u2192 chance \u2192 s'`\n",
    "- **Entropy tracking**: Identifies which transitions are deterministic\n",
    "- **Macro cache**: Stores and reuses discovered temporal abstractions\n",
    "- **Hierarchical MCTS**: Plans at multiple temporal scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install torch numpy pyyaml tqdm tensorboard matplotlib python-chess -q\n\n# Check GPU availability\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    DEVICE = torch.device(\"cuda\")\nelse:\n    DEVICE = torch.device(\"cpu\")\nprint(f\"Using device: {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure\n",
    "import os\n",
    "import numpy as np\n",
    "directories = ['config', 'games', 'networks', 'mcts', 'training', 'utils', 'runs']\n",
    "for d in directories:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "print(\"Project structure created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/__init__.py\n",
    "from .config import Config, load_config\n",
    "from .support import scalar_to_support, support_to_scalar\n",
    "\n",
    "__all__ = [\"Config\", \"load_config\", \"scalar_to_support\", \"support_to_scalar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/config.py\n",
    "\"\"\"Configuration management for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Stochastic MuZero with macro-operator discovery.\"\"\"\n",
    "\n",
    "    # Game settings\n",
    "    game: str = \"2048\"\n",
    "    action_space_size: int = 4\n",
    "    chance_space_size: int = 33\n",
    "\n",
    "    # Network architecture\n",
    "    state_dim: int = 256\n",
    "    hidden_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    observation_dim: int = 496\n",
    "    support_size: int = 31\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 256\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    max_grad_norm: float = 5.0\n",
    "    num_unroll_steps: int = 5\n",
    "    td_steps: int = 10\n",
    "    discount: float = 0.997\n",
    "\n",
    "    # Self-play\n",
    "    num_simulations: int = 50\n",
    "    num_actors: int = 4\n",
    "    max_moves: int = 10000\n",
    "    temperature_init: float = 1.0\n",
    "    temperature_final: float = 0.1\n",
    "    temperature_decay_steps: int = 10000\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer_size: int = 100000\n",
    "    priority_alpha: float = 1.0\n",
    "    priority_beta: float = 1.0\n",
    "\n",
    "    # MCTS\n",
    "    pb_c_base: float = 19652.0\n",
    "    pb_c_init: float = 1.25\n",
    "    root_dirichlet_alpha: float = 0.3\n",
    "    root_exploration_fraction: float = 0.25\n",
    "\n",
    "    # Macro-operator discovery\n",
    "    entropy_threshold: float = 0.1\n",
    "    composition_threshold: float = 0.01\n",
    "    min_macro_length: int = 2\n",
    "    max_macro_length: int = 8\n",
    "    macro_confidence_decay: float = 0.9\n",
    "    macro_confidence_boost: float = 1.05\n",
    "    max_macros: int = 1000\n",
    "\n",
    "    # Chance node handling\n",
    "    chance_entropy_threshold: float = 0.5\n",
    "    top_k_chances: int = 5\n",
    "\n",
    "    # Logging\n",
    "    log_interval: int = 100\n",
    "    save_interval: int = 1000\n",
    "    eval_interval: int = 500\n",
    "\n",
    "    # Device\n",
    "    device: str = \"cuda\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {k: getattr(self, k) for k in self.__dataclass_fields__}\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict[str, Any]) -> \"Config\":\n",
    "        valid_keys = cls.__dataclass_fields__.keys()\n",
    "        filtered = {k: v for k, v in d.items() if k in valid_keys}\n",
    "        return cls(**filtered)\n",
    "\n",
    "\n",
    "def load_config(config_path: Optional[str] = None, **overrides) -> Config:\n",
    "    config_dict = {}\n",
    "    if config_path is not None:\n",
    "        path = Path(config_path)\n",
    "        if path.exists():\n",
    "            with open(path, \"r\") as f:\n",
    "                config_dict = yaml.safe_load(f) or {}\n",
    "    config_dict.update(overrides)\n",
    "    return Config.from_dict(config_dict)\n",
    "\n",
    "\n",
    "def save_config(config: Config, path: str) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        yaml.dump(config.to_dict(), f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils/support.py\n",
    "\"\"\"Support-based scalar transformations for MuZero.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def scalar_to_support(x: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Transform scalar values to categorical support representation.\"\"\"\n",
    "    eps = 0.001\n",
    "    transformed = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + eps * x\n",
    "    transformed = torch.clamp(transformed, -support_size, support_size)\n",
    "    shifted = transformed + support_size\n",
    "    \n",
    "    floor_idx = shifted.floor().long()\n",
    "    ceil_idx = floor_idx + 1\n",
    "    floor_idx = torch.clamp(floor_idx, 0, 2 * support_size)\n",
    "    ceil_idx = torch.clamp(ceil_idx, 0, 2 * support_size)\n",
    "    \n",
    "    ceil_weight = shifted - floor_idx.float()\n",
    "    floor_weight = 1.0 - ceil_weight\n",
    "    \n",
    "    batch_shape = x.shape\n",
    "    support_dim = 2 * support_size + 1\n",
    "    \n",
    "    flat_floor = floor_idx.flatten()\n",
    "    flat_ceil = ceil_idx.flatten()\n",
    "    flat_floor_weight = floor_weight.flatten()\n",
    "    flat_ceil_weight = ceil_weight.flatten()\n",
    "    \n",
    "    output = torch.zeros(flat_floor.numel(), support_dim, device=x.device, dtype=x.dtype)\n",
    "    output.scatter_add_(1, flat_floor.unsqueeze(1), flat_floor_weight.unsqueeze(1))\n",
    "    output.scatter_add_(1, flat_ceil.unsqueeze(1), flat_ceil_weight.unsqueeze(1))\n",
    "    \n",
    "    return output.view(*batch_shape, support_dim)\n",
    "\n",
    "\n",
    "def support_to_scalar(probs: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Transform categorical support distribution back to scalar values.\"\"\"\n",
    "    support = torch.arange(-support_size, support_size + 1, device=probs.device, dtype=probs.dtype)\n",
    "    expected = (probs * support).sum(dim=-1)\n",
    "    eps = 0.001\n",
    "    sign = torch.sign(expected)\n",
    "    abs_expected = torch.abs(expected)\n",
    "    return sign * ((abs_expected + 1).square() - 1) / (1 + 2 * eps * (abs_expected + 1))\n",
    "\n",
    "\n",
    "def compute_cross_entropy_loss(pred_logits: torch.Tensor, target_scalar: torch.Tensor, support_size: int) -> torch.Tensor:\n",
    "    \"\"\"Compute cross-entropy loss between predicted logits and target scalar.\"\"\"\n",
    "    target_probs = scalar_to_support(target_scalar, support_size)\n",
    "    log_probs = F.log_softmax(pred_logits, dim=-1)\n",
    "    loss = -(target_probs * log_probs).sum(dim=-1)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Game Environment (2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile games/__init__.py\nfrom .base import Game\nfrom .game_2048 import Game2048\nfrom .game_tictactoe import GameTicTacToe\nfrom .game_chess import GameChess\n\n__all__ = [\"Game\", \"Game2048\", \"GameTicTacToe\", \"GameChess\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile games/base.py\n\"\"\"Abstract base class for game environments.\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, TypeVar\nimport numpy as np\nimport torch\n\nState = TypeVar(\"State\")\nAfterstate = TypeVar(\"Afterstate\")\nChanceOutcome = int\n\n\n@dataclass\nclass StepResult:\n    \"\"\"Result of applying an action and chance outcome.\"\"\"\n    afterstate: Any\n    next_state: Any\n    reward: float\n    done: bool\n    chance_outcome: int\n    info: Dict[str, Any]\n\n\nclass Game(ABC):\n    \"\"\"Abstract base class for game environments with afterstate separation.\"\"\"\n\n    @property\n    @abstractmethod\n    def action_space_size(self) -> int:\n        pass\n\n    @property\n    @abstractmethod\n    def chance_space_size(self) -> int:\n        pass\n\n    @property\n    @abstractmethod\n    def observation_shape(self) -> Tuple[int, ...]:\n        pass\n\n    @abstractmethod\n    def reset(self) -> State:\n        pass\n\n    @abstractmethod\n    def clone_state(self, state: State) -> State:\n        pass\n\n    @abstractmethod\n    def legal_actions(self, state: State) -> List[int]:\n        pass\n\n    @abstractmethod\n    def apply_action(self, state: State, action: int) -> Tuple[Afterstate, float, Dict[str, Any]]:\n        pass\n\n    @abstractmethod\n    def sample_chance(self, afterstate: Afterstate, info: Dict[str, Any]) -> ChanceOutcome:\n        pass\n\n    @abstractmethod\n    def get_chance_distribution(self, afterstate: Afterstate, info: Dict[str, Any]) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def apply_chance(self, afterstate: Afterstate, chance: ChanceOutcome) -> State:\n        pass\n\n    @abstractmethod\n    def is_terminal(self, state: State) -> bool:\n        pass\n\n    @abstractmethod\n    def encode_state(self, state: State) -> torch.Tensor:\n        pass\n\n    @abstractmethod\n    def encode_afterstate(self, afterstate: Afterstate) -> torch.Tensor:\n        pass\n\n    def step(self, state: State, action: int) -> StepResult:\n        \"\"\"Full step: apply action, sample chance, apply chance.\"\"\"\n        afterstate, reward, info = self.apply_action(state, action)\n        chance = self.sample_chance(afterstate, info)\n        next_state = self.apply_chance(afterstate, chance)\n        done = self.is_terminal(next_state)\n        return StepResult(afterstate=afterstate, next_state=next_state, reward=reward, done=done, chance_outcome=chance, info=info)\n\n    @property\n    def is_two_player(self) -> bool:\n        \"\"\"Whether this is a two-player alternating game (requires value negation).\"\"\"\n        return False\n\n    def current_player(self, state: State) -> int:\n        \"\"\"Return current player (0 or 1). Override for two-player games.\"\"\"\n        return 0\n\n    def get_canonical_state(self, state: State) -> State:\n        \"\"\"Get canonical form of state (for symmetry handling).\"\"\"\n        return state"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile games/game_2048.py\n",
    "\"\"\"2048 game environment with afterstate separation.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from .base import Game, ChanceOutcome\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State2048:\n",
    "    \"\"\"State of a 2048 game.\"\"\"\n",
    "    grid: np.ndarray\n",
    "    score: int = 0\n",
    "    done: bool = False\n",
    "\n",
    "    def copy(self) -> \"State2048\":\n",
    "        return State2048(grid=self.grid.copy(), score=self.score, done=self.done)\n",
    "\n",
    "\n",
    "ACTION_UP, ACTION_RIGHT, ACTION_DOWN, ACTION_LEFT = 0, 1, 2, 3\n",
    "ACTION_NAMES = [\"up\", \"right\", \"down\", \"left\"]\n",
    "\n",
    "\n",
    "class Game2048(Game):\n",
    "    \"\"\"2048 game with explicit afterstate separation.\"\"\"\n",
    "\n",
    "    BITS_PER_TILE = 31\n",
    "    GRID_SIZE = 4\n",
    "\n",
    "    def __init__(self):\n",
    "        self._rng = np.random.default_rng()\n",
    "\n",
    "    @property\n",
    "    def action_space_size(self) -> int:\n",
    "        return 4\n",
    "\n",
    "    @property\n",
    "    def chance_space_size(self) -> int:\n",
    "        return 33\n",
    "\n",
    "    @property\n",
    "    def observation_shape(self) -> Tuple[int, ...]:\n",
    "        return (self.BITS_PER_TILE * self.GRID_SIZE * self.GRID_SIZE,)\n",
    "\n",
    "    def reset(self) -> State2048:\n",
    "        grid = np.zeros((self.GRID_SIZE, self.GRID_SIZE), dtype=np.int64)\n",
    "        empty = list(zip(*np.where(grid == 0)))\n",
    "        pos1 = empty[self._rng.integers(len(empty))]\n",
    "        grid[pos1] = 2 if self._rng.random() < 0.9 else 4\n",
    "        empty = list(zip(*np.where(grid == 0)))\n",
    "        pos2 = empty[self._rng.integers(len(empty))]\n",
    "        grid[pos2] = 2 if self._rng.random() < 0.9 else 4\n",
    "        return State2048(grid=grid, score=0, done=False)\n",
    "\n",
    "    def clone_state(self, state: State2048) -> State2048:\n",
    "        return state.copy()\n",
    "\n",
    "    def legal_actions(self, state: State2048) -> List[int]:\n",
    "        legal = []\n",
    "        for action in range(4):\n",
    "            afterstate, _, _ = self.apply_action(state, action)\n",
    "            if not np.array_equal(afterstate.grid, state.grid):\n",
    "                legal.append(action)\n",
    "        return legal\n",
    "\n",
    "    def _slide_and_merge_line(self, line: np.ndarray) -> Tuple[np.ndarray, int]:\n",
    "        non_zero = line[line != 0]\n",
    "        if len(non_zero) == 0:\n",
    "            return np.zeros_like(line), 0\n",
    "        merged, score, i = [], 0, 0\n",
    "        while i < len(non_zero):\n",
    "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
    "                merged_val = non_zero[i] * 2\n",
    "                merged.append(merged_val)\n",
    "                score += merged_val\n",
    "                i += 2\n",
    "            else:\n",
    "                merged.append(non_zero[i])\n",
    "                i += 1\n",
    "        result = np.zeros_like(line)\n",
    "        result[:len(merged)] = merged\n",
    "        return result, score\n",
    "\n",
    "    def _apply_action_to_grid(self, grid: np.ndarray, action: int) -> Tuple[np.ndarray, int]:\n",
    "        new_grid = grid.copy()\n",
    "        total_score = 0\n",
    "        if action == ACTION_UP:\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                new_grid[:, col], score = self._slide_and_merge_line(grid[:, col])\n",
    "                total_score += score\n",
    "        elif action == ACTION_DOWN:\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                merged, score = self._slide_and_merge_line(grid[:, col][::-1])\n",
    "                new_grid[:, col] = merged[::-1]\n",
    "                total_score += score\n",
    "        elif action == ACTION_LEFT:\n",
    "            for row in range(self.GRID_SIZE):\n",
    "                new_grid[row, :], score = self._slide_and_merge_line(grid[row, :])\n",
    "                total_score += score\n",
    "        elif action == ACTION_RIGHT:\n",
    "            for row in range(self.GRID_SIZE):\n",
    "                merged, score = self._slide_and_merge_line(grid[row, :][::-1])\n",
    "                new_grid[row, :] = merged[::-1]\n",
    "                total_score += score\n",
    "        return new_grid, total_score\n",
    "\n",
    "    def apply_action(self, state: State2048, action: int) -> Tuple[State2048, float, Dict[str, Any]]:\n",
    "        new_grid, score_gained = self._apply_action_to_grid(state.grid, action)\n",
    "        empty_positions = list(zip(*np.where(new_grid == 0)))\n",
    "        afterstate = State2048(grid=new_grid, score=state.score + score_gained, done=False)\n",
    "        info = {\"empty_positions\": empty_positions, \"grid_changed\": not np.array_equal(new_grid, state.grid)}\n",
    "        return afterstate, float(score_gained), info\n",
    "\n",
    "    def sample_chance(self, afterstate: State2048, info: Dict[str, Any]) -> ChanceOutcome:\n",
    "        empty_positions = info.get(\"empty_positions\", [])\n",
    "        grid_changed = info.get(\"grid_changed\", True)\n",
    "        if not grid_changed or len(empty_positions) == 0:\n",
    "            return 0\n",
    "        pos_idx = self._rng.integers(len(empty_positions))\n",
    "        row, col = empty_positions[pos_idx]\n",
    "        flat_pos = row * self.GRID_SIZE + col\n",
    "        if self._rng.random() < 0.9:\n",
    "            return flat_pos + 1\n",
    "        else:\n",
    "            return flat_pos + 17\n",
    "\n",
    "    def get_chance_distribution(self, afterstate: State2048, info: Dict[str, Any]) -> np.ndarray:\n",
    "        dist = np.zeros(self.chance_space_size, dtype=np.float32)\n",
    "        empty_positions = info.get(\"empty_positions\", [])\n",
    "        grid_changed = info.get(\"grid_changed\", True)\n",
    "        if not grid_changed or len(empty_positions) == 0:\n",
    "            dist[0] = 1.0\n",
    "            return dist\n",
    "        prob_per_pos = 1.0 / len(empty_positions)\n",
    "        for row, col in empty_positions:\n",
    "            flat_pos = row * self.GRID_SIZE + col\n",
    "            dist[flat_pos + 1] = prob_per_pos * 0.9\n",
    "            dist[flat_pos + 17] = prob_per_pos * 0.1\n",
    "        return dist\n",
    "\n",
    "    def apply_chance(self, afterstate: State2048, chance: ChanceOutcome) -> State2048:\n",
    "        if chance == 0:\n",
    "            next_state = afterstate.copy()\n",
    "            if len(self.legal_actions(next_state)) == 0:\n",
    "                next_state.done = True\n",
    "            return next_state\n",
    "        if chance <= 16:\n",
    "            flat_pos, value = chance - 1, 2\n",
    "        else:\n",
    "            flat_pos, value = chance - 17, 4\n",
    "        row, col = flat_pos // self.GRID_SIZE, flat_pos % self.GRID_SIZE\n",
    "        next_grid = afterstate.grid.copy()\n",
    "        next_grid[row, col] = value\n",
    "        next_state = State2048(grid=next_grid, score=afterstate.score, done=False)\n",
    "        if len(self.legal_actions(next_state)) == 0:\n",
    "            next_state.done = True\n",
    "        return next_state\n",
    "\n",
    "    def is_terminal(self, state: State2048) -> bool:\n",
    "        return state.done\n",
    "\n",
    "    def encode_state(self, state: State2048) -> torch.Tensor:\n",
    "        return self._encode_grid(state.grid)\n",
    "\n",
    "    def encode_afterstate(self, afterstate: State2048) -> torch.Tensor:\n",
    "        return self._encode_grid(afterstate.grid)\n",
    "\n",
    "    def _encode_grid(self, grid: np.ndarray) -> torch.Tensor:\n",
    "        features = []\n",
    "        for row in range(self.GRID_SIZE):\n",
    "            for col in range(self.GRID_SIZE):\n",
    "                val = grid[row, col]\n",
    "                if val == 0:\n",
    "                    bits = [0] * self.BITS_PER_TILE\n",
    "                else:\n",
    "                    exp = int(np.log2(val))\n",
    "                    bits = [(exp >> i) & 1 for i in range(self.BITS_PER_TILE)]\n",
    "                features.extend(bits)\n",
    "        return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    def get_max_tile(self, state: State2048) -> int:\n",
    "        return int(state.grid.max())\n",
    "\n",
    "    def render(self, state: State2048) -> str:\n",
    "        lines = [f\"Score: {state.score}\", \"-\" * 25]\n",
    "        for row in range(self.GRID_SIZE):\n",
    "            cells = [f\"{state.grid[row, col]:5d}\" if state.grid[row, col] > 0 else \"    .\" for col in range(self.GRID_SIZE)]\n",
    "            lines.append(\" \".join(cells))\n",
    "        lines.append(\"-\" * 25)\n",
    "        if state.done:\n",
    "            lines.append(\"GAME OVER\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "source": "%%writefile games/game_tictactoe.py\n\"\"\"Tic-Tac-Toe: minimal fully deterministic two-player game.\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\nfrom .base import Game, ChanceOutcome\n\n\n@dataclass\nclass TicTacToeState:\n    board: np.ndarray  # 3x3, 0=empty, 1=X, 2=O\n    current_player: int = 1\n    done: bool = False\n    winner: Optional[int] = None\n\n    def copy(self):\n        return TicTacToeState(board=self.board.copy(), current_player=self.current_player, done=self.done, winner=self.winner)\n\n\nclass GameTicTacToe(Game):\n    \"\"\"Fully deterministic two-player game. All transitions have entropy 0.\"\"\"\n\n    def __init__(self):\n        self._rng = np.random.default_rng()\n\n    @property\n    def action_space_size(self) -> int:\n        return 9\n\n    @property\n    def chance_space_size(self) -> int:\n        return 1\n\n    @property\n    def observation_shape(self) -> Tuple[int, ...]:\n        return (27,)  # 3 planes of 3x3\n\n    @property\n    def is_two_player(self) -> bool:\n        return True\n\n    def current_player(self, state: TicTacToeState) -> int:\n        return 0 if state.current_player == 1 else 1\n\n    def reset(self) -> TicTacToeState:\n        return TicTacToeState(board=np.zeros((3, 3), dtype=np.int32), current_player=1)\n\n    def clone_state(self, state):\n        return state.copy()\n\n    def legal_actions(self, state) -> List[int]:\n        if state.done:\n            return []\n        return [i for i in range(9) if state.board[i // 3, i % 3] == 0]\n\n    def _check_winner(self, board):\n        for p in [1, 2]:\n            for r in range(3):\n                if all(board[r, c] == p for c in range(3)):\n                    return p\n            for c in range(3):\n                if all(board[r, c] == p for r in range(3)):\n                    return p\n            if all(board[i, i] == p for i in range(3)):\n                return p\n            if all(board[i, 2 - i] == p for i in range(3)):\n                return p\n        return None\n\n    def apply_action(self, state, action):\n        row, col = action // 3, action % 3\n        new_board = state.board.copy()\n        new_board[row, col] = state.current_player\n        winner = self._check_winner(new_board)\n        is_draw = winner is None and np.all(new_board != 0)\n        if winner is not None:\n            done, reward = True, 1.0\n        elif is_draw:\n            done, reward, winner = True, 0.0, 0\n        else:\n            done, reward = False, 0.0\n        next_player = 2 if state.current_player == 1 else 1\n        afterstate = TicTacToeState(board=new_board, current_player=next_player, done=done, winner=winner)\n        return afterstate, reward, {}\n\n    def sample_chance(self, afterstate, info):\n        return 0\n\n    def get_chance_distribution(self, afterstate, info):\n        return np.array([1.0], dtype=np.float32)\n\n    def apply_chance(self, afterstate, chance):\n        return afterstate\n\n    def is_terminal(self, state):\n        return state.done\n\n    def encode_state(self, state):\n        me = state.current_player\n        opp = 2 if me == 1 else 1\n        my_pieces = (state.board == me).astype(np.float32).flatten()\n        opp_pieces = (state.board == opp).astype(np.float32).flatten()\n        empty = (state.board == 0).astype(np.float32).flatten()\n        return torch.tensor(np.concatenate([my_pieces, opp_pieces, empty]))\n\n    def encode_afterstate(self, afterstate):\n        return self.encode_state(afterstate)\n\n    def render(self, state):\n        symbols = {0: \".\", 1: \"X\", 2: \"O\"}\n        lines = [\" \".join(symbols[state.board[r, c]] for c in range(3)) for r in range(3)]\n        lines.append(f\"Player: {'X' if state.current_player == 1 else 'O'}\")\n        if state.done:\n            lines.append(f\"Result: {'Draw' if state.winner == 0 else f'{symbols[state.winner]} wins!'}\")\n        return \"\\n\".join(lines)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile games/game_chess.py\n\"\"\"Chess: fully deterministic two-player game with rich rule structure.\n\nUses python-chess as the rule engine. The agent must discover legal moves\nthrough play \u2014 they are NOT hardcoded into the policy. The model learns\nwhich actions are legal by experiencing rejection of illegal moves.\n\nAlphaZero-style action encoding:\n- 4672 actions = 64 from-squares x 73 move types\n- 56 queen-like moves (8 directions x 7 distances)\n- 8 knight moves\n- 9 underpromotions (3 directions x 3 piece types)\n- Queen promotions encoded as queen-like moves\n\nState encoding:\n- 22 planes x 64 squares = 1408 features (flattened for MLP)\n- Board always from current player's perspective (flipped for black)\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\n\ntry:\n    import chess\nexcept ImportError:\n    raise ImportError(\"python-chess is required: pip install python-chess\")\n\nfrom .base import Game, ChanceOutcome\n\n# Move encoding constants\n\n# 8 directions for queen-like moves: (file_delta, rank_delta) per unit step\nQUEEN_DIRECTIONS = [\n    (0, 1),    # 0: N\n    (1, 1),    # 1: NE\n    (1, 0),    # 2: E\n    (1, -1),   # 3: SE\n    (0, -1),   # 4: S\n    (-1, -1),  # 5: SW\n    (-1, 0),   # 6: W\n    (-1, 1),   # 7: NW\n]\n\n# 8 knight move offsets: (file_delta, rank_delta)\nKNIGHT_MOVES = [\n    (1, 2), (2, 1), (2, -1), (1, -2),\n    (-1, -2), (-2, -1), (-2, 1), (-1, 2),\n]\n\n# Underpromotion: 3 forward directions x 3 piece types\n# Directions from current player's perspective (always moving \"north\")\nUNDERPROMO_DIRECTIONS = [(-1, 1), (0, 1), (1, 1)]  # Left capture, straight, right capture\nUNDERPROMO_PIECES = [chess.KNIGHT, chess.BISHOP, chess.ROOK]\n\n\nclass GameChess(Game):\n    \"\"\"\n    Chess with AlphaZero-style encoding for Stochastic MuZero.\n\n    Fully deterministic (chance_space_size=1), so every transition has\n    entropy ~ 0. All trajectory segments are macro candidates, enabling\n    discovery of opening sequences, tactical motifs, etc.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @property\n    def action_space_size(self) -> int:\n        return 4672  # 64 from-squares x 73 move types\n\n    @property\n    def chance_space_size(self) -> int:\n        return 1  # Fully deterministic\n\n    @property\n    def observation_shape(self) -> Tuple[int, ...]:\n        return (1408,)  # 22 planes x 64 squares\n\n    @property\n    def is_two_player(self) -> bool:\n        return True\n\n    def current_player(self, state: chess.Board) -> int:\n        return 0 if state.turn == chess.WHITE else 1\n\n    def reset(self) -> chess.Board:\n        return chess.Board()\n\n    def clone_state(self, state: chess.Board) -> chess.Board:\n        return state.copy()\n\n    def legal_actions(self, state: chess.Board) -> List[int]:\n        \"\"\"Get all legal actions as AlphaZero-style action indices.\"\"\"\n        if state.is_game_over():\n            return []\n        actions = set()\n        is_black = state.turn == chess.BLACK\n        for move in state.legal_moves:\n            action = self._move_to_action(move, is_black)\n            actions.add(action)\n        return sorted(actions)\n\n    def apply_action(\n        self, state: chess.Board, action: int\n    ) -> Tuple[chess.Board, float, Dict[str, Any]]:\n        \"\"\"Apply action. Returns (afterstate, reward, info).\"\"\"\n        board = state.copy()\n        is_black = board.turn == chess.BLACK\n        move = self._action_to_move(action, is_black, board)\n\n        if move is None or move not in board.legal_moves:\n            # Invalid action -- return unchanged state with zero reward.\n            # The MCTS should only select legal actions, so this is a fallback.\n            return board, 0.0, {\"invalid\": True}\n\n        board.push(move)\n\n        # Reward: +1 for checkmate (from perspective of player who just moved)\n        reward = 0.0\n        if board.is_checkmate():\n            reward = 1.0\n\n        return board, reward, {}\n\n    def sample_chance(\n        self, afterstate: chess.Board, info: Dict[str, Any]\n    ) -> ChanceOutcome:\n        return 0  # Deterministic\n\n    def get_chance_distribution(\n        self, afterstate: chess.Board, info: Dict[str, Any]\n    ) -> np.ndarray:\n        return np.array([1.0], dtype=np.float32)\n\n    def apply_chance(\n        self, afterstate: chess.Board, chance: ChanceOutcome\n    ) -> chess.Board:\n        return afterstate  # Identity for deterministic games\n\n    def is_terminal(self, state: chess.Board) -> bool:\n        return state.is_game_over()\n\n    def encode_state(self, state: chess.Board) -> torch.Tensor:\n        \"\"\"\n        Encode board as 22 planes x 64 squares = 1408 features.\n\n        Always from current player's perspective (board flipped for black).\n\n        Planes:\n         0-5:  Current player pieces (P, N, B, R, Q, K)\n         6-11: Opponent pieces (P, N, B, R, Q, K)\n         12:   My kingside castling\n         13:   My queenside castling\n         14:   Opponent kingside castling\n         15:   Opponent queenside castling\n         16:   En passant square\n         17:   Halfmove clock (normalized)\n         18:   Fullmove number (normalized)\n         19:   Color to move (always 1 -- we see from own perspective)\n         20:   Twofold repetition\n         21:   Threefold repetition\n        \"\"\"\n        planes = np.zeros((22, 8, 8), dtype=np.float32)\n        is_black = state.turn == chess.BLACK\n\n        # Piece planes (0-5: current player, 6-11: opponent)\n        me = state.turn\n        opp = not state.turn\n        for pt in range(1, 7):  # PAWN=1 .. KING=6\n            for sq in state.pieces(pt, me):\n                r, f = chess.square_rank(sq), chess.square_file(sq)\n                if is_black:\n                    r = 7 - r\n                planes[pt - 1, r, f] = 1.0\n\n            for sq in state.pieces(pt, opp):\n                r, f = chess.square_rank(sq), chess.square_file(sq)\n                if is_black:\n                    r = 7 - r\n                planes[pt + 5, r, f] = 1.0\n\n        # Castling rights (planes 12-15): my KS, my QS, opp KS, opp QS\n        if is_black:\n            planes[12] = float(state.has_kingside_castling_rights(chess.BLACK))\n            planes[13] = float(state.has_queenside_castling_rights(chess.BLACK))\n            planes[14] = float(state.has_kingside_castling_rights(chess.WHITE))\n            planes[15] = float(state.has_queenside_castling_rights(chess.WHITE))\n        else:\n            planes[12] = float(state.has_kingside_castling_rights(chess.WHITE))\n            planes[13] = float(state.has_queenside_castling_rights(chess.WHITE))\n            planes[14] = float(state.has_kingside_castling_rights(chess.BLACK))\n            planes[15] = float(state.has_queenside_castling_rights(chess.BLACK))\n\n        # En passant (plane 16)\n        if state.ep_square is not None:\n            r = chess.square_rank(state.ep_square)\n            f = chess.square_file(state.ep_square)\n            if is_black:\n                r = 7 - r\n            planes[16, r, f] = 1.0\n\n        # Halfmove clock (plane 17, normalized to [0, 1])\n        planes[17] = state.halfmove_clock / 100.0\n\n        # Fullmove number (plane 18, normalized)\n        planes[18] = min(state.fullmove_number / 200.0, 1.0)\n\n        # Color to move (plane 19): always 1 from own perspective\n        planes[19] = 1.0\n\n        # Repetition planes (20-21)\n        if state.is_repetition(2):\n            planes[20] = 1.0\n        if state.is_repetition(3):\n            planes[21] = 1.0\n\n        return torch.tensor(planes.reshape(-1), dtype=torch.float32)\n\n    def encode_afterstate(self, afterstate: chess.Board) -> torch.Tensor:\n        return self.encode_state(afterstate)\n\n    # ------------------------------------------------------------------\n    # Action encoding / decoding\n    # ------------------------------------------------------------------\n\n    def _move_to_action(self, move: chess.Move, is_black: bool) -> int:\n        \"\"\"Convert a chess.Move to an action index in [0, 4671].\"\"\"\n        from_sq = move.from_square\n        to_sq = move.to_square\n\n        # Mirror squares for black so encoding is always from own perspective\n        if is_black:\n            from_sq = chess.square_mirror(from_sq)\n            to_sq = chess.square_mirror(to_sq)\n\n        from_file = chess.square_file(from_sq)\n        from_rank = chess.square_rank(from_sq)\n        to_file = chess.square_file(to_sq)\n        to_rank = chess.square_rank(to_sq)\n\n        df = to_file - from_file\n        dr = to_rank - from_rank\n\n        # 1) Underpromotion (knight, bishop, rook)\n        if move.promotion is not None and move.promotion != chess.QUEEN:\n            try:\n                dir_idx = UNDERPROMO_DIRECTIONS.index((df, dr))\n            except ValueError:\n                dir_idx = 1  # fallback to straight\n            piece_idx = UNDERPROMO_PIECES.index(move.promotion)\n            move_type = 64 + dir_idx * 3 + piece_idx\n\n        # 2) Knight move\n        elif (df, dr) in KNIGHT_MOVES:\n            knight_idx = KNIGHT_MOVES.index((df, dr))\n            move_type = 56 + knight_idx\n\n        # 3) Queen-like move (straight / diagonal slides, pawn pushes, queen promos)\n        else:\n            direction = self._delta_to_direction(df, dr)\n            distance = max(abs(df), abs(dr))\n            move_type = direction * 7 + (distance - 1)\n\n        return from_sq * 73 + move_type\n\n    def _action_to_move(\n        self, action: int, is_black: bool, board: chess.Board\n    ) -> Optional[chess.Move]:\n        \"\"\"Convert an action index back to a chess.Move (or None if invalid).\"\"\"\n        from_sq = action // 73\n        move_type = action % 73\n\n        from_file = chess.square_file(from_sq)\n        from_rank = chess.square_rank(from_sq)\n\n        promotion = None\n\n        if move_type < 56:\n            # Queen-like move\n            direction = move_type // 7\n            distance = move_type % 7 + 1\n            df, dr = QUEEN_DIRECTIONS[direction]\n            to_file = from_file + df * distance\n            to_rank = from_rank + dr * distance\n\n        elif move_type < 64:\n            # Knight move\n            knight_idx = move_type - 56\n            df, dr = KNIGHT_MOVES[knight_idx]\n            to_file = from_file + df\n            to_rank = from_rank + dr\n\n        else:\n            # Underpromotion\n            under_idx = move_type - 64\n            dir_idx = under_idx // 3\n            piece_idx = under_idx % 3\n            df, dr = UNDERPROMO_DIRECTIONS[dir_idx]\n            to_file = from_file + df\n            to_rank = from_rank + dr\n            promotion = UNDERPROMO_PIECES[piece_idx]\n\n        # Bounds check\n        if not (0 <= to_file <= 7 and 0 <= to_rank <= 7):\n            return None\n\n        to_sq = chess.square(to_file, to_rank)\n\n        # Detect queen promotion: pawn reaching last rank via queen-like move\n        if promotion is None and to_rank == 7:\n            actual_from = chess.square_mirror(from_sq) if is_black else from_sq\n            piece = board.piece_at(actual_from)\n            if piece is not None and piece.piece_type == chess.PAWN:\n                promotion = chess.QUEEN\n\n        # Unmirror for black\n        if is_black:\n            from_sq = chess.square_mirror(from_sq)\n            to_sq = chess.square_mirror(to_sq)\n\n        return chess.Move(from_sq, to_sq, promotion=promotion)\n\n    @staticmethod\n    def _delta_to_direction(df: int, dr: int) -> int:\n        \"\"\"Map (file_delta, rank_delta) to one of 8 compass directions.\"\"\"\n        if df == 0 and dr > 0:\n            return 0   # N\n        if df > 0 and dr > 0:\n            return 1   # NE\n        if df > 0 and dr == 0:\n            return 2   # E\n        if df > 0 and dr < 0:\n            return 3   # SE\n        if df == 0 and dr < 0:\n            return 4   # S\n        if df < 0 and dr < 0:\n            return 5   # SW\n        if df < 0 and dr == 0:\n            return 6   # W\n        if df < 0 and dr > 0:\n            return 7   # NW\n        return 0  # fallback (shouldn't happen for valid moves)\n\n    def render(self, state: chess.Board) -> str:\n        \"\"\"Human-readable board display.\"\"\"\n        turn_str = \"White\" if state.turn == chess.WHITE else \"Black\"\n        status = \"\"\n        if state.is_checkmate():\n            winner = \"Black\" if state.turn == chess.WHITE else \"White\"\n            status = f\"\\nCheckmate! {winner} wins.\"\n        elif state.is_stalemate():\n            status = \"\\nStalemate -- draw.\"\n        elif state.is_check():\n            status = \"\\nCheck!\"\n        return f\"{state}\\nTurn: {turn_str} | Move: {state.fullmove_number}{status}\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile networks/__init__.py\n",
    "from .muzero_network import MuZeroNetwork\n",
    "\n",
    "__all__ = [\"MuZeroNetwork\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile networks/muzero_network.py\n",
    "\"\"\"Combined MuZero network with all components.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(current_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ReLU()])\n",
    "            current_dim = hidden_dim\n",
    "        layers.extend([nn.Linear(current_dim, output_dim), nn.LayerNorm(output_dim)])\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NetworkOutput:\n",
    "    state: torch.Tensor\n",
    "    policy_logits: torch.Tensor\n",
    "    value_logits: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DynamicsOutput:\n",
    "    afterstate: torch.Tensor\n",
    "    next_state: torch.Tensor\n",
    "    reward_logits: torch.Tensor\n",
    "    chance_logits: torch.Tensor\n",
    "    chance_entropy: torch.Tensor\n",
    "    afterstate_policy_logits: torch.Tensor\n",
    "    afterstate_value_logits: torch.Tensor\n",
    "\n",
    "\n",
    "class MuZeroNetwork(nn.Module):\n",
    "    \"\"\"Complete Stochastic MuZero network.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_dim: int, action_space_size: int, chance_space_size: int,\n",
    "                 state_dim: int = 256, hidden_dim: int = 128, num_layers: int = 2, support_size: int = 31):\n",
    "        super().__init__()\n",
    "        self.observation_dim = observation_dim\n",
    "        self.action_space_size = action_space_size\n",
    "        self.chance_space_size = chance_space_size\n",
    "        self.state_dim = state_dim\n",
    "        self.support_size = support_size\n",
    "\n",
    "        # Representation\n",
    "        self.representation = MLP(observation_dim, hidden_dim, state_dim, num_layers)\n",
    "        \n",
    "        # Afterstate dynamics\n",
    "        self.afterstate_dynamics = MLP(state_dim + action_space_size, hidden_dim, state_dim, num_layers)\n",
    "        \n",
    "        # Chance encoder\n",
    "        self.chance_encoder = MLP(state_dim, hidden_dim, chance_space_size, num_layers)\n",
    "        \n",
    "        # Dynamics\n",
    "        self.dynamics_trunk = MLP(state_dim + chance_space_size, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.dynamics_state_head = nn.Sequential(nn.Linear(hidden_dim, state_dim), nn.LayerNorm(state_dim))\n",
    "        self.dynamics_reward_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "        \n",
    "        # Prediction\n",
    "        self.prediction_trunk = MLP(state_dim, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.policy_head = nn.Linear(hidden_dim, action_space_size)\n",
    "        self.value_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "        \n",
    "        # Afterstate prediction\n",
    "        self.afterstate_trunk = MLP(state_dim, hidden_dim, hidden_dim, num_layers - 1)\n",
    "        self.afterstate_policy_head = nn.Linear(hidden_dim, action_space_size)\n",
    "        self.afterstate_value_head = nn.Linear(hidden_dim, 2 * support_size + 1)\n",
    "\n",
    "    def initial_inference(self, observation: torch.Tensor) -> NetworkOutput:\n",
    "        state = self.representation(observation)\n",
    "        features = self.prediction_trunk(state)\n",
    "        policy_logits = self.policy_head(features)\n",
    "        value_logits = self.value_head(features)\n",
    "        return NetworkOutput(state=state, policy_logits=policy_logits, value_logits=value_logits)\n",
    "\n",
    "    def recurrent_inference(self, state: torch.Tensor, action: torch.Tensor, chance: Optional[torch.Tensor] = None) -> DynamicsOutput:\n",
    "        # Afterstate\n",
    "        if action.dim() == 1:\n",
    "            action_onehot = F.one_hot(action, self.action_space_size).float()\n",
    "        else:\n",
    "            action_onehot = action\n",
    "        afterstate = self.afterstate_dynamics(torch.cat([state, action_onehot], dim=-1))\n",
    "        \n",
    "        # Chance\n",
    "        chance_logits = self.chance_encoder(afterstate)\n",
    "        probs = F.softmax(chance_logits, dim=-1)\n",
    "        log_probs = F.log_softmax(chance_logits, dim=-1)\n",
    "        entropy = -(probs * log_probs).sum(dim=-1)\n",
    "        \n",
    "        if chance is None:\n",
    "            chance = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "        \n",
    "        chance_onehot = F.one_hot(chance, self.chance_space_size).float()\n",
    "        \n",
    "        # Dynamics\n",
    "        dyn_features = self.dynamics_trunk(torch.cat([afterstate, chance_onehot], dim=-1))\n",
    "        next_state = self.dynamics_state_head(dyn_features)\n",
    "        reward_logits = self.dynamics_reward_head(dyn_features)\n",
    "        \n",
    "        # Afterstate prediction\n",
    "        as_features = self.afterstate_trunk(afterstate)\n",
    "        as_policy = self.afterstate_policy_head(as_features)\n",
    "        as_value = self.afterstate_value_head(as_features)\n",
    "        \n",
    "        return DynamicsOutput(afterstate=afterstate, next_state=next_state, reward_logits=reward_logits,\n",
    "                             chance_logits=chance_logits, chance_entropy=entropy,\n",
    "                             afterstate_policy_logits=as_policy, afterstate_value_logits=as_value)\n",
    "\n",
    "    def predict_state(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.prediction_trunk(state)\n",
    "        return self.policy_head(features), self.value_head(features)\n",
    "\n",
    "    def predict_afterstate(self, afterstate: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.afterstate_trunk(afterstate)\n",
    "        return self.afterstate_policy_head(features), self.afterstate_value_head(features)\n",
    "\n",
    "    def unroll(self, observation: torch.Tensor, actions: torch.Tensor, chances: torch.Tensor) -> Dict[str, List[torch.Tensor]]:\n",
    "        batch_size, K = actions.shape\n",
    "        initial = self.initial_inference(observation)\n",
    "        \n",
    "        states = [initial.state]\n",
    "        afterstates = []\n",
    "        policy_logits = [initial.policy_logits]\n",
    "        value_logits = [initial.value_logits]\n",
    "        reward_logits = []\n",
    "        chance_logits = []\n",
    "        chance_entropies = []\n",
    "        \n",
    "        current_state = initial.state\n",
    "        for k in range(K):\n",
    "            current_state = scale_gradient(current_state, 0.5)\n",
    "            dynamics_out = self.recurrent_inference(current_state, actions[:, k], chances[:, k])\n",
    "            \n",
    "            afterstates.append(dynamics_out.afterstate)\n",
    "            states.append(dynamics_out.next_state)\n",
    "            reward_logits.append(dynamics_out.reward_logits)\n",
    "            chance_logits.append(dynamics_out.chance_logits)\n",
    "            chance_entropies.append(dynamics_out.chance_entropy)\n",
    "            \n",
    "            next_policy, next_value = self.predict_state(dynamics_out.next_state)\n",
    "            policy_logits.append(next_policy)\n",
    "            value_logits.append(next_value)\n",
    "            current_state = dynamics_out.next_state\n",
    "        \n",
    "        return {\"states\": states, \"afterstates\": afterstates, \"policy_logits\": policy_logits,\n",
    "                \"value_logits\": value_logits, \"reward_logits\": reward_logits,\n",
    "                \"chance_logits\": chance_logits, \"chance_entropies\": chance_entropies}\n",
    "\n",
    "\n",
    "class ScaleGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale):\n",
    "        ctx.scale = scale\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output * ctx.scale, None\n",
    "\n",
    "\n",
    "def scale_gradient(x: torch.Tensor, scale: float) -> torch.Tensor:\n",
    "    return ScaleGradient.apply(x, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MCTS with Macro Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile mcts/__init__.py\nfrom .node import Node\nfrom .tree_search import StochasticMCTS, MCTSConfig\nfrom .macro_cache import MacroCache, MacroOperator\n\n__all__ = [\"Node\", \"StochasticMCTS\", \"MCTSConfig\", \"MacroCache\", \"MacroOperator\"]"
  },
  {
   "cell_type": "code",
   "source": "%%writefile mcts/node.py\n\"\"\"MCTS tree node for Stochastic MuZero.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\n\n\n@dataclass\nclass Node:\n    hidden_state: Optional[torch.Tensor] = None\n    is_chance_node: bool = False\n    prior: float = 0.0\n    visit_count: int = 0\n    value_sum: float = 0.0\n    reward: float = 0.0\n    children: Dict[int, \"Node\"] = field(default_factory=dict)\n    parent: Optional[\"Node\"] = None\n    action_from_parent: Optional[int] = None\n    macro_id: Optional[int] = None\n    macro_confidence: float = 1.0\n    chance_entropy: float = 0.0\n    to_play: int = -1\n\n    @property\n    def expanded(self) -> bool:\n        return len(self.children) > 0\n\n    @property\n    def value(self) -> float:\n        return self.value_sum / self.visit_count if self.visit_count > 0 else 0.0\n\n    def expand(self, hidden_state: torch.Tensor, policy_logits: torch.Tensor, legal_actions: List[int], is_chance_node: bool = False):\n        self.hidden_state = hidden_state\n        policy = torch.softmax(policy_logits, dim=-1).cpu().numpy()\n        legal_mask = np.zeros(len(policy))\n        legal_mask[legal_actions] = 1.0\n        policy = policy * legal_mask\n        policy_sum = policy.sum()\n        if policy_sum > 0:\n            policy = policy / policy_sum\n        else:\n            policy[legal_actions] = 1.0 / len(legal_actions)\n        for action in legal_actions:\n            self.children[action] = Node(is_chance_node=is_chance_node, prior=float(policy[action]), parent=self, action_from_parent=action)\n\n    def expand_chance(self, hidden_state: torch.Tensor, chance_logits: torch.Tensor, top_k: int = 5, entropy_threshold: float = 0.5):\n        self.hidden_state = hidden_state\n        probs = torch.softmax(chance_logits, dim=-1).cpu().numpy()\n        log_probs = np.log(probs + 1e-10)\n        entropy = -np.sum(probs * log_probs)\n        self.chance_entropy = entropy\n        if entropy < entropy_threshold:\n            top_indices = np.argsort(probs)[-top_k:][::-1]\n            top_probs = probs[top_indices]\n            top_probs = top_probs / top_probs.sum()\n            for idx, prob in zip(top_indices, top_probs):\n                self.children[int(idx)] = Node(is_chance_node=False, prior=float(prob), parent=self, action_from_parent=int(idx))\n            return list(top_indices), True\n        else:\n            sampled = np.random.choice(len(probs), p=probs)\n            self.children[int(sampled)] = Node(is_chance_node=False, prior=1.0, parent=self, action_from_parent=int(sampled))\n            return [int(sampled)], False\n\n    def add_exploration_noise(self, dirichlet_alpha: float, exploration_fraction: float):\n        if not self.children:\n            return\n        actions = list(self.children.keys())\n        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n        for i, action in enumerate(actions):\n            self.children[action].prior = self.children[action].prior * (1 - exploration_fraction) + noise[i] * exploration_fraction\n\n    def select_child(self, pb_c_base: float, pb_c_init: float, discount: float, min_max_stats):\n        best_score, best_action, best_child = float(\"-inf\"), None, None\n        for action, child in self.children.items():\n            pb_c = np.log((self.visit_count + pb_c_base + 1) / pb_c_base) + pb_c_init\n            prior_score = pb_c * child.prior * np.sqrt(self.visit_count) / (1 + child.visit_count)\n            value_score = min_max_stats.normalize(child.reward + discount * child.value) if child.visit_count > 0 else 0.0\n            score = prior_score + value_score\n            if score > best_score:\n                best_score, best_action, best_child = score, action, child\n        return best_action, best_child\n\n    def select_action(self, temperature: float = 1.0) -> int:\n        actions = list(self.children.keys())\n        visit_counts = np.array([self.children[a].visit_count for a in actions])\n        if temperature == 0:\n            return actions[np.argmax(visit_counts)]\n        counts_temp = visit_counts ** (1.0 / temperature)\n        probs = counts_temp / counts_temp.sum()\n        return int(np.random.choice(actions, p=probs))\n\n    def get_policy(self):\n        actions = np.array(list(self.children.keys()))\n        visit_counts = np.array([self.children[a].visit_count for a in actions])\n        probs = visit_counts / visit_counts.sum()\n        return actions, probs\n\n\nclass MinMaxStats:\n    def __init__(self):\n        self.minimum = float(\"inf\")\n        self.maximum = float(\"-inf\")\n\n    def update(self, value: float):\n        self.minimum = min(self.minimum, value)\n        self.maximum = max(self.maximum, value)\n\n    def normalize(self, value: float) -> float:\n        return (value - self.minimum) / (self.maximum - self.minimum) if self.maximum > self.minimum else value",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile mcts/macro_cache.py\n\"\"\"Macro-operator cache for learned temporal abstractions.\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom collections import defaultdict\n\n\n@dataclass\nclass MacroOperator:\n    id: int\n    action_sequence: Tuple[int, ...]\n    length: int\n    precondition_features: Optional[torch.Tensor] = None\n    confidence: float = 1.0\n    usage_count: int = 0\n    success_count: int = 0\n    creation_step: int = 0\n    entropy_history: List[float] = field(default_factory=list)\n    max_entropy_seen: float = 0.0\n\n    @property\n    def success_rate(self) -> float:\n        return self.success_count / self.usage_count if self.usage_count > 0 else 1.0\n\n    def record_usage(self, success: bool, entropy: float):\n        self.usage_count += 1\n        if success:\n            self.success_count += 1\n        self.entropy_history.append(entropy)\n        self.max_entropy_seen = max(self.max_entropy_seen, entropy)\n\n\nclass MacroCache:\n    def __init__(self, state_dim: int = 256, entropy_threshold: float = 0.1, composition_threshold: float = 0.01,\n                 min_macro_length: int = 2, max_macro_length: int = 8, max_macros: int = 1000,\n                 confidence_decay: float = 0.9, confidence_boost: float = 1.05, min_confidence: float = 0.5):\n        self.state_dim = state_dim\n        self.entropy_threshold = entropy_threshold\n        self.composition_threshold = composition_threshold\n        self.min_macro_length = min_macro_length\n        self.max_macro_length = max_macro_length\n        self.max_macros = max_macros\n        self.confidence_decay = confidence_decay\n        self.confidence_boost = confidence_boost\n        self.min_confidence = min_confidence\n        \n        self.macros: Dict[int, MacroOperator] = {}\n        self.action_index: Dict[Tuple[int, ...], List[int]] = defaultdict(list)\n        self._next_id = 0\n        self.total_discoveries = 0\n        self.total_uses = 0\n        self.total_successes = 0\n\n    def discover_macro(self, states: List[torch.Tensor], actions: List[int], entropies: List[float], training_step: int = 0):\n        k = len(actions)\n        if k < self.min_macro_length or k > self.max_macro_length:\n            return None\n        if max(entropies) > self.entropy_threshold:\n            return None\n        action_tuple = tuple(actions)\n        if action_tuple in self.action_index:\n            for macro_id in self.action_index[action_tuple]:\n                self.macros[macro_id].entropy_history.append(max(entropies))\n            return None\n        \n        macro = MacroOperator(id=self._next_id, action_sequence=action_tuple, length=k,\n                             precondition_features=states[0].detach().clone() if states[0] is not None else None,\n                             confidence=1.0, creation_step=training_step, entropy_history=[max(entropies)], max_entropy_seen=max(entropies))\n        self._next_id += 1\n        self.total_discoveries += 1\n        self.macros[macro.id] = macro\n        self.action_index[action_tuple].append(macro.id)\n        \n        if len(self.macros) > self.max_macros:\n            worst_id = min(self.macros.keys(), key=lambda m: self.macros[m].confidence)\n            del self.macros[worst_id]\n        return macro\n\n    def get_applicable_macros(self, state: torch.Tensor, legal_actions: List[int]) -> List[MacroOperator]:\n        applicable = [m for m in self.macros.values() if m.action_sequence[0] in legal_actions and m.confidence >= self.min_confidence]\n        applicable.sort(key=lambda m: m.confidence, reverse=True)\n        return applicable\n\n    def update_macro(self, macro_id: int, success: bool, entropy: float):\n        if macro_id not in self.macros:\n            return\n        macro = self.macros[macro_id]\n        macro.record_usage(success, entropy)\n        self.total_uses += 1\n        if entropy > self.entropy_threshold:\n            macro.confidence *= self.confidence_decay\n        elif success:\n            macro.confidence = min(1.0, macro.confidence * self.confidence_boost)\n            self.total_successes += 1\n        else:\n            macro.confidence *= self.confidence_decay\n\n    def get_statistics(self) -> Dict[str, float]:\n        return {\n            \"num_macros\": len(self.macros),\n            \"total_discoveries\": self.total_discoveries,\n            \"total_uses\": self.total_uses,\n            \"total_successes\": self.total_successes,\n            \"success_rate\": self.total_successes / self.total_uses if self.total_uses > 0 else 0.0,\n            \"avg_confidence\": np.mean([m.confidence for m in self.macros.values()]) if self.macros else 0.0,\n            \"avg_length\": np.mean([m.length for m in self.macros.values()]) if self.macros else 0.0,\n        }\n\n\ndef discover_macros_from_trajectory(trajectory: List[Dict], macro_cache: MacroCache, min_length: int = 2, max_length: int = 8, training_step: int = 0):\n    discoveries = []\n    n = len(trajectory)\n    if n < min_length:\n        return discoveries\n    for length in range(min_length, min(max_length + 1, n + 1)):\n        for start in range(n - length + 1):\n            segment = trajectory[start:start + length]\n            states = [t[\"state\"] for t in segment]\n            if \"next_state\" in segment[-1] and segment[-1][\"next_state\"] is not None:\n                states.append(segment[-1][\"next_state\"])\n            else:\n                continue\n            actions = [t[\"action\"] for t in segment]\n            entropies = [t[\"entropy\"] for t in segment]\n            macro = macro_cache.discover_macro(states=states, actions=actions, entropies=entropies, training_step=training_step)\n            if macro is not None:\n                discoveries.append(macro)\n    return discoveries",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile mcts/tree_search.py\n\"\"\"Stochastic MCTS with macro-operator support.\n\nThis implements Monte Carlo Tree Search for Stochastic MuZero with:\n1. Alternating decision and chance nodes\n2. Entropy-based chance node expansion (enumerate vs sample)\n3. Macro-operator lookup and usage during planning\n4. Proper value backup through stochastic branches\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .node import Node, MinMaxStats\nfrom .macro_cache import MacroCache, MacroOperator\n\n\n@dataclass\nclass MCTSConfig:\n    \"\"\"Configuration for MCTS.\"\"\"\n\n    num_simulations: int = 50\n    discount: float = 0.997\n    pb_c_base: float = 19652.0\n    pb_c_init: float = 1.25\n    root_dirichlet_alpha: float = 0.3\n    root_exploration_fraction: float = 0.25\n\n    # Chance node handling\n    entropy_threshold: float = 0.5\n    top_k_chances: int = 5\n\n    # Macro support\n    use_macros: bool = True\n    macro_verification_threshold: float = 0.1\n\n    # Action space size (needed for policy output)\n    action_space_size: int = 4\n\n    # Two-player support\n    is_two_player: bool = False\n\n\nclass StochasticMCTS:\n    \"\"\"\n    Monte Carlo Tree Search for Stochastic MuZero.\n\n    Handles:\n    - Decision nodes: Agent chooses action\n    - Chance nodes: Environment samples outcome\n    - Macro-operators: Skip deterministic segments\n    \"\"\"\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        config: MCTSConfig,\n        macro_cache: Optional[MacroCache] = None,\n    ):\n        self.model = model\n        self.config = config\n        self.macro_cache = macro_cache\n        self.min_max_stats = MinMaxStats()\n\n    @torch.no_grad()\n    def search(\n        self,\n        observation: torch.Tensor,\n        legal_actions: List[int],\n        add_exploration_noise: bool = True,\n    ) -> Node:\n        \"\"\"\n        Run MCTS from the given observation.\n\n        Args:\n            observation: Root observation (batch_size=1, observation_dim)\n            legal_actions: List of legal actions at root\n            add_exploration_noise: Whether to add Dirichlet noise at root\n\n        Returns:\n            Root node after search\n        \"\"\"\n        # Ensure batch dimension\n        if observation.dim() == 1:\n            observation = observation.unsqueeze(0)\n\n        # Initial inference at root\n        initial = self.model.initial_inference(observation)\n\n        # Create root node\n        root = Node(to_play=0)\n        root.expand(\n            hidden_state=initial.state,\n            policy_logits=initial.policy_logits.squeeze(0),\n            legal_actions=legal_actions,\n            is_chance_node=True,  # Children are chance nodes (afterstates)\n        )\n\n        # Add exploration noise\n        if add_exploration_noise:\n            root.add_exploration_noise(\n                dirichlet_alpha=self.config.root_dirichlet_alpha,\n                exploration_fraction=self.config.root_exploration_fraction,\n            )\n\n        # Run simulations\n        for _ in range(self.config.num_simulations):\n            node = root\n            search_path = [node]\n\n            # Selection: traverse tree until leaf\n            while node.expanded:\n                # Check for applicable macros\n                if (\n                    self.config.use_macros\n                    and self.macro_cache is not None\n                    and not node.is_chance_node\n                ):\n                    macro = self._try_macro(node, search_path)\n                    if macro is not None:\n                        node = macro\n                        continue\n\n                # Normal selection\n                action, child = node.select_child(\n                    pb_c_base=self.config.pb_c_base,\n                    pb_c_init=self.config.pb_c_init,\n                    discount=self.config.discount,\n                    min_max_stats=self.min_max_stats,\n                )\n                search_path.append(child)\n                node = child\n\n            # Expansion\n            parent = search_path[-2] if len(search_path) > 1 else None\n            value = self._expand(node, parent)\n\n            # Backpropagation\n            self._backpropagate(search_path, value)\n\n        return root\n\n    def _try_macro(\n        self,\n        node: Node,\n        search_path: List[Node],\n    ) -> Optional[Node]:\n        \"\"\"\n        Try to use a macro-operator from this node.\n\n        Returns the node reached after applying the macro,\n        or None if no applicable macro was found.\n        \"\"\"\n        if node.hidden_state is None:\n            return None\n\n        # Get legal actions (actions with children)\n        legal_actions = list(node.children.keys())\n\n        # Find applicable macros\n        macros = self.macro_cache.get_applicable_macros(\n            state=node.hidden_state.squeeze(0),\n            legal_actions=legal_actions,\n        )\n\n        if not macros:\n            return None\n\n        # Try the highest-confidence macro\n        macro = macros[0]\n\n        # Verify macro is still valid (low entropy)\n        current_state = node.hidden_state\n        total_reward = 0.0\n        max_entropy = 0.0\n\n        for action in macro.action_sequence:\n            # Ensure action is still legal\n            if action not in node.children:\n                self.macro_cache.update_macro(macro.id, success=False, entropy=1.0)\n                return None\n\n            action_tensor = torch.tensor([action], device=current_state.device)\n\n            # Get dynamics\n            dynamics_out = self.model.recurrent_inference(\n                current_state, action_tensor\n            )\n\n            max_entropy = max(max_entropy, dynamics_out.chance_entropy.item())\n\n            # Check if still deterministic\n            if max_entropy > self.config.macro_verification_threshold:\n                self.macro_cache.update_macro(macro.id, success=False, entropy=max_entropy)\n                return None\n\n            # Most likely chance outcome\n            chance = torch.argmax(dynamics_out.chance_logits, dim=-1)\n            next_state, reward_logits = self.model.dynamics(\n                dynamics_out.afterstate, chance\n            )\n\n            # Get scalar reward\n            reward_probs = F.softmax(reward_logits, dim=-1)\n            reward = self._support_to_scalar(reward_probs).item()\n            total_reward += reward * (self.config.discount ** len(search_path))\n\n            current_state = next_state\n\n            # Update search path through macro\n            # Create virtual nodes for the path\n            virtual_node = Node(\n                hidden_state=dynamics_out.afterstate,\n                is_chance_node=True,\n                reward=reward,\n                parent=search_path[-1],\n                action_from_parent=action,\n                macro_id=macro.id,\n            )\n            search_path.append(virtual_node)\n\n        # Success - update macro statistics\n        self.macro_cache.update_macro(macro.id, success=True, entropy=max_entropy)\n\n        # Create final node after macro\n        final_node = Node(\n            hidden_state=current_state,\n            is_chance_node=False,\n            reward=total_reward,\n            parent=search_path[-1],\n            macro_id=macro.id,\n            macro_confidence=macro.confidence,\n        )\n        search_path.append(final_node)\n\n        return final_node\n\n    def _expand(self, node: Node, parent: Optional[Node]) -> float:\n        \"\"\"\n        Expand a leaf node and return its value.\n\n        Args:\n            node: Leaf node to expand\n            parent: Parent node (needed for dynamics)\n\n        Returns:\n            Value estimate for backpropagation\n        \"\"\"\n        if parent is None:\n            # Root node already expanded in search()\n            return 0.0\n\n        action = node.action_from_parent\n        parent_state = parent.hidden_state\n\n        if node.is_chance_node:\n            # This is a chance node (after action, before environment response)\n            # Expand with chance outcomes\n\n            action_tensor = torch.tensor([action], device=parent_state.device)\n            dynamics_out = self.model.recurrent_inference(parent_state, action_tensor)\n\n            # Expand chance node\n            chance_indices, enumerated = node.expand_chance(\n                hidden_state=dynamics_out.afterstate,\n                chance_logits=dynamics_out.chance_logits.squeeze(0),\n                top_k=self.config.top_k_chances,\n                entropy_threshold=self.config.entropy_threshold,\n            )\n\n            # For two-player games, set to_play on decision node children\n            if self.config.is_two_player and parent is not None:\n                parent_to_play = parent.to_play if parent.to_play >= 0 else 0\n                for child in node.children.values():\n                    child.to_play = 1 - parent_to_play\n\n            # Get afterstate value\n            _, value_logits = self.model.predict_afterstate(dynamics_out.afterstate)\n            value_probs = F.softmax(value_logits, dim=-1)\n            value = self._support_to_scalar(value_probs).item()\n\n            return value\n        else:\n            # This is a decision node (after chance resolved)\n            # Need to compute state from parent's afterstate + chance\n\n            # Get chance outcome that led here\n            chance = node.action_from_parent\n            chance_tensor = torch.tensor([chance], device=parent_state.device)\n\n            # Parent is afterstate, compute next state\n            dyn_features = self.model.dynamics_trunk(torch.cat([parent_state, F.one_hot(chance_tensor, self.model.chance_space_size).float()], dim=-1))\n            next_state = self.model.dynamics_state_head(dyn_features)\n            reward_logits = self.model.dynamics_reward_head(dyn_features)\n\n            # Get reward\n            reward_probs = F.softmax(reward_logits, dim=-1)\n            reward = self._support_to_scalar(reward_probs).item()\n            node.reward = reward\n\n            # Get policy and value at next state\n            policy_logits, value_logits = self.model.predict_state(next_state)\n\n            # Expand with all actions (no legal action filtering in latent space)\n            node.expand(\n                hidden_state=next_state,\n                policy_logits=policy_logits.squeeze(0),\n                legal_actions=list(range(policy_logits.shape[-1])),\n                is_chance_node=True,  # Children will be chance nodes\n            )\n\n            value_probs = F.softmax(value_logits, dim=-1)\n            value = self._support_to_scalar(value_probs).item()\n\n            return value\n\n    def _backpropagate(self, search_path: List[Node], value: float) -> None:\n        \"\"\"\n        Backpropagate value through the search path.\n\n        For single-player: straightforward value backup.\n        For two-player: negate value at decision node boundaries (zero-sum).\n        Values at each node are stored from that node's player's perspective.\n\n        Args:\n            search_path: Path from root to leaf\n            value: Value at leaf node (from leaf player's perspective)\n        \"\"\"\n        for node in reversed(search_path):\n            node.visit_count += 1\n            node.value_sum += value\n\n            # Update min-max stats\n            self.min_max_stats.update(node.reward + self.config.discount * value)\n\n            # Compute backed-up value from this node's perspective\n            backed_value = node.reward + self.config.discount * value\n\n            # For two-player zero-sum games, negate at decision nodes\n            # since the parent decision node belongs to the opponent\n            if self.config.is_two_player and not node.is_chance_node:\n                value = -backed_value\n            else:\n                value = backed_value\n\n    def _support_to_scalar(self, probs: torch.Tensor) -> torch.Tensor:\n        \"\"\"Convert categorical support to scalar value.\"\"\"\n        support_size = (probs.shape[-1] - 1) // 2\n        support = torch.arange(\n            -support_size, support_size + 1,\n            device=probs.device, dtype=probs.dtype\n        )\n        expected = (probs * support).sum(dim=-1)\n\n        # Inverse transformation\n        eps = 0.001\n        sign = torch.sign(expected)\n        abs_expected = torch.abs(expected)\n        return sign * ((abs_expected + 1).square() - 1) / (1 + 2 * eps * (abs_expected + 1))\n\n    def get_action_policy(\n        self, root: Node, temperature: float = 1.0\n    ) -> Tuple[int, np.ndarray]:\n        \"\"\"\n        Get action and policy from search results.\n\n        Args:\n            root: Root node after search\n            temperature: Temperature for action selection\n\n        Returns:\n            (selected_action, policy_distribution)\n        \"\"\"\n        actions, probs = root.get_policy()\n\n        # Create full policy array sized by action space, not number of children\n        policy = np.zeros(self.config.action_space_size)\n        for action, prob in zip(actions, probs):\n            policy[action] = prob\n\n        # Select action\n        selected = root.select_action(temperature=temperature)\n\n        return selected, policy\n\n\ndef run_mcts(\n    model: torch.nn.Module,\n    observation: torch.Tensor,\n    legal_actions: List[int],\n    config: Optional[MCTSConfig] = None,\n    macro_cache: Optional[MacroCache] = None,\n    add_noise: bool = True,\n) -> Tuple[int, np.ndarray, float, Node]:\n    \"\"\"\n    Convenience function to run MCTS and get results.\n\n    Args:\n        model: MuZero network\n        observation: Current observation\n        legal_actions: Legal actions\n        config: MCTS configuration (uses defaults if None)\n        macro_cache: Optional macro cache\n        add_noise: Whether to add exploration noise\n\n    Returns:\n        (action, policy, root_value, root_node)\n    \"\"\"\n    if config is None:\n        config = MCTSConfig()\n\n    mcts = StochasticMCTS(model, config, macro_cache)\n    root = mcts.search(observation, legal_actions, add_exploration_noise=add_noise)\n\n    action, policy = mcts.get_action_policy(root, temperature=1.0)\n    root_value = root.value\n\n    return action, policy, root_value, root"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Components"
   ]
  },
  {
   "cell_type": "code",
   "source": "%%writefile training/__init__.py\nfrom .replay_buffer import ReplayBuffer, GameHistory\nfrom .trainer import Trainer, TrainerConfig\n\n__all__ = [\"ReplayBuffer\", \"GameHistory\", \"Trainer\", \"TrainerConfig\"]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile training/replay_buffer.py\n\"\"\"Replay buffer for Stochastic MuZero.\n\nStores game trajectories with:\n- Observations, actions, rewards\n- MCTS policies and values\n- Chance outcomes (for stochastic games)\n- Entropy at each transition (for macro discovery)\n\nSupports prioritized experience replay based on TD error.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple, Any\nimport numpy as np\nimport torch\nfrom collections import deque\n\n\n@dataclass\nclass GameHistory:\n    \"\"\"\n    Complete history of a single game/episode.\n\n    Stores all information needed for training and macro discovery.\n    \"\"\"\n\n    # Core trajectory data\n    observations: List[torch.Tensor] = field(default_factory=list)\n    actions: List[int] = field(default_factory=list)\n    rewards: List[float] = field(default_factory=list)\n\n    # MCTS outputs\n    policies: List[np.ndarray] = field(default_factory=list)\n    root_values: List[float] = field(default_factory=list)\n\n    # Stochastic MuZero specific\n    chance_outcomes: List[int] = field(default_factory=list)\n    afterstates: List[torch.Tensor] = field(default_factory=list)\n\n    # Macro discovery data\n    entropies: List[float] = field(default_factory=list)\n    latent_states: List[torch.Tensor] = field(default_factory=list)\n\n    # Two-player game support\n    to_play: List[int] = field(default_factory=list)  # Player at each step (0 or 1)\n    is_two_player: bool = False\n\n    # Priority sampling\n    priorities: Optional[np.ndarray] = None\n    game_priority: float = 1.0\n\n    # Metadata\n    total_reward: float = 0.0\n    max_tile: int = 0  # For 2048\n    length: int = 0\n\n    def append(\n        self,\n        observation: torch.Tensor,\n        action: int,\n        reward: float,\n        policy: np.ndarray,\n        root_value: float,\n        chance_outcome: int = 0,\n        entropy: float = 0.0,\n        latent_state: Optional[torch.Tensor] = None,\n        afterstate: Optional[torch.Tensor] = None,\n        player: int = 0,\n    ) -> None:\n        \"\"\"Append a transition to the history.\"\"\"\n        self.observations.append(observation)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.policies.append(policy)\n        self.root_values.append(root_value)\n        self.chance_outcomes.append(chance_outcome)\n        self.entropies.append(entropy)\n        self.to_play.append(player)\n\n        if latent_state is not None:\n            self.latent_states.append(latent_state)\n        if afterstate is not None:\n            self.afterstates.append(afterstate)\n\n        self.total_reward += reward\n        self.length += 1\n\n    def compute_target_values(\n        self,\n        discount: float,\n        td_steps: int,\n    ) -> List[float]:\n        \"\"\"\n        Compute n-step return targets.\n\n        For single-player: target_t = r_t + gr_{t+1} + ... + g^n v_{t+n}\n        For two-player: rewards and bootstrap values are negated when the\n        player changes, since values are from the current player's perspective.\n\n        Args:\n            discount: Discount factor\n            td_steps: Number of steps for TD target (n)\n\n        Returns:\n            List of target values for each position\n        \"\"\"\n        targets = []\n        n = len(self.rewards)\n\n        for i in range(n):\n            value = 0.0\n            for j in range(td_steps):\n                if i + j < n:\n                    # For two-player games, negate reward when player differs\n                    if self.is_two_player and self.to_play:\n                        same_player = self.to_play[i + j] == self.to_play[i]\n                        sign = 1.0 if same_player else -1.0\n                    else:\n                        sign = 1.0\n                    value += (discount ** j) * sign * self.rewards[i + j]\n                else:\n                    break\n\n            # Bootstrap from value estimate\n            bootstrap_idx = i + td_steps\n            if bootstrap_idx < n:\n                if self.is_two_player and self.to_play:\n                    same_player = self.to_play[bootstrap_idx] == self.to_play[i]\n                    sign = 1.0 if same_player else -1.0\n                else:\n                    sign = 1.0\n                value += (discount ** td_steps) * sign * self.root_values[bootstrap_idx]\n\n            targets.append(value)\n\n        return targets\n\n    def compute_priorities(self, td_steps: int, discount: float) -> None:\n        \"\"\"Compute priority scores based on TD error.\"\"\"\n        targets = self.compute_target_values(discount, td_steps)\n\n        # Priority = |target - root_value|\n        self.priorities = np.array([\n            abs(target - root_value)\n            for target, root_value in zip(targets, self.root_values)\n        ])\n\n        # Game priority = max priority in game\n        self.game_priority = float(np.max(self.priorities)) if len(self.priorities) > 0 else 1.0\n\n\n@dataclass\nclass Batch:\n    \"\"\"Training batch.\"\"\"\n\n    observations: torch.Tensor  # (batch, observation_dim)\n    actions: torch.Tensor  # (batch, K)\n    target_values: torch.Tensor  # (batch, K+1)\n    target_rewards: torch.Tensor  # (batch, K)\n    target_policies: torch.Tensor  # (batch, K+1, action_space)\n    chance_outcomes: torch.Tensor  # (batch, K)\n    weights: torch.Tensor  # (batch,) importance sampling weights\n\n    # Indices for priority updates\n    game_indices: List[int] = field(default_factory=list)\n    position_indices: List[int] = field(default_factory=list)\n\n\nclass ReplayBuffer:\n    \"\"\"\n    Replay buffer with prioritized sampling.\n\n    Stores complete game histories and samples positions\n    for training with importance sampling.\n    \"\"\"\n\n    def __init__(\n        self,\n        capacity: int = 100000,\n        batch_size: int = 256,\n        num_unroll_steps: int = 5,\n        td_steps: int = 10,\n        discount: float = 0.997,\n        priority_alpha: float = 1.0,\n        priority_beta: float = 1.0,\n    ):\n        self.capacity = capacity\n        self.batch_size = batch_size\n        self.num_unroll_steps = num_unroll_steps\n        self.td_steps = td_steps\n        self.discount = discount\n        self.priority_alpha = priority_alpha\n        self.priority_beta = priority_beta\n\n        # Storage\n        self.games: deque = deque(maxlen=capacity)\n        self.total_positions = 0\n\n        # Statistics\n        self.games_added = 0\n        self.total_samples = 0\n\n    def save_game(self, game: GameHistory) -> None:\n        \"\"\"Add a completed game to the buffer.\"\"\"\n        # Compute priorities\n        game.compute_priorities(self.td_steps, self.discount)\n\n        # Update total positions\n        if len(self.games) == self.games.maxlen:\n            old_game = self.games[0]\n            self.total_positions -= old_game.length\n\n        self.games.append(game)\n        self.total_positions += game.length\n        self.games_added += 1\n\n    def sample_batch(self, device: torch.device = torch.device(\"cpu\")) -> Batch:\n        \"\"\"\n        Sample a batch of positions for training.\n\n        Uses prioritized sampling at both game and position level.\n\n        Returns:\n            Batch object with all training data\n        \"\"\"\n        # Compute game sampling probabilities\n        game_priorities = np.array([g.game_priority ** self.priority_alpha for g in self.games])\n        game_probs = game_priorities / game_priorities.sum()\n\n        # Sample games\n        game_indices = np.random.choice(\n            len(self.games),\n            size=self.batch_size,\n            p=game_probs,\n            replace=True,\n        )\n\n        # Sample positions within games\n        observations = []\n        actions = []\n        target_values = []\n        target_rewards = []\n        target_policies = []\n        chance_outcomes = []\n        weights = []\n        position_indices = []\n\n        for game_idx in game_indices:\n            game = self.games[game_idx]\n\n            # Sample position within game\n            if game.priorities is not None:\n                pos_priorities = game.priorities ** self.priority_alpha\n                pos_probs = pos_priorities / pos_priorities.sum()\n                pos_idx = np.random.choice(len(game.observations), p=pos_probs)\n            else:\n                pos_idx = np.random.randint(len(game.observations))\n\n            # Compute importance sampling weight\n            total_prob = game_probs[game_idx]\n            if game.priorities is not None:\n                total_prob *= pos_probs[pos_idx]\n            weight = (1.0 / (self.total_positions * total_prob)) ** self.priority_beta\n\n            # Get observation\n            obs = game.observations[pos_idx]\n            observations.append(obs)\n\n            # Get action sequence (pad if needed)\n            action_seq = []\n            reward_seq = []\n            chance_seq = []\n            value_targets = []\n            policy_targets = []\n\n            # Compute value targets\n            all_targets = game.compute_target_values(self.discount, self.td_steps)\n            value_targets.append(all_targets[pos_idx])\n\n            for k in range(self.num_unroll_steps):\n                idx = pos_idx + k\n                if idx < len(game.actions):\n                    action_seq.append(game.actions[idx])\n                    reward_seq.append(game.rewards[idx])\n                    chance_seq.append(game.chance_outcomes[idx])\n                    policy_targets.append(game.policies[idx])\n                    if idx + 1 < len(all_targets):\n                        value_targets.append(all_targets[idx + 1])\n                    else:\n                        value_targets.append(0.0)\n                else:\n                    # Pad with zeros\n                    action_seq.append(0)\n                    reward_seq.append(0.0)\n                    chance_seq.append(0)\n                    policy_targets.append(game.policies[-1])\n                    value_targets.append(0.0)\n\n            # Initial policy\n            policy_targets.insert(0, game.policies[pos_idx])\n\n            actions.append(action_seq)\n            target_rewards.append(reward_seq)\n            target_values.append(value_targets)\n            target_policies.append(policy_targets)\n            chance_outcomes.append(chance_seq)\n            weights.append(weight)\n            position_indices.append(pos_idx)\n\n        # Normalize weights\n        weights = np.array(weights)\n        weights = weights / weights.max()\n\n        # Convert to tensors\n        self.total_samples += self.batch_size\n\n        # Stack observations\n        obs_tensor = torch.stack(observations).to(device)\n\n        # Convert action space size from policies\n        action_space_size = len(target_policies[0][0])\n\n        # Pad policies to same size\n        padded_policies = []\n        for policy_seq in target_policies:\n            padded_seq = []\n            for p in policy_seq:\n                if len(p) < action_space_size:\n                    padded = np.zeros(action_space_size)\n                    padded[:len(p)] = p\n                    padded_seq.append(padded)\n                else:\n                    padded_seq.append(p)\n            padded_policies.append(padded_seq)\n\n        return Batch(\n            observations=obs_tensor,\n            actions=torch.tensor(actions, dtype=torch.long, device=device),\n            target_values=torch.tensor(target_values, dtype=torch.float32, device=device),\n            target_rewards=torch.tensor(target_rewards, dtype=torch.float32, device=device),\n            target_policies=torch.tensor(\n                np.array(padded_policies), dtype=torch.float32, device=device\n            ),\n            chance_outcomes=torch.tensor(chance_outcomes, dtype=torch.long, device=device),\n            weights=torch.tensor(weights, dtype=torch.float32, device=device),\n            game_indices=list(game_indices),\n            position_indices=position_indices,\n        )\n\n    def update_priorities(\n        self,\n        game_indices: List[int],\n        position_indices: List[int],\n        td_errors: np.ndarray,\n    ) -> None:\n        \"\"\"Update priorities based on new TD errors.\"\"\"\n        for game_idx, pos_idx, error in zip(game_indices, position_indices, td_errors):\n            if game_idx < len(self.games):\n                game = self.games[game_idx]\n                if game.priorities is not None and pos_idx < len(game.priorities):\n                    game.priorities[pos_idx] = abs(error)\n                    game.game_priority = float(np.max(game.priorities))\n\n    def get_statistics(self) -> Dict[str, float]:\n        \"\"\"Get buffer statistics.\"\"\"\n        return {\n            \"num_games\": len(self.games),\n            \"total_positions\": self.total_positions,\n            \"games_added\": self.games_added,\n            \"total_samples\": self.total_samples,\n            \"avg_game_length\": (\n                self.total_positions / len(self.games) if self.games else 0.0\n            ),\n            \"avg_total_reward\": (\n                np.mean([g.total_reward for g in self.games]) if self.games else 0.0\n            ),\n        }\n\n    def __len__(self) -> int:\n        return self.total_positions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/trainer.py\n",
    "\"\"\"Training loop for Stochastic MuZero.\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from .replay_buffer import Batch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.support import scalar_to_support, compute_cross_entropy_loss\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    max_grad_norm: float = 5.0\n",
    "    policy_loss_weight: float = 1.0\n",
    "    value_loss_weight: float = 0.5\n",
    "    reward_loss_weight: float = 1.0\n",
    "    chance_loss_weight: float = 1.0\n",
    "    support_size: int = 31\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, config: TrainerConfig, device=torch.device(\"cpu\")):\n",
    "        self.model = model.to(device)\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        self.training_step = 0\n",
    "        self.loss_history = {\"total\": [], \"policy\": [], \"value\": [], \"reward\": [], \"chance\": []}\n",
    "\n",
    "    def train_step(self, batch: Batch) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        unroll_outputs = self.model.unroll(observation=batch.observations, actions=batch.actions, chances=batch.chance_outcomes)\n",
    "        \n",
    "        K = batch.actions.shape[1]\n",
    "        \n",
    "        # Policy loss\n",
    "        policy_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K + 1):\n",
    "            predicted_policy = F.log_softmax(unroll_outputs[\"policy_logits\"][k], dim=-1)\n",
    "            target_policy = batch.target_policies[:, k, :]\n",
    "            mask = target_policy.sum(dim=-1) > 0\n",
    "            if mask.any():\n",
    "                policy_loss += -(target_policy[mask] * predicted_policy[mask]).sum(dim=-1).mean()\n",
    "        policy_loss = policy_loss / (K + 1)\n",
    "        \n",
    "        # Value loss\n",
    "        value_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K + 1):\n",
    "            value_loss += compute_cross_entropy_loss(unroll_outputs[\"value_logits\"][k], batch.target_values[:, k], self.config.support_size)\n",
    "        value_loss = value_loss / (K + 1)\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K):\n",
    "            reward_loss += compute_cross_entropy_loss(unroll_outputs[\"reward_logits\"][k], batch.target_rewards[:, k], self.config.support_size)\n",
    "        reward_loss = reward_loss / max(K, 1)\n",
    "        \n",
    "        # Chance loss\n",
    "        chance_loss = torch.tensor(0.0, device=self.device)\n",
    "        for k in range(K):\n",
    "            predicted_chance = F.log_softmax(unroll_outputs[\"chance_logits\"][k], dim=-1)\n",
    "            target_chance = batch.chance_outcomes[:, k]\n",
    "            mask = target_chance >= 0\n",
    "            if mask.any():\n",
    "                chance_loss += F.nll_loss(predicted_chance[mask], target_chance[mask], reduction=\"mean\")\n",
    "        chance_loss = chance_loss / max(K, 1)\n",
    "        \n",
    "        total_loss = (self.config.policy_loss_weight * policy_loss + self.config.value_loss_weight * value_loss +\n",
    "                     self.config.reward_loss_weight * reward_loss + self.config.chance_loss_weight * chance_loss)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.training_step += 1\n",
    "        return {\"total\": total_loss.item(), \"policy\": policy_loss.item(), \"value\": value_loss.item(),\n",
    "                \"reward\": reward_loss.item(), \"chance\": chance_loss.item()}\n",
    "\n",
    "    def save_checkpoint(self, path: str):\n",
    "        torch.save({\"model_state_dict\": self.model.state_dict(), \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                   \"training_step\": self.training_step}, path)\n",
    "\n",
    "    def load_checkpoint(self, path: str):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.training_step = checkpoint[\"training_step\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2048 game\n",
    "from games.game_2048 import Game2048\n",
    "\n",
    "game = Game2048()\n",
    "state = game.reset()\n",
    "print(\"Initial state:\")\n",
    "print(game.render(state))\n",
    "\n",
    "# Play a few random moves\n",
    "for i in range(5):\n",
    "    legal = game.legal_actions(state)\n",
    "    if not legal:\n",
    "        break\n",
    "    action = np.random.choice(legal)\n",
    "    result = game.step(state, action)\n",
    "    state = result.next_state\n",
    "    print(f\"\\nAfter action {['up', 'right', 'down', 'left'][action]} (reward={result.reward}):\")\n",
    "    print(game.render(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model creation\n",
    "from networks.muzero_network import MuZeroNetwork\n",
    "from utils.config import Config\n",
    "\n",
    "config = Config()\n",
    "model = MuZeroNetwork(\n",
    "    observation_dim=config.observation_dim,\n",
    "    action_space_size=config.action_space_size,\n",
    "    chance_space_size=config.chance_space_size,\n",
    "    state_dim=config.state_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "obs = game.encode_state(state).unsqueeze(0).to(DEVICE)\n",
    "output = model.initial_inference(obs)\n",
    "print(f\"State shape: {output.state.shape}\")\n",
    "print(f\"Policy shape: {output.policy_logits.shape}\")\n",
    "print(f\"Value shape: {output.value_logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcts.tree_search import StochasticMCTS, MCTSConfig\n",
    "from mcts.macro_cache import MacroCache, discover_macros_from_trajectory\n",
    "from training.replay_buffer import ReplayBuffer, GameHistory\n",
    "from training.trainer import Trainer, TrainerConfig\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "NUM_ITERATIONS = 100  # Increase for better results\n",
    "GAMES_PER_ITERATION = 5\n",
    "BATCHES_PER_ITERATION = 20\n",
    "NUM_SIMULATIONS = 25  # Reduced for speed\n",
    "\n",
    "# Initialize components\n",
    "game = Game2048()\n",
    "model = MuZeroNetwork(\n",
    "    observation_dim=config.observation_dim,\n",
    "    action_space_size=config.action_space_size,\n",
    "    chance_space_size=config.chance_space_size,\n",
    "    state_dim=config.state_dim,\n",
    "    hidden_dim=config.hidden_dim,\n",
    ").to(DEVICE)\n",
    "\n",
    "macro_cache = MacroCache(state_dim=config.state_dim, entropy_threshold=config.entropy_threshold)\n",
    "replay_buffer = ReplayBuffer(capacity=10000, batch_size=64, num_unroll_steps=config.num_unroll_steps)\n",
    "trainer = Trainer(model, TrainerConfig(), DEVICE)\n",
    "mcts_config = MCTSConfig(num_simulations=NUM_SIMULATIONS)\n",
    "\n",
    "# Tracking\n",
    "rewards_history = []\n",
    "max_tiles_history = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_mcts(game, model, mcts_config, device, max_moves=500):\n",
    "    \"\"\"Play one game with MCTS.\"\"\"\n",
    "    mcts = StochasticMCTS(model, mcts_config)\n",
    "    state = game.reset()\n",
    "    history = GameHistory()\n",
    "    \n",
    "    model.eval()\n",
    "    for step in range(max_moves):\n",
    "        if game.is_terminal(state):\n",
    "            break\n",
    "            \n",
    "        obs = game.encode_state(state).to(device)\n",
    "        legal = game.legal_actions(state)\n",
    "        if not legal:\n",
    "            break\n",
    "            \n",
    "        root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=True)\n",
    "        temp = 1.0 if step < 30 else 0.1\n",
    "        action, policy = mcts.get_action_policy(root, temp)\n",
    "        \n",
    "        result = game.step(state, action)\n",
    "        \n",
    "        # Get entropy\n",
    "        with torch.no_grad():\n",
    "            dyn_out = model.recurrent_inference(root.hidden_state, torch.tensor([action], device=device))\n",
    "            entropy = dyn_out.chance_entropy.item()\n",
    "        \n",
    "        # Pad policy to action_space_size\n",
    "        full_policy = np.zeros(game.action_space_size)\n",
    "        for i, p in enumerate(policy):\n",
    "            if i < len(full_policy):\n",
    "                full_policy[i] = p\n",
    "        \n",
    "        history.append(\n",
    "            observation=obs.cpu(),\n",
    "            action=action,\n",
    "            reward=result.reward,\n",
    "            policy=full_policy,\n",
    "            root_value=root.value,\n",
    "            chance_outcome=result.chance_outcome,\n",
    "            entropy=entropy,\n",
    "            latent_state=root.hidden_state.cpu() if root.hidden_state is not None else None,\n",
    "        )\n",
    "        \n",
    "        state = result.next_state\n",
    "    \n",
    "    history.max_tile = game.get_max_tile(state)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Starting training for {NUM_ITERATIONS} iterations...\")\n",
    "print(f\"  Games per iteration: {GAMES_PER_ITERATION}\")\n",
    "print(f\"  MCTS simulations: {NUM_SIMULATIONS}\")\n",
    "print()\n",
    "\n",
    "for iteration in tqdm(range(NUM_ITERATIONS), desc=\"Training\"):\n",
    "    # Self-play\n",
    "    iter_rewards = []\n",
    "    iter_max_tiles = []\n",
    "    \n",
    "    for _ in range(GAMES_PER_ITERATION):\n",
    "        history = play_game_with_mcts(game, model, mcts_config, DEVICE, max_moves=500)\n",
    "        replay_buffer.save_game(history)\n",
    "        iter_rewards.append(history.total_reward)\n",
    "        iter_max_tiles.append(history.max_tile)\n",
    "    \n",
    "    rewards_history.append(np.mean(iter_rewards))\n",
    "    max_tiles_history.append(np.max(iter_max_tiles))\n",
    "    \n",
    "    # Training\n",
    "    if len(replay_buffer) >= 64:\n",
    "        iter_losses = []\n",
    "        for _ in range(BATCHES_PER_ITERATION):\n",
    "            batch = replay_buffer.sample_batch(DEVICE)\n",
    "            losses = trainer.train_step(batch)\n",
    "            iter_losses.append(losses[\"total\"])\n",
    "        loss_history.append(np.mean(iter_losses))\n",
    "    \n",
    "    # Logging\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        macro_stats = macro_cache.get_statistics()\n",
    "        print(f\"\\nIter {iteration + 1}: Reward={rewards_history[-1]:.0f}, MaxTile={max_tiles_history[-1]}, \"\n",
    "              f\"Loss={loss_history[-1] if loss_history else 0:.4f}, Macros={macro_stats['num_macros']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(rewards_history)\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Average Reward\")\n",
    "axes[0].set_title(\"Training Reward\")\n",
    "\n",
    "axes[1].plot(max_tiles_history)\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Max Tile\")\n",
    "axes[1].set_title(\"Best Tile Achieved\")\n",
    "axes[1].set_yscale('log', base=2)\n",
    "\n",
    "if loss_history:\n",
    "    axes[2].plot(loss_history)\n",
    "    axes[2].set_xlabel(\"Iteration\")\n",
    "    axes[2].set_ylabel(\"Loss\")\n",
    "    axes[2].set_title(\"Training Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a game with visualization\n",
    "print(\"Playing a game with trained model...\\n\")\n",
    "\n",
    "model.eval()\n",
    "mcts = StochasticMCTS(model, MCTSConfig(num_simulations=50))\n",
    "\n",
    "state = game.reset()\n",
    "print(\"Initial:\")\n",
    "print(game.render(state))\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(200):\n",
    "    if game.is_terminal(state):\n",
    "        break\n",
    "    \n",
    "    obs = game.encode_state(state).to(DEVICE)\n",
    "    legal = game.legal_actions(state)\n",
    "    if not legal:\n",
    "        break\n",
    "    \n",
    "    root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=False)\n",
    "    action, _ = mcts.get_action_policy(root, temperature=0.0)  # Greedy\n",
    "    \n",
    "    result = game.step(state, action)\n",
    "    total_reward += result.reward\n",
    "    state = result.next_state\n",
    "    \n",
    "    if step % 50 == 49:\n",
    "        print(f\"\\nStep {step + 1}:\")\n",
    "        print(game.render(state))\n",
    "\n",
    "print(f\"\\nFinal state:\")\n",
    "print(game.render(state))\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "print(f\"Max tile: {game.get_max_tile(state)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_checkpoint(\"runs/stochastic_muzero_2048.pt\")\n",
    "print(\"Model saved to runs/stochastic_muzero_2048.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Macro Analysis\n",
    "\n",
    "Analyze the discovered macro-operators to see what temporal abstractions emerged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze macros\n",
    "stats = macro_cache.get_statistics()\n",
    "print(\"Macro-Operator Statistics:\")\n",
    "print(f\"  Total macros discovered: {stats['total_discoveries']}\")\n",
    "print(f\"  Active macros: {stats['num_macros']}\")\n",
    "print(f\"  Total uses: {stats['total_uses']}\")\n",
    "print(f\"  Success rate: {stats['success_rate']:.2%}\")\n",
    "print(f\"  Average confidence: {stats['avg_confidence']:.3f}\")\n",
    "print(f\"  Average length: {stats['avg_length']:.1f} steps\")\n",
    "\n",
    "# Show top macros\n",
    "if macro_cache.macros:\n",
    "    print(\"\\nTop 10 macros by usage:\")\n",
    "    sorted_macros = sorted(macro_cache.macros.values(), key=lambda m: m.usage_count, reverse=True)[:10]\n",
    "    action_names = [\"up\", \"right\", \"down\", \"left\"]\n",
    "    for m in sorted_macros:\n",
    "        actions_str = \" -> \".join(action_names[a] for a in m.action_sequence)\n",
    "        print(f\"  [{actions_str}] uses={m.usage_count}, success={m.success_rate:.2%}, conf={m.confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Train longer**: Increase `NUM_ITERATIONS` to 500-1000 for better performance\n",
    "2. **More MCTS simulations**: Increase `NUM_SIMULATIONS` to 100-200\n",
    "3. **Add Go environment**: Implement Go game interface for fully deterministic setting\n",
    "4. **Analyze entropy distribution**: Track how entropy separates deterministic vs stochastic transitions\n",
    "5. **Measure planning speedup**: Compare planning with and without macro usage"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Deterministic Games: Tic-Tac-Toe & Chess\n\nThese fully deterministic two-player games have entropy \u2248 0 at every transition, making every trajectory segment a macro candidate. This validates the macro discovery pipeline.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test Tic-Tac-Toe\nfrom games.game_tictactoe import GameTicTacToe\n\nttt = GameTicTacToe()\nstate = ttt.reset()\nprint(\"Tic-Tac-Toe test:\")\nprint(ttt.render(state))\nprint(f\"Legal actions: {ttt.legal_actions(state)}\")\nprint(f\"Observation shape: {ttt.encode_state(state).shape}\")\nprint(f\"Is two-player: {ttt.is_two_player}\")\nprint(f\"Chance space: {ttt.chance_space_size}\")\n\n# Play a random game\nstate = ttt.reset()\nwhile not ttt.is_terminal(state):\n    actions = ttt.legal_actions(state)\n    action = np.random.choice(actions)\n    result = ttt.step(state, action)\n    state = result.next_state\nprint(f\"\\nRandom game result:\\n{ttt.render(state)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Train on Tic-Tac-Toe (deterministic two-player game)\nTTT_ITERATIONS = 200\nTTT_GAMES = 10\nTTT_BATCHES = 20\nTTT_SIMS = 25\n\nttt_game = GameTicTacToe()\nttt_model = MuZeroNetwork(\n    observation_dim=27,  # 3 planes x 3x3\n    action_space_size=9,\n    chance_space_size=1,\n    state_dim=128,\n    hidden_dim=64,\n).to(DEVICE)\n\nttt_macro_cache = MacroCache(state_dim=128, entropy_threshold=0.1)\nttt_buffer = ReplayBuffer(capacity=5000, batch_size=64, num_unroll_steps=3, td_steps=5, discount=1.0)\nttt_trainer = Trainer(ttt_model, TrainerConfig(support_size=31), DEVICE)\nttt_mcts_config = MCTSConfig(num_simulations=TTT_SIMS, action_space_size=9, is_two_player=True, discount=1.0)\n\nttt_rewards = []\nttt_losses = []\n\nprint(f\"Training Tic-Tac-Toe for {TTT_ITERATIONS} iterations...\")\nfor iteration in tqdm(range(TTT_ITERATIONS), desc=\"TTT Training\"):\n    iter_rewards = []\n    for _ in range(TTT_GAMES):\n        mcts = StochasticMCTS(ttt_model, ttt_mcts_config, ttt_macro_cache)\n        state = ttt_game.reset()\n        history = GameHistory(is_two_player=True)\n        ttt_model.eval()\n        \n        for step in range(20):\n            if ttt_game.is_terminal(state):\n                break\n            obs = ttt_game.encode_state(state).to(DEVICE)\n            legal = ttt_game.legal_actions(state)\n            if not legal:\n                break\n            root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=True)\n            temp = 1.0 if step < 5 else 0.1\n            action, policy = mcts.get_action_policy(root, temp)\n            result = ttt_game.step(state, action)\n            \n            with torch.no_grad():\n                dyn_out = ttt_model.recurrent_inference(root.hidden_state, torch.tensor([action], device=DEVICE))\n                entropy = dyn_out.chance_entropy.item()\n            \n            full_policy = np.zeros(9)\n            for i, p in enumerate(policy):\n                if i < 9:\n                    full_policy[i] = p\n            \n            player = ttt_game.current_player(state)\n            history.append(observation=obs.cpu(), action=action, reward=result.reward, policy=full_policy,\n                          root_value=root.value, chance_outcome=result.chance_outcome, entropy=entropy,\n                          latent_state=root.hidden_state.cpu() if root.hidden_state is not None else None,\n                          player=player)\n            state = result.next_state\n        \n        ttt_buffer.save_game(history)\n        iter_rewards.append(history.total_reward)\n    \n    ttt_rewards.append(np.mean(iter_rewards))\n    \n    if len(ttt_buffer) >= 64:\n        iter_losses = []\n        for _ in range(TTT_BATCHES):\n            batch = ttt_buffer.sample_batch(DEVICE)\n            losses = ttt_trainer.train_step(batch)\n            iter_losses.append(losses[\"total\"])\n        ttt_losses.append(np.mean(iter_losses))\n    \n    if (iteration + 1) % 50 == 0:\n        macro_stats = ttt_macro_cache.get_statistics()\n        print(f\"\\nIter {iteration+1}: Reward={ttt_rewards[-1]:.2f}, Loss={ttt_losses[-1] if ttt_losses else 0:.4f}, Macros={macro_stats['num_macros']}\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].plot(ttt_rewards)\naxes[0].set_xlabel(\"Iteration\"); axes[0].set_ylabel(\"Avg Reward\"); axes[0].set_title(\"TTT Reward\")\nif ttt_losses:\n    axes[1].plot(ttt_losses)\n    axes[1].set_xlabel(\"Iteration\"); axes[1].set_ylabel(\"Loss\"); axes[1].set_title(\"TTT Loss\")\nplt.tight_layout(); plt.show()\n\n# Macro analysis\nstats = ttt_macro_cache.get_statistics()\nprint(f\"\\nTTT Macro Stats: {stats['num_macros']} macros, {stats['total_discoveries']} discovered\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test Chess\n!pip install python-chess -q\nfrom games.game_chess import GameChess\n\nchess_game = GameChess()\nstate = chess_game.reset()\nprint(\"Chess test:\")\nprint(chess_game.render(state))\nprint(f\"\\nLegal actions: {len(chess_game.legal_actions(state))} moves\")\nprint(f\"Observation shape: {chess_game.encode_state(state).shape}\")\nprint(f\"Action space: {chess_game.action_space_size}\")\nprint(f\"Is two-player: {chess_game.is_two_player}\")\n\n# Play a few random moves\nfor i in range(4):\n    actions = chess_game.legal_actions(state)\n    action = np.random.choice(actions)\n    result = chess_game.step(state, action)\n    state = result.next_state\nprint(f\"\\nAfter 4 random moves:\\n{chess_game.render(state)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train on Chess (deterministic two-player game)\n# Note: Chess is much more complex. This is a minimal run to validate the pipeline.\n# For meaningful macro discovery, train for 1000+ iterations.\n\nCHESS_ITERATIONS = 50\nCHESS_GAMES = 3\nCHESS_BATCHES = 15\nCHESS_SIMS = 20\nCHESS_MAX_MOVES = 80\n\nchess_model = MuZeroNetwork(\n    observation_dim=1408,  # 22 planes x 8x8\n    action_space_size=4672,\n    chance_space_size=1,\n    state_dim=256,\n    hidden_dim=128,\n).to(DEVICE)\n\nchess_macro_cache = MacroCache(state_dim=256, entropy_threshold=0.1)\nchess_buffer = ReplayBuffer(capacity=10000, batch_size=64, num_unroll_steps=5, td_steps=10, discount=1.0)\nchess_trainer = Trainer(chess_model, TrainerConfig(support_size=31), DEVICE)\nchess_mcts_config = MCTSConfig(\n    num_simulations=CHESS_SIMS,\n    action_space_size=4672,\n    is_two_player=True,\n    discount=1.0,\n)\n\nchess_rewards = []\nchess_losses = []\nchess_game_lengths = []\n\nprint(f\"Training Chess for {CHESS_ITERATIONS} iterations...\")\nprint(f\"  Model params: {sum(p.numel() for p in chess_model.parameters()):,}\")\nprint(f\"  Games per iteration: {CHESS_GAMES}\")\nprint(f\"  MCTS simulations: {CHESS_SIMS}\")\nprint()\n\nfor iteration in tqdm(range(CHESS_ITERATIONS), desc=\"Chess Training\"):\n    iter_rewards = []\n    iter_lengths = []\n\n    for _ in range(CHESS_GAMES):\n        mcts = StochasticMCTS(chess_model, chess_mcts_config, chess_macro_cache)\n        state = chess_game.reset()\n        history = GameHistory(is_two_player=True)\n        chess_model.eval()\n\n        for step in range(CHESS_MAX_MOVES):\n            if chess_game.is_terminal(state):\n                break\n            obs = chess_game.encode_state(state).to(DEVICE)\n            legal = chess_game.legal_actions(state)\n            if not legal:\n                break\n            root = mcts.search(obs.unsqueeze(0), legal, add_exploration_noise=True)\n            temp = 1.0 if step < 30 else 0.1\n            action, policy = mcts.get_action_policy(root, temp)\n            result = chess_game.step(state, action)\n\n            with torch.no_grad():\n                dyn_out = chess_model.recurrent_inference(root.hidden_state, torch.tensor([action], device=DEVICE))\n                entropy = dyn_out.chance_entropy.item()\n\n            player = chess_game.current_player(state)\n            history.append(\n                observation=obs.cpu(), action=action, reward=result.reward,\n                policy=policy, root_value=root.value,\n                chance_outcome=result.chance_outcome, entropy=entropy,\n                latent_state=root.hidden_state.cpu() if root.hidden_state is not None else None,\n                player=player,\n            )\n            state = result.next_state\n\n        chess_buffer.save_game(history)\n        iter_rewards.append(history.total_reward)\n        iter_lengths.append(history.length)\n\n    chess_rewards.append(np.mean(iter_rewards))\n    chess_game_lengths.append(np.mean(iter_lengths))\n\n    if len(chess_buffer) >= 64:\n        iter_losses = []\n        for _ in range(CHESS_BATCHES):\n            batch = chess_buffer.sample_batch(DEVICE)\n            losses = chess_trainer.train_step(batch)\n            iter_losses.append(losses[\"total\"])\n        chess_losses.append(np.mean(iter_losses))\n\n    if (iteration + 1) % 10 == 0:\n        macro_stats = chess_macro_cache.get_statistics()\n        avg_len = chess_game_lengths[-1] if chess_game_lengths else 0\n        print(f\"\\nIter {iteration+1}: Reward={chess_rewards[-1]:.2f}, \"\n              f\"Avg Length={avg_len:.0f}, \"\n              f\"Loss={chess_losses[-1] if chess_losses else 0:.4f}, \"\n              f\"Macros={macro_stats['num_macros']}\")\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\naxes[0].plot(chess_rewards)\naxes[0].set_xlabel(\"Iteration\"); axes[0].set_ylabel(\"Avg Reward\"); axes[0].set_title(\"Chess Reward\")\nif chess_losses:\n    axes[1].plot(chess_losses)\n    axes[1].set_xlabel(\"Iteration\"); axes[1].set_ylabel(\"Loss\"); axes[1].set_title(\"Chess Loss\")\naxes[2].plot(chess_game_lengths)\naxes[2].set_xlabel(\"Iteration\"); axes[2].set_ylabel(\"Avg Length\"); axes[2].set_title(\"Chess Game Length\")\nplt.tight_layout(); plt.show()\n\n# Macro analysis\nstats = chess_macro_cache.get_statistics()\nprint(f\"\\nChess Macro Stats: {stats['num_macros']} macros, {stats['total_discoveries']} discovered\")\nif stats['num_macros'] > 0:\n    print(\"\\nAll transitions are deterministic (entropy \\u2248 0)\")\n    print(\"Every trajectory segment qualifies as a macro candidate!\")\n    top = chess_macro_cache.get_top_macros(5)\n    for i, m in enumerate(top):\n        print(f\"  Macro {i+1}: actions={m.action_sequence}, confidence={m.confidence:.3f}, uses={m.usage_count}\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}